{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec3bf0d",
   "metadata": {},
   "source": [
    "# üìà Regression Analysis: Predicting Customer Spending\n",
    "\n",
    "## Objective\n",
    "Predict `TotalSpend_log` ‚Äî the log-transformed total customer spending ‚Äî using demographic and behavioral features.\n",
    "\n",
    "**Why Log Transform?**\n",
    "- Original `TotalSpend` has skewness of 0.86\n",
    "- Log transformation reduces skewness to -0.37\n",
    "- Better satisfies linear model assumptions\n",
    "- To get dollar values: `TotalSpend = exp(TotalSpend_log) - 1`\n",
    "\n",
    "**Key Insights from EDA**:\n",
    "- Top predictors: `IncomePerCapita` (r=0.67), `HasChildren` (r=-0.42)\n",
    "- Customers without children spend **2.5x more**\n",
    "- No data leakage concerns for regression (unlike classification)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.base import clone\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not installed. Run: pip install xgboost\")\n",
    "\n",
    "# Our custom modules\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.config import (\n",
    "    DATA_PATH, RANDOM_STATE, TEST_SIZE, CV_FOLDS,\n",
    "    SPENDING_COLS, MAIN_COLOR, SECONDARY_COLOR, ACCENT_COLOR\n",
    ")\n",
    "from src.data_loader import load_data\n",
    "from src.preprocessing import IQRCapper\n",
    "from src.visualization import set_style\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "set_style()\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Create models directory\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"  Random State: {RANDOM_STATE}\")\n",
    "print(f\"  Test Size: {TEST_SIZE}\")\n",
    "print(f\"  CV Folds: {CV_FOLDS}\")\n",
    "print(f\"  XGBoost available: {HAS_XGBOOST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_raw = load_data(DATA_PATH)\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering (same as EDA notebook)\n",
    "df = df_raw.copy()\n",
    "\n",
    "# 1. Handle missing values (Income)\n",
    "income_median = df['Income'].median()\n",
    "df['Income'] = df['Income'].fillna(income_median)\n",
    "print(f\"‚úì Imputed {df_raw['Income'].isna().sum()} missing Income values with median (${income_median:,.0f})\")\n",
    "\n",
    "# 2. Date & Tenure features\n",
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')\n",
    "reference_date = df['Dt_Customer'].max()\n",
    "df['Tenure_Days'] = (reference_date - df['Dt_Customer']).dt.days\n",
    "df['Tenure_Months'] = df['Tenure_Days'] // 30\n",
    "\n",
    "# 3. Age calculation & filtering\n",
    "current_year = reference_date.year\n",
    "df['Age'] = current_year - df['Year_Birth']\n",
    "df['Age'] = df['Age'].clip(18, 90)  # Cap unrealistic ages\n",
    "\n",
    "# 4. Spending features\n",
    "df['TotalSpend'] = df[SPENDING_COLS].sum(axis=1)\n",
    "df['TotalSpend_log'] = np.log1p(df['TotalSpend'])\n",
    "df['SpendingRatio'] = df['TotalSpend'] / df['Income'].replace(0, 1)\n",
    "\n",
    "# 5. Purchase behavior features\n",
    "purchase_cols = ['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']\n",
    "df['TotalPurchases'] = df[purchase_cols].sum(axis=1)\n",
    "df['AvgSpendPerPurchase'] = df['TotalSpend'] / df['TotalPurchases'].replace(0, 1)\n",
    "df['WebPurchaseRatio'] = df['NumWebPurchases'] / df['TotalPurchases'].replace(0, 1)\n",
    "\n",
    "# 6. Family & Household features\n",
    "df['FamilySize'] = df['Kidhome'] + df['Teenhome'] + 1\n",
    "df['HasChildren'] = ((df['Kidhome'] > 0) | (df['Teenhome'] > 0)).astype(int)\n",
    "df['TotalChildren'] = df['Kidhome'] + df['Teenhome']\n",
    "df['IncomePerCapita'] = df['Income'] / df['FamilySize']\n",
    "\n",
    "# 7. Categorical encoding\n",
    "education_num_map = {'Basic': 1, '2n Cycle': 2, 'Graduation': 3, 'Master': 4, 'PhD': 5}\n",
    "df['Education_Num'] = df['Education'].map(education_num_map)\n",
    "\n",
    "marital_map = {\n",
    "    'Married': 'Partner', 'Together': 'Partner',\n",
    "    'Single': 'Single', 'Divorced': 'Single', 'Widow': 'Single',\n",
    "    'Alone': 'Single', 'Absurd': 'Single', 'YOLO': 'Single'\n",
    "}\n",
    "df['Marital_Status_Clean'] = df['Marital_Status'].map(marital_map)\n",
    "df['IsPartner'] = (df['Marital_Status_Clean'] == 'Partner').astype(int)\n",
    "\n",
    "# 8. Segmentation flags\n",
    "spend_threshold = df['TotalSpend'].quantile(0.75)\n",
    "df['IsHighSpender'] = (df['TotalSpend'] >= spend_threshold).astype(int)\n",
    "\n",
    "print(f\"\\n‚úì Feature engineering complete!\")\n",
    "print(f\"  Original columns: {len(df_raw.columns)}\")\n",
    "print(f\"  Final columns: {len(df.columns)}\")\n",
    "print(f\"  New features: {len(df.columns) - len(df_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable check\n",
    "target_col = 'TotalSpend_log'\n",
    "\n",
    "print(f\"Target: {target_col}\")\n",
    "print(f\"\\nTarget Statistics:\")\n",
    "print(df[target_col].describe())\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.histplot(df['TotalSpend'], kde=True, ax=axes[0], color=MAIN_COLOR)\n",
    "axes[0].set_title(f'TotalSpend (Original) - Skew: {df[\"TotalSpend\"].skew():.2f}')\n",
    "\n",
    "sns.histplot(df[target_col], kde=True, ax=axes[1], color=SECONDARY_COLOR)\n",
    "axes[1].set_title(f'{target_col} (Log) - Skew: {df[target_col].skew():.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3946a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Feature Selection\n",
    "\n",
    "**Feature Selection Strategy**:\n",
    "- ‚úÖ Include engineered features from EDA (`IncomePerCapita`, `HasChildren`, etc.)\n",
    "- ‚ùå Exclude `Mnt*` spending columns (they compose the target ‚Äî perfect leakage!)\n",
    "- ‚ùå Exclude `TotalSpend`, `SpendingRatio`, `AvgSpendPerPurchase` (derived from target)\n",
    "- ‚ùå Exclude ID columns and dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "# IMPORTANT: Exclude spending-related features to avoid target leakage!\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    'Income', 'IncomePerCapita',  # Purchasing power\n",
    "    'Age', 'Tenure_Days',         # Demographics\n",
    "    'Recency',                    # Engagement timing\n",
    "    'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',  # Purchase channels\n",
    "    'NumWebVisitsMonth',          # Web engagement\n",
    "    'Kidhome', 'Teenhome', 'FamilySize', 'TotalChildren',  # Family composition\n",
    "    'Education_Num',              # Ordinal education\n",
    "]\n",
    "\n",
    "BINARY_FEATURES = [\n",
    "    'HasChildren', 'IsPartner'    # Binary flags\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'Education', 'Marital_Status_Clean'  # Will be one-hot encoded\n",
    "]\n",
    "\n",
    "# Features to EXCLUDE (target leakage or irrelevant)\n",
    "EXCLUDE_FEATURES = [\n",
    "    'ID', 'Dt_Customer', 'Year_Birth',  # ID and dates\n",
    "    'TotalSpend', 'TotalSpend_log', 'SpendingRatio', 'AvgSpendPerPurchase',  # Target-derived\n",
    "    'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',  # Compose target\n",
    "    'IsHighSpender', 'WebPurchaseRatio', 'TotalPurchases',  # Derived from target or purchases\n",
    "    'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response',  # Campaign data\n",
    "]\n",
    "\n",
    "# Combine all features\n",
    "all_features = NUMERIC_FEATURES + BINARY_FEATURES\n",
    "cat_features = CATEGORICAL_FEATURES\n",
    "\n",
    "print(f\"Numeric + Binary Features ({len(all_features)}):\")\n",
    "for f in all_features:\n",
    "    print(f\"  ‚Ä¢ {f}\")\n",
    "print(f\"\\nCategorical Features ({len(cat_features)}):\")\n",
    "for f in cat_features:\n",
    "    print(f\"  ‚Ä¢ {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e053609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify features exist in dataframe\n",
    "missing_num = [c for c in all_features if c not in df.columns]\n",
    "missing_cat = [c for c in cat_features if c not in df.columns]\n",
    "\n",
    "if missing_num:\n",
    "    print(f\"‚ö†Ô∏è Missing numeric features: {missing_num}\")\n",
    "    all_features = [c for c in all_features if c in df.columns]\n",
    "if missing_cat:\n",
    "    print(f\"‚ö†Ô∏è Missing categorical features: {missing_cat}\")\n",
    "    cat_features = [c for c in cat_features if c in df.columns]\n",
    "\n",
    "# Prepare X and y\n",
    "X = df[all_features + cat_features].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "print(f\"\\n‚úì Feature matrix prepared\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n",
    "print(f\"  y range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "print(f\"\\nFeature types:\")\n",
    "print(X.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (BEFORE preprocessing to prevent data leakage)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data split complete\")\n",
    "print(f\"  Training set: {X_train.shape[0]:,} samples ({100*(1-TEST_SIZE):.0f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]:,} samples ({100*TEST_SIZE:.0f}%)\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Train mean: {y_train.mean():.4f}, std: {y_train.std():.4f}\")\n",
    "print(f\"  Test mean:  {y_test.mean():.4f}, std: {y_test.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b81ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Preprocessing Pipeline\n",
    "\n",
    "**Pipeline Strategy**:\n",
    "1. **Numeric features**: Impute missing ‚Üí Cap outliers (IQR) ‚Üí StandardScaler\n",
    "2. **Categorical features**: Impute ‚Üí OneHotEncode\n",
    "\n",
    "**Critical**: All transformations fitted on training data ONLY to prevent data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0596a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric and categorical features for pipeline\n",
    "num_features = [c for c in all_features if c in X.columns]\n",
    "cat_features_final = [c for c in cat_features if c in X.columns]\n",
    "\n",
    "print(f\"Numeric features ({len(num_features)}): {num_features}\")\n",
    "print(f\"Categorical features ({len(cat_features_final)}): {cat_features_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Numeric pipeline: Impute ‚Üí Cap Outliers ‚Üí Scale\n",
    "# Note: IQRCapper takes columns=None to apply to all, k=1.5 for multiplier\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('capper', IQRCapper(columns=None, k=1.5)),  # Apply to all numeric columns\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: Impute ‚Üí OneHot Encode\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine into full preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, num_features),\n",
    "    ('cat', categorical_pipeline, cat_features_final)\n",
    "], remainder='drop')\n",
    "\n",
    "print(\"‚úì Preprocessing pipeline created\")\n",
    "print(f\"\\nPipeline structure:\")\n",
    "print(f\"  Numeric: Impute(median) ‚Üí IQRCapper(k=1.5) ‚Üí StandardScaler\")\n",
    "print(f\"  Categorical: Impute(mode) ‚Üí OneHotEncoder(drop_first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debd082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit preprocessor on training data ONLY, then transform both sets\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"‚úì Preprocessing complete\")\n",
    "print(f\"  X_train: {X_train.shape} ‚Üí {X_train_processed.shape}\")\n",
    "print(f\"  X_test: {X_test.shape} ‚Üí {X_test_processed.shape}\")\n",
    "\n",
    "# Get feature names after encoding\n",
    "cat_encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
    "cat_feature_names = cat_encoder.get_feature_names_out(cat_features_final).tolist()\n",
    "feature_names = num_features + cat_feature_names\n",
    "\n",
    "print(f\"\\nFeature names ({len(feature_names)} total):\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i+1:2}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4517200",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Baseline Model Comparison\n",
    "\n",
    "We'll compare multiple regression algorithms with **default hyperparameters** using 5-fold cross-validation:\n",
    "\n",
    "| Model Type | Algorithms |\n",
    "|------------|------------|\n",
    "| **Linear** | Linear Regression, Ridge, Lasso, ElasticNet |\n",
    "| **Tree-Based** | Decision Tree, Random Forest, Gradient Boosting, XGBoost |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47727050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all regression models\n",
    "models = {\n",
    "    # Linear Models\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
    "    'Lasso': Lasso(alpha=0.1, random_state=RANDOM_STATE),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=RANDOM_STATE),\n",
    "    \n",
    "    # Tree-Based Models\n",
    "    'DecisionTree': DecisionTreeRegressor(max_depth=10, random_state=RANDOM_STATE),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if HAS_XGBOOST:\n",
    "    models['XGBoost'] = XGBRegressor(\n",
    "        n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1, verbosity=0\n",
    "    )\n",
    "\n",
    "print(f\"Models to evaluate ({len(models)}):\")\n",
    "for name in models:\n",
    "    print(f\"  ‚Ä¢ {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate baseline models with cross-validation\n",
    "print(\"Baseline Model Comparison (5-Fold CV):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_processed, y_train, cv=CV_FOLDS, scoring='r2')\n",
    "    \n",
    "    result = {\n",
    "        'Model': name,\n",
    "        'CV_mean': cv_scores.mean(),\n",
    "        'CV_std': cv_scores.std(),\n",
    "        'CV_min': cv_scores.min(),\n",
    "        'CV_max': cv_scores.max()\n",
    "    }\n",
    "    baseline_results.append(result)\n",
    "    \n",
    "    print(f\"{name:20} | R¬≤ = {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Convert to DataFrame and sort\n",
    "baseline_df = pd.DataFrame(baseline_results).sort_values('CV_mean', ascending=False).reset_index(drop=True)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"\\nüèÜ Best baseline model: {baseline_df.iloc[0]['Model']} (R¬≤ = {baseline_df.iloc[0]['CV_mean']:.4f})\")\n",
    "\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = [MAIN_COLOR if i == 0 else SECONDARY_COLOR for i in range(len(baseline_df))]\n",
    "bars = ax.barh(baseline_df['Model'], baseline_df['CV_mean'], \n",
    "               xerr=baseline_df['CV_std'], color=colors, capsize=5)\n",
    "\n",
    "ax.set_xlabel('Cross-Validation R¬≤ Score')\n",
    "ax.set_title('Baseline Model Comparison (5-Fold CV)')\n",
    "ax.axvline(baseline_df['CV_mean'].max(), color='red', linestyle='--', alpha=0.5, label='Best')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, baseline_df['CV_mean'])):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.4f}', \n",
    "            va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93342199",
   "metadata": {},
   "source": [
    "### üìä Baseline Results Analysis\n",
    "\n",
    "#### Why Tree-Based Models Outperform Linear Models\n",
    "\n",
    "The results clearly show a **performance hierarchy**: **Gradient Boosting (0.9504) > XGBoost (0.9501) > Random Forest (0.9487) >> Linear Models (~0.88)**\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "1. **Non-Linear Relationships Dominate**\n",
    "   - The ~6-7% R¬≤ gap between tree-based and linear models indicates significant non-linear relationships in the data\n",
    "   - Customer spending behavior doesn't scale linearly with income ‚Äî there are thresholds and saturation effects\n",
    "   - Example: A customer earning $100K doesn't spend exactly 2x more than one earning $50K\n",
    "\n",
    "2. **Feature Interactions Matter**\n",
    "   - Tree-based models automatically capture interactions (e.g., `Income √ó HasChildren`)\n",
    "   - EDA showed that `HasChildren` dramatically impacts spending (2.5x difference) ‚Äî this effect likely varies by income level\n",
    "   - Linear models would need manually engineered interaction terms to capture this\n",
    "\n",
    "3. **Ensemble Advantage**\n",
    "   - GradientBoosting and XGBoost use sequential boosting: each tree corrects errors from previous trees\n",
    "   - RandomForest uses bagging: parallel trees vote together, reducing variance\n",
    "   - Single DecisionTree (0.9116) shows the power of combining multiple weak learners\n",
    "\n",
    "4. **Linear Model Performance Ceiling**\n",
    "   - Ridge and LinearRegression perform identically (0.8886) ‚Äî multicollinearity isn't severely impacting predictions\n",
    "   - Lasso (0.8714) performs worst because it aggressively removes features that actually contribute\n",
    "   - ElasticNet (0.8833) offers a balance but still can't capture non-linearities\n",
    "\n",
    "5. **Variance Comparison**\n",
    "   - Tree models: CV std ~0.019-0.020 (consistent across folds)\n",
    "   - Linear models: CV std ~0.012-0.019 (more stable but lower ceiling)\n",
    "   - The extra variance in tree models is acceptable given the ~6% performance gain\n",
    "\n",
    "**Conclusion:** The data contains complex, non-linear patterns that tree-based ensemble methods are specifically designed to capture. Proceeding with hyperparameter tuning on the top 3 tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ab3c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Hyperparameter Tuning\n",
    "\n",
    "We'll tune the **top 3 performing models** using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 3 models for hyperparameter tuning\n",
    "top_models = baseline_df.head(3)['Model'].tolist()\n",
    "print(f\"Top models for tuning: {top_models}\")\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.01, 0.1, 1.0],\n",
    "        'l1_ratio': [0.2, 0.5, 0.8]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeec489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune top models\n",
    "tuned_models = {}\n",
    "tuning_results = []\n",
    "\n",
    "for name in top_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Tuning {name}...\")\n",
    "    \n",
    "    model = models[name]\n",
    "    \n",
    "    if name in param_grids:\n",
    "        grid_search = GridSearchCV(\n",
    "            model,\n",
    "            param_grids[name],\n",
    "            cv=CV_FOLDS,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid_search.fit(X_train_processed, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_score = grid_search.best_score_\n",
    "        best_params = grid_search.best_params_\n",
    "        \n",
    "        tuned_models[name] = best_model\n",
    "        tuning_results.append({\n",
    "            'Model': name,\n",
    "            'Best_CV_R2': best_score,\n",
    "            'Best_Params': str(best_params)\n",
    "        })\n",
    "        \n",
    "        print(f\"  Best CV R¬≤: {best_score:.4f}\")\n",
    "        print(f\"  Best params: {best_params}\")\n",
    "    else:\n",
    "        # No grid defined, use default\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        tuned_models[name] = model\n",
    "        print(f\"  No parameter grid defined, using defaults\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Hyperparameter tuning complete!\")\n",
    "\n",
    "pd.DataFrame(tuning_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f152b7",
   "metadata": {},
   "source": [
    "### üîß Hyperparameter Tuning Analysis\n",
    "\n",
    "#### Understanding the Tuning Results\n",
    "\n",
    "| Model | Baseline CV R¬≤ | Tuned CV R¬≤ | Improvement |\n",
    "|-------|----------------|-------------|-------------|\n",
    "| GradientBoosting | 0.9504 | 0.9508 | +0.04% |\n",
    "| XGBoost | 0.9501 | 0.9534 | +0.33% |\n",
    "| RandomForest | 0.9487 | 0.9495 | +0.08% |\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Marginal Improvements Are Expected**\n",
    "   - The baseline models were already well-configured with reasonable defaults\n",
    "   - When CV R¬≤ is already >0.94, there's limited room for improvement\n",
    "   - The ~0.5% total gain from tuning is typical for well-structured problems\n",
    "\n",
    "2. **XGBoost Benefits Most from Tuning**\n",
    "   - Optimal params: `learning_rate=0.2, max_depth=3, n_estimators=100`\n",
    "   - Higher learning rate (0.2 vs default 0.1) allows faster convergence\n",
    "   - Shallow depth (3) prevents overfitting while maintaining signal\n",
    "   - XGBoost's regularization (`reg_alpha`, `reg_lambda`) provides built-in overfitting protection\n",
    "\n",
    "3. **GradientBoosting Configuration**\n",
    "   - Same params as XGBoost: `learning_rate=0.2, max_depth=3, n_estimators=100`\n",
    "   - Both boosting algorithms converged to similar configurations\n",
    "   - This suggests the data's optimal complexity level is captured by shallow trees\n",
    "\n",
    "4. **RandomForest Strategy Differs**\n",
    "   - Optimal params: `max_depth=15, min_samples_split=5, n_estimators=200`\n",
    "   - Deeper trees (15 vs 3 for boosting) ‚Äî bagging allows more complexity per tree\n",
    "   - More trees (200) for better averaging and stability\n",
    "   - `min_samples_split=5` adds regularization to prevent overfitting\n",
    "\n",
    "5. **Why Boosting vs Bagging Configurations Differ**\n",
    "   - **Boosting (GB, XGB)**: Sequential learning requires shallow trees to avoid overfitting in early iterations\n",
    "   - **Bagging (RF)**: Parallel independent trees can be deeper since averaging reduces variance\n",
    "   - This is a fundamental difference in how these ensemble methods work\n",
    "\n",
    "**Next Step:** Evaluate these tuned models on the held-out test set to determine the true winner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c871f85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Final Model Evaluation on Test Set\n",
    "\n",
    "Now we evaluate the tuned models on the **held-out test set** (never seen during training/tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b320975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all tuned models on test set\n",
    "print(\"Final Model Evaluation (Test Set):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_results = []\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_processed)\n",
    "    y_test_pred = model.predict(X_test_processed)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'Model': name,\n",
    "        'R¬≤_train': train_r2,\n",
    "        'R¬≤_test': test_r2,\n",
    "        'RMSE_test': test_rmse,\n",
    "        'MAE_test': test_mae,\n",
    "        'Overfit_Gap': train_r2 - test_r2\n",
    "    }\n",
    "    final_results.append(result)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train R¬≤:  {train_r2:.4f}\")\n",
    "    print(f\"  Test R¬≤:   {test_r2:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Test MAE:  {test_mae:.4f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    if train_r2 - test_r2 > 0.05:\n",
    "        print(f\"  ‚ö†Ô∏è Potential overfitting (gap: {train_r2 - test_r2:.3f})\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "results_df = pd.DataFrame(final_results).sort_values('R¬≤_test', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c777f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = tuned_models[best_model_name]\n",
    "best_r2 = results_df.iloc[0]['R¬≤_test']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Test R¬≤:   {best_r2:.4f}\")\n",
    "print(f\"   Test RMSE: {results_df.iloc[0]['RMSE_test']:.4f}\")\n",
    "print(f\"   Test MAE:  {results_df.iloc[0]['MAE_test']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train vs Test R¬≤ comparison visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.barh(x - width/2, results_df['R¬≤_train'], width, label='Train R¬≤', color=MAIN_COLOR)\n",
    "bars2 = ax.barh(x + width/2, results_df['R¬≤_test'], width, label='Test R¬≤', color=SECONDARY_COLOR)\n",
    "\n",
    "ax.set_xlabel('R¬≤ Score')\n",
    "ax.set_title('Train vs Test R¬≤ by Model (Check for Overfitting)')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(results_df['Model'])\n",
    "ax.legend()\n",
    "ax.axvline(0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81cba8",
   "metadata": {},
   "source": [
    "### üèÜ Final Model Evaluation Analysis\n",
    "\n",
    "#### Why RandomForest Emerged as the Best Model\n",
    "\n",
    "| Model | CV R¬≤ (Training) | Test R¬≤ | Overfit Gap | Verdict |\n",
    "|-------|------------------|---------|-------------|---------|\n",
    "| **RandomForest** | 0.9495 | **0.9703** | 0.0208 | üèÜ Best Test Performance |\n",
    "| XGBoost | 0.9534 | 0.9691 | 0.0142 | ü•à Lowest Overfitting |\n",
    "| GradientBoosting | 0.9508 | 0.9655 | 0.0191 | ü•â Solid Performance |\n",
    "\n",
    "**Critical Insight: CV Score ‚â† Test Score**\n",
    "\n",
    "RandomForest had the **lowest CV score** (0.9495) during tuning, yet achieved the **highest test score** (0.9703)! Here's why:\n",
    "\n",
    "1. **RandomForest Generalizes Better**\n",
    "   - RF uses bagging (bootstrap aggregating) which inherently reduces variance\n",
    "   - Each tree sees a different random subset of data AND features\n",
    "   - This double randomness creates a more robust model that generalizes to unseen data\n",
    "   - Test R¬≤ improvement: +2.08% over CV (0.9703 vs 0.9495)\n",
    "\n",
    "2. **XGBoost's Slight Overfitting**\n",
    "   - Best CV score (0.9534) but only 0.9691 on test\n",
    "   - Boosting methods can memorize training patterns more aggressively\n",
    "   - Even with regularization, XGBoost slightly overfits compared to RF\n",
    "\n",
    "3. **The Overfitting Gap Tells a Story**\n",
    "   - All models show Train R¬≤ > Test R¬≤ (expected behavior)\n",
    "   - RF: Train 0.9911, Test 0.9703 ‚Üí Gap 0.0208 (acceptable)\n",
    "   - XGB: Train 0.9834, Test 0.9691 ‚Üí Gap 0.0142 (best generalization gap)\n",
    "   - GB: Train 0.9846, Test 0.9655 ‚Üí Gap 0.0191 (middle ground)\n",
    "\n",
    "4. **Absolute Performance Metrics**\n",
    "   - RandomForest Test RMSE: 0.2449 (lowest error)\n",
    "   - RandomForest Test MAE: 0.1709 (lowest mean absolute error)\n",
    "   - These log-scale metrics translate to ~$212 and ~$113 in dollars\n",
    "\n",
    "**Why This Matters for Business:**\n",
    "- A model that performs well on unseen data will make better predictions on new customers\n",
    "- RandomForest's robustness makes it safer for production deployment\n",
    "- The 97.03% variance explained means we capture nearly all predictable spending patterns\n",
    "\n",
    "**Selected Model: RandomForest** with the understanding that XGBoost could be a viable alternative if we needed slightly better interpretability through feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321ccb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Best Model Analysis\n",
    "\n",
    "### 7.1 Actual vs Predicted Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5495ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with best model\n",
    "y_test_pred = best_model.predict(X_test_processed)\n",
    "\n",
    "# Actual vs Predicted plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Scatter plot\n",
    "axes[0].scatter(y_test, y_test_pred, alpha=0.5, color=MAIN_COLOR)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual TotalSpend_log')\n",
    "axes[0].set_ylabel('Predicted TotalSpend_log')\n",
    "axes[0].set_title(f'{best_model_name}: Actual vs Predicted')\n",
    "axes[0].legend()\n",
    "\n",
    "# Add R¬≤ annotation\n",
    "axes[0].text(0.05, 0.95, f'R¬≤ = {best_r2:.4f}', transform=axes[0].transAxes, \n",
    "             fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "# Right: Distribution comparison\n",
    "axes[1].hist(y_test, bins=30, alpha=0.5, label='Actual', color=MAIN_COLOR)\n",
    "axes[1].hist(y_test_pred, bins=30, alpha=0.5, label='Predicted', color=SECONDARY_COLOR)\n",
    "axes[1].set_xlabel('TotalSpend_log')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution: Actual vs Predicted')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0].scatter(y_test_pred, residuals, alpha=0.5, color=MAIN_COLOR)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residuals vs Predicted')\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(residuals, bins=30, color=MAIN_COLOR, edgecolor='white')\n",
    "axes[1].axvline(0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'Residual Distribution (mean={residuals.mean():.4f})')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual Statistics:\")\n",
    "print(f\"  Mean:     {residuals.mean():.4f} (should be ~0)\")\n",
    "print(f\"  Std:      {residuals.std():.4f}\")\n",
    "print(f\"  Skewness: {stats.skew(residuals):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a318a",
   "metadata": {},
   "source": [
    "### üìä Residual Analysis Interpretation\n",
    "\n",
    "#### What the Residual Plots Tell Us\n",
    "\n",
    "**1. Residuals vs Predicted (Left Plot)**\n",
    "- **Random scatter around 0:** ‚úì Good ‚Äî no systematic prediction bias\n",
    "- **Constant spread:** ‚úì Homoscedasticity ‚Äî error variance is relatively constant\n",
    "- **No patterns:** ‚úì Model captures non-linear relationships adequately\n",
    "- **Slight funnel at high predictions:** Minor heteroscedasticity at extremes (expected for log-transformed data)\n",
    "\n",
    "**2. Residual Distribution (Center Plot)**\n",
    "- **Centered at ~0:** ‚úì Mean residual ‚âà 0 ‚Äî unbiased predictions\n",
    "- **Approximately symmetric:** ‚úì Errors are balanced (no systematic over/under-prediction)\n",
    "- **Near-normal shape:** ‚úì Supports validity of standard error estimates\n",
    "- **Slight left tail:** Some customers are predicted higher than actual (model occasionally overestimates)\n",
    "\n",
    "**3. Q-Q Plot (Right Plot)**\n",
    "- **Points follow diagonal:** ‚úì Residuals are approximately normally distributed\n",
    "- **Slight deviation at tails:** Expected for real-world data ‚Äî extreme values deviate from normality\n",
    "- **No severe S-curves:** ‚úì No major distributional issues\n",
    "\n",
    "#### Residual Statistics\n",
    "\n",
    "| Metric | Value | Assessment |\n",
    "|--------|-------|------------|\n",
    "| Mean | ~0.00 | ‚úì Unbiased predictions |\n",
    "| Std Dev | ~0.24 | Typical error magnitude on log scale |\n",
    "| Skewness | ~-0.3 | Slight left skew (minor) |\n",
    "\n",
    "**Key Takeaway:**\n",
    "The residual diagnostics confirm our model is well-specified:\n",
    "- No systematic prediction errors\n",
    "- Error variance is stable\n",
    "- No major violations of regression assumptions\n",
    "\n",
    "This gives confidence that our R¬≤ of 0.9703 is a reliable measure of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cdf07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance = best_model.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    top_n = min(15, len(importance_df))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = [MAIN_COLOR if i < 5 else SECONDARY_COLOR for i in range(top_n)]\n",
    "    plt.barh(range(top_n), top_features['importance'].values[::-1], color=colors[::-1])\n",
    "    plt.yticks(range(top_n), top_features['feature'].values[::-1])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'{best_model_name} ‚Äî Top {top_n} Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(f\"{best_model_name} doesn't have feature_importances_ attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f9977",
   "metadata": {},
   "source": [
    "### üéØ Feature Importance Deep Dive\n",
    "\n",
    "#### Interpreting the Feature Hierarchy\n",
    "\n",
    "| Rank | Feature | Importance | Interpretation |\n",
    "|------|---------|------------|----------------|\n",
    "| 1 | **NumCatalogPurchases** | 70.6% | Dominant predictor ‚Äî catalog buyers are high-value customers |\n",
    "| 2 | NumWebPurchases | 15.3% | Second strongest purchase channel signal |\n",
    "| 3 | NumStorePurchases | 4.9% | In-store purchases indicate engaged customers |\n",
    "| 4 | Income | 4.8% | Purchasing power matters, but less than behavior |\n",
    "| 5 | IncomePerCapita | 1.8% | Per-person income adds incremental signal |\n",
    "| 6-10 | Tenure, WebVisits, Recency, Age, Deals | <1% each | Minor contributors |\n",
    "\n",
    "**Critical Business Insights:**\n",
    "\n",
    "1. **Purchase Behavior Dominates Over Demographics**\n",
    "   - Top 3 features (92% importance) are all **behavioral** (purchase channels)\n",
    "   - Income ranks only 4th despite strong correlation (r=0.67 from EDA)\n",
    "   - **Implication:** What customers DO matters more than who they ARE\n",
    "\n",
    "2. **Why NumCatalogPurchases is Dominant (70.6%)**\n",
    "   - Catalog purchases indicate:\n",
    "     - High engagement (customers browse physical catalogs)\n",
    "     - Higher average order value (catalog items often premium)\n",
    "     - Stronger brand loyalty (willing to order without in-store inspection)\n",
    "   - This single feature explains most of the variance in spending\n",
    "   - **Caveat:** High importance may indicate near-target leakage ‚Äî catalog purchases likely correlate strongly with spending categories\n",
    "\n",
    "3. **The Purchase Channel Hierarchy**\n",
    "   - Catalog (70.6%) > Web (15.3%) > Store (4.9%) > Deals (0.2%)\n",
    "   - Deal purchases have minimal importance ‚Äî discount shoppers don't drive total spending\n",
    "   - Web purchases show emerging importance of digital channel\n",
    "\n",
    "4. **Income Features Combined = 6.6%**\n",
    "   - `Income` (4.8%) + `IncomePerCapita` (1.8%) = 6.6% combined\n",
    "   - While EDA showed Income as top correlator, trees find purchase behavior more predictive\n",
    "   - This is because purchase behavior is *downstream* of income (mediates the relationship)\n",
    "\n",
    "5. **Feature Engineering Validation**\n",
    "   - `IncomePerCapita` (our engineered feature) ranks 5th ‚Äî adds value beyond raw Income\n",
    "   - `Tenure_Days` (0.8%) captures customer lifetime\n",
    "   - `HasChildren`, `Education`, `Marital_Status` have minimal direct importance (though may interact with other features)\n",
    "\n",
    "**Model Interpretation Caveat:**\n",
    "RandomForest feature importance uses **Mean Decrease in Impurity (MDI)**. This can overestimate importance of:\n",
    "- High-cardinality features\n",
    "- Features with many splits\n",
    "\n",
    "For production, consider SHAP values for more accurate, instance-level explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients (for linear models)\n",
    "if hasattr(best_model, 'coef_'):\n",
    "    coef_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': best_model.coef_\n",
    "    })\n",
    "    coef_df['abs_coef'] = coef_df['coefficient'].abs()\n",
    "    coef_df = coef_df.sort_values('abs_coef', ascending=False)\n",
    "    \n",
    "    # Plot top 15 coefficients\n",
    "    top_n = min(15, len(coef_df))\n",
    "    top_coefs = coef_df.head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = [MAIN_COLOR if c > 0 else ACCENT_COLOR for c in top_coefs['coefficient']]\n",
    "    plt.barh(range(top_n), top_coefs['coefficient'].values[::-1], color=colors[::-1])\n",
    "    plt.yticks(range(top_n), top_coefs['feature'].values[::-1])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.axvline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "    plt.title(f'{best_model_name} ‚Äî Top {top_n} Coefficients (by magnitude)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTop 10 Coefficients (by magnitude):\")\n",
    "    print(coef_df[['feature', 'coefficient']].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(f\"{best_model_name} doesn't have coef_ attribute (not a linear model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d9954",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Learning Curves (Bias-Variance Analysis)\n",
    "\n",
    "Learning curves help diagnose whether the model suffers from:\n",
    "- **High Bias** (underfitting): Both curves plateau at low score\n",
    "- **High Variance** (overfitting): Large gap between train and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute learning curves\n",
    "print(f\"Computing learning curves for {best_model_name}...\")\n",
    "\n",
    "# Clone model for fresh training\n",
    "model_clone = clone(best_model)\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    model_clone,\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)\n",
    "val_std = val_scores.std(axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=MAIN_COLOR)\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color=SECONDARY_COLOR)\n",
    "plt.plot(train_sizes, train_mean, 'o-', color=MAIN_COLOR, label='Training Score')\n",
    "plt.plot(train_sizes, val_mean, 'o-', color=SECONDARY_COLOR, label='Validation Score')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title(f'{best_model_name} ‚Äî Learning Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Diagnosis\n",
    "gap = train_mean[-1] - val_mean[-1]\n",
    "print(f\"\\nDiagnosis:\")\n",
    "print(f\"  Final train score: {train_mean[-1]:.4f}\")\n",
    "print(f\"  Final validation score: {val_mean[-1]:.4f}\")\n",
    "print(f\"  Gap: {gap:.4f}\")\n",
    "\n",
    "if gap > 0.1:\n",
    "    print(\"  ‚ö†Ô∏è High variance (overfitting) - consider more regularization or simpler model\")\n",
    "elif val_mean[-1] < 0.5:\n",
    "    print(\"  ‚ö†Ô∏è High bias (underfitting) - consider more features or complex model\")\n",
    "else:\n",
    "    print(\"  ‚úì Model appears well-balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a8918b",
   "metadata": {},
   "source": [
    "### üìà Learning Curves Diagnosis\n",
    "\n",
    "#### Understanding the Bias-Variance Trade-off\n",
    "\n",
    "The learning curves reveal crucial information about our model's behavior:\n",
    "\n",
    "**What the Curves Show:**\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Final Training Score | 0.9912 | Model fits training data nearly perfectly |\n",
    "| Final Validation Score | 0.9495 | Strong generalization to unseen folds |\n",
    "| Gap | 0.0417 | Acceptable variance (< 0.1 threshold) |\n",
    "| Convergence | ~800 samples | Validation score plateaus after 800 samples |\n",
    "\n",
    "**Detailed Analysis:**\n",
    "\n",
    "1. **Model is Well-Balanced ‚úì**\n",
    "   - Gap of 0.0417 is below the 0.1 \"high variance\" threshold\n",
    "   - Both curves plateau (no signs of underfitting)\n",
    "   - Validation score ~0.95 indicates strong generalization\n",
    "\n",
    "2. **Training Curve Characteristics**\n",
    "   - Starts high (~0.98) and remains flat\n",
    "   - This is typical for RandomForest ‚Äî trees can fit any training data\n",
    "   - High training score is expected and not concerning\n",
    "\n",
    "3. **Validation Curve Characteristics**\n",
    "   - Starts lower (~0.90 at 100 samples) and climbs to ~0.95\n",
    "   - **Upward trend** indicates the model benefits from more data\n",
    "   - Curve is still slightly rising at 1,400 samples ‚Äî more data could help marginally\n",
    "\n",
    "4. **What This Tells Us About Data Size**\n",
    "   - Diminishing returns after ~800 samples\n",
    "   - Our 1,792 training samples are adequate for this problem\n",
    "   - The validation curve's small upward slope suggests 3-5K samples might add ~1% performance\n",
    "\n",
    "5. **Shaded Confidence Bands**\n",
    "   - Training: Very tight bands ‚Äî consistent performance across CV folds\n",
    "   - Validation: Wider bands ‚Äî more variance in out-of-fold predictions\n",
    "   - This is normal; training variance is always lower than validation variance\n",
    "\n",
    "6. **No Signs of Pathological Behavior**\n",
    "   - ‚ùå High Bias (underfitting): Would show both curves at low scores\n",
    "   - ‚ùå High Variance (overfitting): Would show large gap (>0.1)\n",
    "   - ‚úÖ Good Fit: Training high, validation reasonably close\n",
    "\n",
    "**Conclusion:**\n",
    "The RandomForest model achieves an excellent balance between fitting the training data and generalizing to new data. The ~4% gap between training and validation is typical for tree-based ensembles and represents acceptable variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f20e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.5 Advanced Model Analysis\n",
    "\n",
    "### 8.5.1 SHAP Values ‚Äî Explainable AI\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** provides consistent, theoretically-grounded feature attributions that explain:\n",
    "- **Global importance:** Which features matter most overall\n",
    "- **Local explanations:** Why each individual prediction was made\n",
    "- **Feature interactions:** How features work together\n",
    "\n",
    "Unlike RandomForest's built-in feature importance (MDI), SHAP values:\n",
    "- Are based on game theory (Shapley values)\n",
    "- Account for feature correlations\n",
    "- Provide directional impact (positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP if not available\n",
    "# Note: SHAP requires compatible NumPy version. If installation fails, we'll use permutation importance instead.\n",
    "HAS_SHAP = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "    print(f\"‚úì SHAP version: {shap.__version__}\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        print(\"Installing SHAP...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call(['pip', 'install', 'shap', '-q'])\n",
    "        import shap\n",
    "        HAS_SHAP = True\n",
    "        print(f\"‚úì SHAP version: {shap.__version__}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è SHAP installation failed: {e}\")\n",
    "        print(\"   This is often due to NumPy version incompatibility.\")\n",
    "        print(\"   We'll use Permutation Importance as an alternative (equally valid for feature analysis).\")\n",
    "        HAS_SHAP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP or Permutation Importance Analysis\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "if HAS_SHAP:\n",
    "    # Calculate SHAP values for the best model\n",
    "    print(f\"Computing SHAP values for {best_model_name}...\")\n",
    "    print(\"This may take a minute for tree-based models...\\n\")\n",
    "\n",
    "    # Use TreeExplainer for RandomForest (much faster than KernelExplainer)\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "    # Calculate SHAP values for test set (use subset for speed if needed)\n",
    "    sample_size = min(200, len(X_test_processed))\n",
    "    X_sample = X_test_processed[:sample_size]\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "    print(f\"‚úì SHAP values computed for {sample_size} samples\")\n",
    "    print(f\"  Shape: {shap_values.shape}\")\n",
    "    print(f\"  Expected value (base prediction): {explainer.expected_value:.4f}\")\n",
    "else:\n",
    "    # Use Permutation Importance as alternative\n",
    "    print(f\"Computing Permutation Importance for {best_model_name}...\")\n",
    "    print(\"This provides similar insights to SHAP for global feature importance.\\n\")\n",
    "    \n",
    "    perm_importance = permutation_importance(\n",
    "        best_model, X_test_processed, y_test, \n",
    "        n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame for easy analysis\n",
    "    perm_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance_mean': perm_importance.importances_mean,\n",
    "        'importance_std': perm_importance.importances_std\n",
    "    }).sort_values('importance_mean', ascending=False)\n",
    "    \n",
    "    print(f\"‚úì Permutation importance computed\")\n",
    "    print(f\"  Method: Shuffle each feature and measure performance drop\")\n",
    "    print(f\"  Repeats: 10 (for stable estimates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0209e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Visualization\n",
    "if HAS_SHAP:\n",
    "    # SHAP Summary Plot ‚Äî Global Feature Importance with Direction\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)\n",
    "    plt.title(f'{best_model_name} ‚Äî SHAP Summary Plot', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # SHAP Bar Plot ‚Äî Mean Absolute SHAP Values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, plot_type=\"bar\", show=False)\n",
    "    plt.title(f'{best_model_name} ‚Äî Mean |SHAP Value| (Global Importance)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # Permutation Importance Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left: Top 15 features by permutation importance\n",
    "    top_15 = perm_importance_df.head(15)\n",
    "    colors = [MAIN_COLOR if i < 5 else SECONDARY_COLOR for i in range(len(top_15))]\n",
    "    \n",
    "    axes[0].barh(range(len(top_15)), top_15['importance_mean'].values[::-1], \n",
    "                 xerr=top_15['importance_std'].values[::-1], color=colors[::-1], capsize=3)\n",
    "    axes[0].set_yticks(range(len(top_15)))\n",
    "    axes[0].set_yticklabels(top_15['feature'].values[::-1])\n",
    "    axes[0].set_xlabel('Mean Importance (R¬≤ decrease when shuffled)')\n",
    "    axes[0].set_title(f'{best_model_name} ‚Äî Permutation Importance (Top 15)')\n",
    "    \n",
    "    # Right: Compare MDI vs Permutation Importance\n",
    "    comparison_df = importance_df.merge(perm_importance_df, on='feature', how='inner').head(10)\n",
    "    \n",
    "    x = np.arange(len(comparison_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1].barh(x - width/2, comparison_df['importance'], width, label='MDI (Built-in)', color=MAIN_COLOR)\n",
    "    axes[1].barh(x + width/2, comparison_df['importance_mean'], width, label='Permutation', color=SECONDARY_COLOR)\n",
    "    axes[1].set_yticks(x)\n",
    "    axes[1].set_yticklabels(comparison_df['feature'])\n",
    "    axes[1].set_xlabel('Importance Score')\n",
    "    axes[1].set_title('MDI vs Permutation Importance Comparison')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Features by Permutation Importance:\")\n",
    "    print(perm_importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17622749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Dependence Analysis\n",
    "if HAS_SHAP:\n",
    "    # SHAP Dependence Plots ‚Äî Top 4 Features\n",
    "    top_4_features = importance_df.head(4)['feature'].tolist()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(top_4_features):\n",
    "        feature_idx = feature_names.index(feature)\n",
    "        shap.dependence_plot(\n",
    "            feature_idx, shap_values, X_sample, \n",
    "            feature_names=feature_names,\n",
    "            ax=axes[idx], show=False\n",
    "        )\n",
    "        axes[idx].set_title(f'SHAP Dependence: {feature}')\n",
    "\n",
    "    plt.suptitle('Feature Impact on Predictions (SHAP Dependence Plots)', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # Partial Dependence Plots for top features\n",
    "    from sklearn.inspection import PartialDependenceDisplay\n",
    "    \n",
    "    print(\"Partial Dependence Plots ‚Äî How each feature affects predictions\\n\")\n",
    "    \n",
    "    top_4_features = importance_df.head(4)['feature'].tolist()\n",
    "    top_4_indices = [feature_names.index(f) for f in top_4_features]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    for idx, (feature, feature_idx) in enumerate(zip(top_4_features, top_4_indices)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        # Manual partial dependence calculation\n",
    "        feature_values = np.linspace(\n",
    "            X_test_processed[:, feature_idx].min(),\n",
    "            X_test_processed[:, feature_idx].max(),\n",
    "            50\n",
    "        )\n",
    "        \n",
    "        pdp_values = []\n",
    "        for val in feature_values:\n",
    "            X_temp = X_test_processed.copy()\n",
    "            X_temp[:, feature_idx] = val\n",
    "            pdp_values.append(best_model.predict(X_temp).mean())\n",
    "        \n",
    "        ax.plot(feature_values, pdp_values, color=MAIN_COLOR, linewidth=2)\n",
    "        ax.fill_between(feature_values, pdp_values, alpha=0.3, color=MAIN_COLOR)\n",
    "        ax.set_xlabel(f'{feature} (standardized)')\n",
    "        ax.set_ylabel('Predicted TotalSpend_log')\n",
    "        ax.set_title(f'Partial Dependence: {feature}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Partial Dependence Plots ‚Äî Feature Effects on Predictions', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local/Individual Prediction Analysis\n",
    "if HAS_SHAP:\n",
    "    # Local Explanations ‚Äî Individual Predictions (Waterfall Plots)\n",
    "    print(\"Local Explanations ‚Äî Why did the model make these specific predictions?\\n\")\n",
    "\n",
    "    # Select interesting samples: low, medium, high spenders\n",
    "    y_sample = y_test.iloc[:sample_size]\n",
    "    low_idx = y_sample.argmin()\n",
    "    high_idx = y_sample.argmax()\n",
    "    median_idx = (y_sample - y_sample.median()).abs().argmin()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for idx, (sample_idx, label) in enumerate([(low_idx, 'Low Spender'), (median_idx, 'Median Spender'), (high_idx, 'High Spender')]):\n",
    "        plt.subplot(1, 3, idx + 1)\n",
    "        shap.waterfall_plot(\n",
    "            shap.Explanation(\n",
    "                values=shap_values[sample_idx],\n",
    "                base_values=explainer.expected_value,\n",
    "                data=X_sample[sample_idx],\n",
    "                feature_names=feature_names\n",
    "            ),\n",
    "            max_display=10,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f'{label}\\nActual: {np.expm1(y_sample.iloc[sample_idx]):.0f}, Pred: {np.expm1(best_model.predict(X_sample[sample_idx:sample_idx+1])[0]):.0f}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    # Individual Prediction Breakdown using feature contributions\n",
    "    print(\"Individual Prediction Analysis ‚Äî Examining specific customers\\n\")\n",
    "    \n",
    "    # Select interesting samples\n",
    "    low_idx = y_test.argmin()\n",
    "    high_idx = y_test.argmax()\n",
    "    median_idx = (y_test - y_test.median()).abs().argmin()\n",
    "    \n",
    "    sample_indices = [low_idx, median_idx, high_idx]\n",
    "    sample_labels = ['Low Spender', 'Median Spender', 'High Spender']\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    for idx, label in zip(sample_indices, sample_labels):\n",
    "        actual = np.expm1(y_test.iloc[idx])\n",
    "        predicted = np.expm1(best_model.predict(X_test_processed[idx:idx+1])[0])\n",
    "        \n",
    "        # Get tree predictions distribution for this sample\n",
    "        tree_preds = [tree.predict(X_test_processed[idx:idx+1])[0] for tree in best_model.estimators_]\n",
    "        pred_std = np.std(tree_preds)\n",
    "        \n",
    "        print(f\"\\n{label}:\")\n",
    "        print(f\"  Actual spending: ${actual:,.0f}\")\n",
    "        print(f\"  Predicted spending: ${predicted:,.0f} (¬±${np.expm1(pred_std):,.0f})\")\n",
    "        print(f\"  Prediction error: ${predicted - actual:+,.0f}\")\n",
    "        \n",
    "        # Show top feature values for this customer\n",
    "        print(f\"  Top feature values (standardized):\")\n",
    "        for feat in importance_df.head(5)['feature']:\n",
    "            feat_idx = feature_names.index(feat)\n",
    "            print(f\"    {feat}: {X_test_processed[idx, feat_idx]:.2f}\")\n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6453889",
   "metadata": {},
   "source": [
    "### üìä Feature Importance Analysis Interpretation\n",
    "\n",
    "#### Understanding the Methods\n",
    "\n",
    "| Method | Description | Strengths | Limitations |\n",
    "|--------|-------------|-----------|-------------|\n",
    "| **MDI (Built-in)** | Mean Decrease in Impurity | Fast, no retraining | Can overestimate high-cardinality features |\n",
    "| **Permutation** | Shuffle feature, measure R¬≤ drop | Accounts for correlations | Computationally expensive |\n",
    "| **SHAP** | Game-theoretic attribution | Consistent, local explanations | Requires compatible dependencies |\n",
    "| **Partial Dependence** | Average effect of feature | Shows non-linear relationships | Assumes feature independence |\n",
    "\n",
    "#### Key Findings\n",
    "\n",
    "1. **NumCatalogPurchases Dominates** ‚Äî Consistently ranked #1 across all methods\n",
    "   - High values strongly push predictions UP\n",
    "   - This is a strong behavioral signal of engaged, high-value customers\n",
    "\n",
    "2. **Income vs Purchase Behavior**\n",
    "   - Purchase channels (Catalog, Web, Store) collectively outweigh Income\n",
    "   - **Action > Demographics**: What customers DO matters more than who they ARE\n",
    "\n",
    "3. **Directional Insights** (from dependence/partial plots)\n",
    "   - Higher catalog purchases ‚Üí higher spending (strong positive)\n",
    "   - Higher income ‚Üí higher spending (positive, but with diminishing returns)\n",
    "   - More web visits ‚Üí mixed signal (browsers vs buyers)\n",
    "\n",
    "4. **Individual Variation**\n",
    "   - Low spenders: Low activity across all purchase channels\n",
    "   - High spenders: High catalog activity, above-average income\n",
    "   - The model captures these patterns well\n",
    "\n",
    "#### Business Implications\n",
    "\n",
    "- **Targeting**: Prioritize customers with high catalog engagement\n",
    "- **Acquisition**: Look for catalog-responsive leads\n",
    "- **Retention**: Catalog channel is key for high-value customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001154e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.5.2 Feature-wise Cross-Validation Analysis\n",
    "\n",
    "This analysis shows how each feature contributes to model performance by:\n",
    "1. **Ablation Study:** Performance without each feature (drop-one-out)\n",
    "2. **Single Feature Performance:** How well each feature predicts alone\n",
    "3. **Cumulative Feature Addition:** Performance as features are added in importance order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aeaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Ablation Study ‚Äî Drop-one-out analysis\n",
    "print(\"Feature Ablation Study (Drop-One-Out)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"How much does performance DROP when we remove each feature?\\n\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Baseline score with all features\n",
    "baseline_cv = cross_val_score(\n",
    "    RandomForestRegressor(n_estimators=100, max_depth=15, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    X_train_processed, y_train, cv=3, scoring='r2'\n",
    ").mean()\n",
    "\n",
    "ablation_results = []\n",
    "top_10_features = importance_df.head(10)['feature'].tolist()\n",
    "\n",
    "for feature in top_10_features:\n",
    "    feature_idx = feature_names.index(feature)\n",
    "    # Create copy without this feature\n",
    "    X_ablated = np.delete(X_train_processed, feature_idx, axis=1)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    cv_score = cross_val_score(\n",
    "        RandomForestRegressor(n_estimators=100, max_depth=15, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "        X_ablated, y_train, cv=3, scoring='r2'\n",
    "    ).mean()\n",
    "    \n",
    "    drop = baseline_cv - cv_score\n",
    "    ablation_results.append({\n",
    "        'Feature': feature,\n",
    "        'CV_R2_without': cv_score,\n",
    "        'Performance_Drop': drop,\n",
    "        'Drop_Pct': drop / baseline_cv * 100\n",
    "    })\n",
    "    print(f\"Without {feature:25} ‚Üí R¬≤ = {cv_score:.4f} (drop: {drop:+.4f}, {drop/baseline_cv*100:+.1f}%)\")\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results).sort_values('Performance_Drop', ascending=False)\n",
    "print(f\"\\nBaseline R¬≤ (all features): {baseline_cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc019c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ablation results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = [MAIN_COLOR if drop > 0.01 else SECONDARY_COLOR for drop in ablation_df['Performance_Drop']]\n",
    "bars = ax.barh(ablation_df['Feature'], ablation_df['Performance_Drop'], color=colors)\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Performance Drop (R¬≤ reduction when feature removed)')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.set_title('Feature Ablation Study ‚Äî Impact of Removing Each Feature')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, ablation_df['Performance_Drop']):\n",
    "    ax.text(val + 0.001, bar.get_y() + bar.get_height()/2, f'{val:.4f}', \n",
    "            va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîë Key Insight: Features with largest drops are truly essential.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e864eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Feature Addition ‚Äî How performance grows as features are added\n",
    "print(\"Cumulative Feature Addition (by importance order)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Performance as we add features one by one...\\n\")\n",
    "\n",
    "# Get features in importance order\n",
    "ordered_features = importance_df['feature'].tolist()\n",
    "ordered_indices = [feature_names.index(f) for f in ordered_features]\n",
    "\n",
    "cumulative_results = []\n",
    "\n",
    "for n_features in range(1, min(16, len(ordered_features) + 1)):\n",
    "    # Select top n features\n",
    "    selected_indices = ordered_indices[:n_features]\n",
    "    X_subset = X_train_processed[:, selected_indices]\n",
    "    \n",
    "    # Train and evaluate\n",
    "    cv_score = cross_val_score(\n",
    "        RandomForestRegressor(n_estimators=100, max_depth=15, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "        X_subset, y_train, cv=3, scoring='r2'\n",
    "    ).mean()\n",
    "    \n",
    "    cumulative_results.append({\n",
    "        'N_Features': n_features,\n",
    "        'Top_Feature_Added': ordered_features[n_features - 1],\n",
    "        'CV_R2': cv_score\n",
    "    })\n",
    "    print(f\"{n_features:2} features ‚Üí R¬≤ = {cv_score:.4f} (added: {ordered_features[n_features - 1]})\")\n",
    "\n",
    "cumulative_df = pd.DataFrame(cumulative_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cumulative feature addition\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(cumulative_df['N_Features'], cumulative_df['CV_R2'], 'o-', color=MAIN_COLOR, linewidth=2, markersize=8)\n",
    "ax.fill_between(cumulative_df['N_Features'], cumulative_df['CV_R2'], alpha=0.3, color=MAIN_COLOR)\n",
    "ax.axhline(baseline_cv, color='red', linestyle='--', label=f'All Features ({baseline_cv:.4f})')\n",
    "\n",
    "ax.set_xlabel('Number of Features (added in importance order)')\n",
    "ax.set_ylabel('Cross-Validation R¬≤')\n",
    "ax.set_title('Cumulative Feature Addition ‚Äî Performance vs Feature Count')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for key points\n",
    "ax.annotate(f'Top 1: {cumulative_df.iloc[0][\"CV_R2\"]:.3f}', \n",
    "            xy=(1, cumulative_df.iloc[0]['CV_R2']), \n",
    "            xytext=(2, cumulative_df.iloc[0]['CV_R2'] - 0.03),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "top_5_score = cumulative_df.iloc[4]['CV_R2']\n",
    "ax.annotate(f'Top 5: {top_5_score:.3f}', \n",
    "            xy=(5, top_5_score), \n",
    "            xytext=(6, top_5_score - 0.02),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate marginal contribution\n",
    "print(f\"\\nüìà Feature Efficiency Analysis:\")\n",
    "print(f\"  Top 1 feature alone: {cumulative_df.iloc[0]['CV_R2']:.4f} ({cumulative_df.iloc[0]['CV_R2']/baseline_cv*100:.1f}% of full performance)\")\n",
    "print(f\"  Top 3 features: {cumulative_df.iloc[2]['CV_R2']:.4f} ({cumulative_df.iloc[2]['CV_R2']/baseline_cv*100:.1f}% of full performance)\")\n",
    "print(f\"  Top 5 features: {cumulative_df.iloc[4]['CV_R2']:.4f} ({cumulative_df.iloc[4]['CV_R2']/baseline_cv*100:.1f}% of full performance)\")\n",
    "print(f\"  All {len(feature_names)} features: {baseline_cv:.4f} (100%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e60e6",
   "metadata": {},
   "source": [
    "### üìä Feature CV Analysis Interpretation\n",
    "\n",
    "#### Ablation Study Results ‚Äî Drop-One-Out Impact\n",
    "\n",
    "| Feature Removed | New R¬≤ | Performance Drop | % of Total |\n",
    "|-----------------|--------|------------------|------------|\n",
    "| **NumCatalogPurchases** | 0.9220 | **-3.2%** | Critical |\n",
    "| NumWebPurchases | 0.9360 | -1.7% | Important |\n",
    "| NumStorePurchases | 0.9373 | -1.6% | Important |\n",
    "| Income | 0.9511 | -0.1% | Redundant |\n",
    "| IncomePerCapita | 0.9515 | -0.1% | Redundant |\n",
    "| NumWebVisitsMonth | 0.9503 | -0.2% | Minor |\n",
    "| Age, Recency, etc. | ~0.95 | ~0% | Negligible |\n",
    "\n",
    "**Key Insight:** Only 3 features (purchase channels) cause meaningful performance drops. The rest are captured by correlated features.\n",
    "\n",
    "#### Cumulative Addition Results ‚Äî How Many Features Do We Need?\n",
    "\n",
    "| # Features | R¬≤ Score | % of Full Performance | Features Included |\n",
    "|------------|----------|----------------------|-------------------|\n",
    "| 1 | 0.8081 | 84.8% | NumCatalogPurchases |\n",
    "| 2 | 0.9065 | 95.2% | + NumWebPurchases |\n",
    "| 3 | 0.9360 | 98.3% | + NumStorePurchases |\n",
    "| 4 | 0.9493 | 99.7% | + Income |\n",
    "| **5** | **0.9524** | **100.0%** | **+ IncomePerCapita** |\n",
    "| 22 | 0.9525 | 100.0% | All features |\n",
    "\n",
    "**Critical Finding:** **Top 3 features achieve 98.3% of full performance!**\n",
    "\n",
    "#### Business Implications\n",
    "\n",
    "1. **Model Simplification Opportunity**\n",
    "   - Could deploy a 3-5 feature model with minimal accuracy loss\n",
    "   - Simpler models = faster predictions, easier maintenance\n",
    "   - Consider for real-time scoring applications\n",
    "\n",
    "2. **Feature Collection Priority**\n",
    "   - **Must have:** Purchase channel data (Catalog, Web, Store)\n",
    "   - **Nice to have:** Income data\n",
    "   - **Optional:** Demographics, tenure, recency\n",
    "\n",
    "3. **Data Quality Focus**\n",
    "   - Invest in accurate purchase tracking\n",
    "   - Catalog purchase data is most valuable\n",
    "   - Missing purchase data will significantly hurt predictions\n",
    "\n",
    "4. **Feature Engineering Insight**\n",
    "   - `IncomePerCapita` adds marginal value beyond raw `Income`\n",
    "   - Many demographic features are redundant\n",
    "   - Purchase behavior is far more predictive than demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b7db6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.5.3 Prediction Intervals ‚Äî Uncertainty Quantification\n",
    "\n",
    "Point predictions alone don't communicate **uncertainty**. Prediction intervals provide:\n",
    "- **Confidence bounds** for each prediction\n",
    "- **Risk assessment** for business decisions\n",
    "- **Calibration check** ‚Äî do X% intervals contain X% of true values?\n",
    "\n",
    "For RandomForest, we can estimate prediction intervals using:\n",
    "1. **Quantile Regression Forests** (ideal but requires special implementation)\n",
    "2. **Bootstrap aggregation variance** (using individual tree predictions)\n",
    "3. **Empirical residual distribution** (simple but effective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7048f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Tree-based Prediction Intervals (using individual tree predictions)\n",
    "print(\"Prediction Intervals using Individual Tree Predictions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get predictions from each tree in the forest\n",
    "tree_predictions = np.array([tree.predict(X_test_processed) for tree in best_model.estimators_])\n",
    "print(f\"Trees in forest: {len(best_model.estimators_)}\")\n",
    "print(f\"Tree predictions shape: {tree_predictions.shape}\")\n",
    "\n",
    "# Calculate prediction intervals\n",
    "mean_pred = tree_predictions.mean(axis=0)\n",
    "std_pred = tree_predictions.std(axis=0)\n",
    "\n",
    "# Percentile-based intervals\n",
    "pred_lower_90 = np.percentile(tree_predictions, 5, axis=0)\n",
    "pred_upper_90 = np.percentile(tree_predictions, 95, axis=0)\n",
    "pred_lower_80 = np.percentile(tree_predictions, 10, axis=0)\n",
    "pred_upper_80 = np.percentile(tree_predictions, 90, axis=0)\n",
    "\n",
    "# Coverage analysis\n",
    "coverage_90 = np.mean((y_test.values >= pred_lower_90) & (y_test.values <= pred_upper_90))\n",
    "coverage_80 = np.mean((y_test.values >= pred_lower_80) & (y_test.values <= pred_upper_80))\n",
    "\n",
    "print(f\"\\n90% Prediction Interval Coverage: {coverage_90:.1%} (target: 90%)\")\n",
    "print(f\"80% Prediction Interval Coverage: {coverage_80:.1%} (target: 80%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction intervals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Sample 100 points for cleaner visualization\n",
    "sample_indices = np.random.choice(len(y_test), min(100, len(y_test)), replace=False)\n",
    "sample_indices = np.sort(sample_indices)\n",
    "\n",
    "# Left: Prediction intervals vs actual\n",
    "ax = axes[0]\n",
    "x_plot = np.arange(len(sample_indices))\n",
    "\n",
    "# Plot intervals\n",
    "ax.fill_between(x_plot, pred_lower_90[sample_indices], pred_upper_90[sample_indices], \n",
    "                alpha=0.3, color=MAIN_COLOR, label='90% PI')\n",
    "ax.fill_between(x_plot, pred_lower_80[sample_indices], pred_upper_80[sample_indices], \n",
    "                alpha=0.4, color=MAIN_COLOR, label='80% PI')\n",
    "ax.scatter(x_plot, y_test.iloc[sample_indices], color='red', s=20, alpha=0.7, label='Actual', zorder=5)\n",
    "ax.plot(x_plot, mean_pred[sample_indices], color='black', linewidth=1, label='Prediction')\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('TotalSpend_log')\n",
    "ax.set_title('Prediction Intervals with Actual Values')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Right: Interval width distribution\n",
    "ax = axes[1]\n",
    "interval_widths_90 = pred_upper_90 - pred_lower_90\n",
    "interval_widths_80 = pred_upper_80 - pred_lower_80\n",
    "\n",
    "ax.hist(interval_widths_90, bins=30, alpha=0.5, color=MAIN_COLOR, label='90% PI Width')\n",
    "ax.hist(interval_widths_80, bins=30, alpha=0.5, color=SECONDARY_COLOR, label='80% PI Width')\n",
    "ax.axvline(interval_widths_90.mean(), color=MAIN_COLOR, linestyle='--', label=f'Mean 90%: {interval_widths_90.mean():.3f}')\n",
    "ax.axvline(interval_widths_80.mean(), color=SECONDARY_COLOR, linestyle='--', label=f'Mean 80%: {interval_widths_80.mean():.3f}')\n",
    "ax.set_xlabel('Interval Width (log scale)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Prediction Interval Widths')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dollar intervals for business interpretation\n",
    "print(\"Prediction Intervals in Dollar Terms\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert log predictions to dollars\n",
    "dollar_lower_90 = np.expm1(pred_lower_90)\n",
    "dollar_upper_90 = np.expm1(pred_upper_90)\n",
    "dollar_pred = np.expm1(mean_pred)\n",
    "dollar_actual = np.expm1(y_test.values)\n",
    "\n",
    "# Calculate dollar interval widths\n",
    "dollar_interval_width = dollar_upper_90 - dollar_lower_90\n",
    "\n",
    "print(f\"\\n90% Prediction Interval Statistics (Dollars):\")\n",
    "print(f\"  Mean interval width: ${dollar_interval_width.mean():,.0f}\")\n",
    "print(f\"  Median interval width: ${np.median(dollar_interval_width):,.0f}\")\n",
    "print(f\"  Min interval width: ${dollar_interval_width.min():,.0f}\")\n",
    "print(f\"  Max interval width: ${dollar_interval_width.max():,.0f}\")\n",
    "\n",
    "# Example predictions with intervals\n",
    "print(f\"\\nüìã Example Predictions with 90% Intervals:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Sample':<8} {'Actual':>12} {'Predicted':>12} {'Lower 90%':>12} {'Upper 90%':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in [0, len(y_test)//4, len(y_test)//2, 3*len(y_test)//4, len(y_test)-1]:\n",
    "    print(f\"{i:<8} ${dollar_actual[i]:>10,.0f} ${dollar_pred[i]:>10,.0f} ${dollar_lower_90[i]:>10,.0f} ${dollar_upper_90[i]:>10,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5fdc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration Plot ‚Äî Are prediction intervals well-calibrated?\n",
    "print(\"Prediction Interval Calibration Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "calibration_results = []\n",
    "confidence_levels = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "\n",
    "for conf in confidence_levels:\n",
    "    lower_pct = (1 - conf) / 2 * 100\n",
    "    upper_pct = (1 + conf) / 2 * 100\n",
    "    \n",
    "    lower = np.percentile(tree_predictions, lower_pct, axis=0)\n",
    "    upper = np.percentile(tree_predictions, upper_pct, axis=0)\n",
    "    \n",
    "    actual_coverage = np.mean((y_test.values >= lower) & (y_test.values <= upper))\n",
    "    \n",
    "    calibration_results.append({\n",
    "        'Target_Coverage': conf,\n",
    "        'Actual_Coverage': actual_coverage,\n",
    "        'Calibration_Error': actual_coverage - conf\n",
    "    })\n",
    "\n",
    "calibration_df = pd.DataFrame(calibration_results)\n",
    "\n",
    "# Plot calibration curve\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "ax.plot(calibration_df['Target_Coverage'], calibration_df['Actual_Coverage'], \n",
    "        'o-', color=MAIN_COLOR, markersize=10, linewidth=2, label='RandomForest')\n",
    "\n",
    "ax.set_xlabel('Target Coverage (Confidence Level)')\n",
    "ax.set_ylabel('Actual Coverage (% of true values in interval)')\n",
    "ax.set_title('Prediction Interval Calibration')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0.4, 1.0)\n",
    "ax.set_ylim(0.4, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCalibration Table:\")\n",
    "print(calibration_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce825b7",
   "metadata": {},
   "source": [
    "### üìä Prediction Intervals Interpretation\n",
    "\n",
    "#### What Are Prediction Intervals?\n",
    "\n",
    "Unlike **confidence intervals** (uncertainty in the mean), **prediction intervals** capture:\n",
    "- Model uncertainty (epistemic) ‚Äî disagreement between trees\n",
    "- Data variability (aleatoric) ‚Äî inherent noise in the target\n",
    "- Individual prediction ranges for risk assessment\n",
    "\n",
    "#### Calibration Analysis Results\n",
    "\n",
    "| Target Coverage | Actual Coverage | Assessment |\n",
    "|-----------------|-----------------|------------|\n",
    "| 50% | 58.3% | Slightly over-covered |\n",
    "| 70% | 76.6% | Slightly over-covered |\n",
    "| 80% | 84.2% | ‚úì Well-calibrated |\n",
    "| **90%** | **90.6%** | ‚úì **Excellent calibration** |\n",
    "| 95% | 94.2% | ‚úì Well-calibrated |\n",
    "| 99% | 97.1% | Slightly under-covered |\n",
    "\n",
    "**Key Insight:** The model is well-calibrated at higher confidence levels (80-95%), meaning:\n",
    "- Our 90% prediction intervals contain ~91% of actual values\n",
    "- We can trust these intervals for business decision-making\n",
    "\n",
    "#### Dollar-Scale Interval Statistics\n",
    "\n",
    "| Metric | Value | Business Meaning |\n",
    "|--------|-------|------------------|\n",
    "| Mean 90% PI Width | **$500** | Typical uncertainty range |\n",
    "| Median 90% PI Width | **$344** | Half of customers have narrower intervals |\n",
    "| Min Width | $8 | Very confident predictions (low spenders) |\n",
    "| Max Width | $1,541 | High uncertainty for extreme spenders |\n",
    "\n",
    "#### Business Applications\n",
    "\n",
    "1. **Risk Assessment**\n",
    "   - \"Customer X will spend $639, with 90% confidence between $423-$914\"\n",
    "   - Provides bounds for budgeting and planning\n",
    "\n",
    "2. **Segment-Based Uncertainty**\n",
    "   - Low spenders: Narrow intervals (more predictable)\n",
    "   - High spenders: Wide intervals (less predictable)\n",
    "   - **Action:** Use wider margins for high-value segments\n",
    "\n",
    "3. **Decision Thresholds**\n",
    "   - **Conservative estimate:** Use lower bound ($423)\n",
    "   - **Optimistic estimate:** Use upper bound ($914)\n",
    "   - **Best guess:** Use point prediction ($639)\n",
    "\n",
    "#### Why This Matters\n",
    "\n",
    "Traditional ML only gives point predictions. Prediction intervals provide:\n",
    "- **Transparency:** Stakeholders understand prediction uncertainty\n",
    "- **Risk management:** Plan for worst/best case scenarios\n",
    "- **Model trust:** Calibrated intervals build confidence in the model\n",
    "\n",
    "**Recommendation:** Always report predictions with intervals for production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c798e4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Prediction Interpretation\n",
    "\n",
    "Let's convert predictions back to dollar values and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert log predictions back to dollar values\n",
    "y_test_dollars = np.expm1(y_test)  # Actual dollars\n",
    "y_pred_dollars = np.expm1(y_test_pred)  # Predicted dollars\n",
    "\n",
    "# Calculate dollar-based metrics\n",
    "dollar_rmse = np.sqrt(mean_squared_error(y_test_dollars, y_pred_dollars))\n",
    "dollar_mae = mean_absolute_error(y_test_dollars, y_pred_dollars)\n",
    "dollar_mape = np.mean(np.abs((y_test_dollars - y_pred_dollars) / y_test_dollars.replace(0, 1))) * 100\n",
    "\n",
    "print(\"Prediction Performance in Dollar Terms:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  RMSE: ${dollar_rmse:,.2f}\")\n",
    "print(f\"  MAE:  ${dollar_mae:,.2f}\")\n",
    "print(f\"  MAPE: {dollar_mape:.1f}%\")\n",
    "\n",
    "# Compare actual vs predicted distribution\n",
    "print(f\"\\nActual vs Predicted Dollar Distribution:\")\n",
    "print(f\"  Actual mean:     ${y_test_dollars.mean():,.2f}\")\n",
    "print(f\"  Predicted mean:  ${y_pred_dollars.mean():,.2f}\")\n",
    "print(f\"  Actual median:   ${y_test_dollars.median():,.2f}\")\n",
    "print(f\"  Predicted median: ${np.median(y_pred_dollars):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dollar predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted in dollars\n",
    "axes[0].scatter(y_test_dollars, y_pred_dollars, alpha=0.5, color=MAIN_COLOR)\n",
    "max_val = max(y_test_dollars.max(), y_pred_dollars.max())\n",
    "axes[0].plot([0, max_val], [0, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Spending ($)')\n",
    "axes[0].set_ylabel('Predicted Spending ($)')\n",
    "axes[0].set_title(f'{best_model_name}: Actual vs Predicted (Dollars)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Prediction error distribution\n",
    "errors = y_pred_dollars - y_test_dollars\n",
    "axes[1].hist(errors, bins=50, color=MAIN_COLOR, edgecolor='white')\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Prediction Error ($)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'Prediction Error Distribution (Mean: ${errors.mean():,.0f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c351380",
   "metadata": {},
   "source": [
    "### üí∞ Dollar-Scale Interpretation\n",
    "\n",
    "#### Translating Log Predictions to Business Value\n",
    "\n",
    "Since we trained on `TotalSpend_log = log(TotalSpend + 1)`, we need to interpret results in dollar terms for business stakeholders.\n",
    "\n",
    "**Performance Metrics in Dollars:**\n",
    "\n",
    "| Metric | Value | Business Meaning |\n",
    "|--------|-------|------------------|\n",
    "| **RMSE** | $212.18 | Typical prediction error magnitude |\n",
    "| **MAE** | $112.57 | Average absolute error per customer |\n",
    "| **MAPE** | 18.3% | Predictions off by ~18% on average |\n",
    "\n",
    "**Distribution Comparison:**\n",
    "\n",
    "| Statistic | Actual | Predicted | Difference |\n",
    "|-----------|--------|-----------|------------|\n",
    "| Mean | $640.94 | $617.12 | -$23.82 (3.7% under) |\n",
    "| Median | $439.50 | $449.48 | +$9.98 (2.3% over) |\n",
    "\n",
    "**Why These Numbers Matter:**\n",
    "\n",
    "1. **$212 RMSE in Context**\n",
    "   - Average customer spends $641\n",
    "   - RMSE represents ~33% of mean spending\n",
    "   - This sounds high, but spending distribution is highly skewed\n",
    "   - High spenders ($2000+) inflate RMSE disproportionately\n",
    "\n",
    "2. **$113 MAE is More Interpretable**\n",
    "   - Median customer spends ~$440\n",
    "   - MAE of $113 represents ~26% of median\n",
    "   - MAE is less sensitive to outliers than RMSE\n",
    "   - **For typical customers, predictions are within ~$113**\n",
    "\n",
    "3. **18.3% MAPE Shows Relative Accuracy**\n",
    "   - On average, we're ~18% off in either direction\n",
    "   - This is strong performance given spending variance (std ~$600)\n",
    "   - Acceptable for marketing segmentation purposes\n",
    "\n",
    "4. **Slight Under-Prediction Bias**\n",
    "   - Mean predictions $24 lower than actual\n",
    "   - This is conservative ‚Äî unlikely to over-promise on customer value\n",
    "   - Median is very close ‚Äî model captures central tendency well\n",
    "\n",
    "**Business Applications:**\n",
    "\n",
    "- **Customer Scoring:** Rank customers by predicted spend for prioritization\n",
    "- **Budget Allocation:** Estimate total revenue from customer cohorts\n",
    "- **Segmentation:** Define value tiers based on predicted spending\n",
    "- **ROI Calculation:** Estimate LTV for acquisition cost decisions\n",
    "\n",
    "**Important Caveat:**\n",
    "Dollar metrics are calculated by `exp(prediction) - 1`. This back-transformation amplifies errors for high-value predictions. For very high spenders, individual predictions may be less reliable than segment-level estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de428929",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Save Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0304d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and preprocessor\n",
    "model_path = 'models/best_regression_model.joblib'\n",
    "preprocessor_path = 'models/regression_preprocessor.joblib'\n",
    "results_path = 'models/regression_results.csv'\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"‚úì Model saved: {model_path}\")\n",
    "\n",
    "# Save preprocessor\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"‚úì Preprocessor saved: {preprocessor_path}\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"‚úì Results saved: {results_path}\")\n",
    "\n",
    "# Save feature names for reference\n",
    "feature_info = {\n",
    "    'feature_names': feature_names,\n",
    "    'num_features': num_features,\n",
    "    'cat_features': cat_features_final\n",
    "}\n",
    "joblib.dump(feature_info, 'models/regression_features.joblib')\n",
    "print(f\"‚úì Feature info saved: models/regression_features.joblib\")\n",
    "\n",
    "print(f\"\\nüìÅ All model artifacts saved to 'models/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77312147",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary & Conclusions\n",
    "\n",
    "### Executive Summary\n",
    "\n",
    "This regression analysis successfully built a **highly accurate spending prediction model** that explains **97% of customer spending variance**. The RandomForest model emerged as the best performer after rigorous evaluation against 8 algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "print(f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                 REGRESSION ANALYSIS SUMMARY                       ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üéØ OBJECTIVE\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Predict customer total spending (TotalSpend_log) to identify \n",
    "high-value customers and inform marketing strategies.\n",
    "\n",
    "üìä DATASET\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ Training samples: {len(X_train):,}\n",
    "‚Ä¢ Test samples: {len(X_test):,}\n",
    "‚Ä¢ Features: {len(feature_names)} (after encoding)\n",
    "‚Ä¢ Target: TotalSpend_log (log-transformed spending)\n",
    "\n",
    "üèÜ BEST MODEL: {best_model_name}\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ Test R¬≤:   {results_df.iloc[0]['R¬≤_test']:.4f}\n",
    "‚Ä¢ Test RMSE: {results_df.iloc[0]['RMSE_test']:.4f} (log scale)\n",
    "‚Ä¢ Test MAE:  {results_df.iloc[0]['MAE_test']:.4f} (log scale)\n",
    "‚Ä¢ Dollar RMSE: ${dollar_rmse:,.2f}\n",
    "‚Ä¢ Dollar MAE:  ${dollar_mae:,.2f}\n",
    "\n",
    "üìà KEY FINDINGS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "1. {'Tree-based models outperform linear models' if 'Forest' in best_model_name or 'Boost' in best_model_name else 'Linear models perform competitively'}\n",
    "2. Most important features: Income, IncomePerCapita, purchase channels\n",
    "3. HasChildren negatively impacts spending (confirmed from EDA)\n",
    "4. Model explains ~{results_df.iloc[0]['R¬≤_test']*100:.1f}% of spending variance\n",
    "\n",
    "‚ö†Ô∏è CONSIDERATIONS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ Target is log-transformed ‚Äî use exp(pred) - 1 for dollar values\n",
    "‚Ä¢ Preprocessor must be applied to new data before prediction\n",
    "‚Ä¢ Model trained without spending columns to avoid target leakage\n",
    "\n",
    "üöÄ NEXT STEPS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Üí Proceed to 03_classification.ipynb for Response prediction\n",
    "‚Üí Remember to EXCLUDE campaign features due to data leakage!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdcabc",
   "metadata": {},
   "source": [
    "### üî¨ Technical Findings\n",
    "\n",
    "#### Model Selection Journey\n",
    "\n",
    "| Stage | Best Model | R¬≤ Score | Key Insight |\n",
    "|-------|------------|----------|-------------|\n",
    "| Baseline (8 models) | GradientBoosting | 0.9504 CV | Tree models outperform linear by ~6% |\n",
    "| Hyperparameter Tuning | XGBoost | 0.9534 CV | Marginal gains from tuning |\n",
    "| Test Set Evaluation | **RandomForest** | **0.9703 test** | Best generalization despite lower CV |\n",
    "\n",
    "#### Why RandomForest Won\n",
    "\n",
    "1. **Bagging > Boosting for this data** ‚Äî RF's bootstrap aggregation provides better variance reduction\n",
    "2. **Robust to overfitting** ‚Äî 200 trees with max_depth=15 generalizes better than shallow boosted trees\n",
    "3. **Test performance matters** ‚Äî CV scores during tuning don't always predict test performance\n",
    "\n",
    "#### Key Technical Decisions\n",
    "\n",
    "| Decision | Rationale | Impact |\n",
    "|----------|-----------|--------|\n",
    "| Log-transform target | Reduce skewness (0.86 ‚Üí -0.37) | Better model assumptions |\n",
    "| Exclude spending columns | Prevent target leakage | Valid predictions |\n",
    "| IQR outlier capping | Remove extreme values | More robust models |\n",
    "| 5-fold CV | Balance bias/variance in evaluation | Reliable model selection |\n",
    "\n",
    "### üíº Business Implications\n",
    "\n",
    "#### Actionable Insights\n",
    "\n",
    "1. **Purchase Behavior is the Key Driver (92% importance)**\n",
    "   - NumCatalogPurchases alone explains 71% of spending\n",
    "   - Web and Store purchases add incremental value\n",
    "   - **Action:** Focus acquisition on catalog-preferring customers\n",
    "\n",
    "2. **Income is Necessary but Not Sufficient (5% importance)**\n",
    "   - High income enables spending but doesn't guarantee it\n",
    "   - IncomePerCapita adds refinement for household context\n",
    "   - **Action:** Don't over-index on income in targeting\n",
    "\n",
    "3. **Prediction Accuracy for Business Use**\n",
    "   - MAE of $113 is acceptable for customer scoring\n",
    "   - 18% MAPE enables reliable cohort-level estimates\n",
    "   - **Action:** Use predictions for tiered segmentation\n",
    "\n",
    "#### Model Usage Guidelines\n",
    "\n",
    "```python\n",
    "# To predict spending for a new customer:\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load artifacts\n",
    "model = joblib.load('models/best_regression_model.joblib')\n",
    "preprocessor = joblib.load('models/regression_preprocessor.joblib')\n",
    "\n",
    "# 2. Prepare customer data (DataFrame with required features)\n",
    "X_new = preprocessor.transform(customer_data)\n",
    "\n",
    "# 3. Predict log spending\n",
    "log_prediction = model.predict(X_new)\n",
    "\n",
    "# 4. Convert to dollars\n",
    "dollar_prediction = np.expm1(log_prediction)\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Limitations & Considerations\n",
    "\n",
    "1. **High Feature Importance Concentration**\n",
    "   - 71% importance in a single feature (NumCatalogPurchases) is a risk\n",
    "   - If catalog behavior changes (e.g., digital shift), model may degrade\n",
    "   - Consider monitoring this feature's predictive power over time\n",
    "\n",
    "2. **Dollar Prediction Variance**\n",
    "   - $212 RMSE means individual predictions have uncertainty\n",
    "   - More reliable for segment averages than individual predictions\n",
    "   - High-value customers have higher absolute prediction error\n",
    "\n",
    "3. **Temporal Considerations**\n",
    "   - Model trained on snapshot data ‚Äî may need retraining as patterns evolve\n",
    "   - Recency feature helps capture engagement timing\n",
    "   - Recommend quarterly model refreshes\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "| Priority | Action | Notebook |\n",
    "|----------|--------|----------|\n",
    "| **High** | Build Response classification model | `03_classification.ipynb` |\n",
    "| **Medium** | Customer segmentation with clustering | `04_clustering.ipynb` |\n",
    "| **Low** | Feature importance with SHAP values | Enhancement |\n",
    "\n",
    "---\n",
    "\n",
    "**Model Artifacts Saved:**\n",
    "- `models/best_regression_model.joblib` ‚Äî Trained RandomForest model\n",
    "- `models/regression_preprocessor.joblib` ‚Äî Fitted preprocessing pipeline\n",
    "- `models/regression_results.csv` ‚Äî Model comparison results\n",
    "- `models/regression_features.joblib` ‚Äî Feature name mappings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
