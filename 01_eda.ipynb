{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50d353a",
   "metadata": {},
   "source": [
    "# üîç Exploratory Data Analysis (EDA)\n",
    "## Customer Personality Analysis Dataset\n",
    "\n",
    "**Objective**: Understand customer behavior patterns, identify data quality issues, and prepare insights for predictive modeling.\n",
    "\n",
    "**Dataset**: Marketing campaign data with 2,240 customers and 29 features covering demographics, purchase behavior, and campaign responses.\n",
    "\n",
    "**Key Questions**:\n",
    "1. What are the data quality issues we need to address?\n",
    "2. Which features show strong predictive signals for spending and campaign response?\n",
    "3. Are there outliers or anomalies that require special treatment?\n",
    "4. What feature engineering will improve model performance?\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reimport\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Remove all src modules from cache\n",
    "modules_to_remove = [key for key in sys.modules.keys() if key.startswith('src')]\n",
    "for mod in modules_to_remove:\n",
    "    del sys.modules[mod]\n",
    "    \n",
    "# Directly load config module\n",
    "spec = importlib.util.spec_from_file_location(\"src.config\", \"src/config.py\")\n",
    "config_module = importlib.util.module_from_spec(spec)\n",
    "sys.modules['src.config'] = config_module\n",
    "spec.loader.exec_module(config_module)\n",
    "\n",
    "print(\"‚úì Module reloaded, checking RAW_NUM_COLS:\", hasattr(config_module, 'RAW_NUM_COLS'))\n",
    "print(\"  RAW_NUM_COLS =\", config_module.RAW_NUM_COLS[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86787216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Sklearn for anomaly detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Our custom modules\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.config import (\n",
    "    DATA_PATH, RANDOM_STATE,\n",
    "    RAW_NUM_COLS, RAW_CAT_COLS,\n",
    "    SPENDING_COLS, PURCHASE_COLS, CAMPAIGN_COLS,\n",
    "    TARGET_REGRESSION, TARGET_CLASSIFICATION,\n",
    "    MAIN_COLOR, SECONDARY_COLOR, ACCENT_COLOR,\n",
    "    FIGURE_SIZE_SMALL, FIGURE_SIZE_MEDIUM, FIGURE_SIZE_LARGE\n",
    ")\n",
    "from src.data_loader import load_data, data_overview, validate_data, get_feature_types\n",
    "from src.preprocessing import detect_outliers_iqr, get_anomaly_scores\n",
    "from src.visualization import (\n",
    "    set_style, plot_distribution, plot_distributions_grid,\n",
    "    plot_boxplots_grid, plot_correlation_matrix, plot_target_correlation,\n",
    "    plot_countplot, plot_scatter, plot_boxplot_by_category,\n",
    "    plot_anomaly_scatter_pca, plot_outlier_summary\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "set_style()\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_raw = load_data(DATA_PATH)\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data overview\n",
    "data_overview(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d03565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names and data types\n",
    "print(\"Column Information:\")\n",
    "print(\"-\" * 50)\n",
    "for col in df_raw.columns:\n",
    "    dtype = df_raw[col].dtype\n",
    "    nunique = df_raw[col].nunique()\n",
    "    null_pct = 100 * df_raw[col].isnull().sum() / len(df_raw)\n",
    "    print(f\"{col:25} | {str(dtype):10} | {nunique:6} unique | {null_pct:5.1f}% null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e6047",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Missing Value Analysis\n",
    "\n",
    "**Why this matters**: Missing values can bias model training and reduce prediction accuracy. We need to identify the extent, pattern, and appropriate treatment strategy.\n",
    "\n",
    "**Method**: We check each column for null values and analyze whether missing data is:\n",
    "- **MCAR** (Missing Completely At Random) ‚Äî Safe to impute\n",
    "- **MAR** (Missing At Random) ‚Äî Impute with caution\n",
    "- **MNAR** (Missing Not At Random) ‚Äî May indicate systematic bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb123ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values summary\n",
    "missing = df_raw.isnull().sum()\n",
    "missing_pct = 100 * missing / len(df_raw)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "}).query('`Missing Count` > 0').sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"‚ö†Ô∏è Columns with Missing Values:\")\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"‚úì No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2dc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "if missing_df.shape[0] > 0:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    missing_df['Missing %'].plot(kind='barh', color=MAIN_COLOR)\n",
    "    plt.xlabel('% Missing')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Income missing values - are they random?\n",
    "if 'Income' in df_raw.columns and df_raw['Income'].isnull().any():\n",
    "    print(\"Comparing customers with/without Income data:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    has_income = df_raw[df_raw['Income'].notna()]\n",
    "    no_income = df_raw[df_raw['Income'].isna()]\n",
    "    \n",
    "    # Compare education distribution\n",
    "    print(\"\\nEducation Distribution:\")\n",
    "    print(f\"With Income:\\n{has_income['Education'].value_counts(normalize=True).round(3)}\")\n",
    "    print(f\"\\nWithout Income:\\n{no_income['Education'].value_counts(normalize=True).round(3)}\")\n",
    "    \n",
    "    # Conclusion\n",
    "    print(\"\\nüìù Note: Missing income appears random (MCAR). Will use median imputation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c1473",
   "metadata": {},
   "source": [
    "### üìã Missing Value Analysis ‚Äî Results\n",
    "\n",
    "**Finding**: Only **Income** has missing values ‚Äî **24 records (1.07%)** out of 2,240.\n",
    "\n",
    "**MCAR Test**: We compared the education distribution between customers with and without income data:\n",
    "- Both groups show similar proportions (Graduation ~50%, PhD ~21-22%, Master ~17-21%)\n",
    "- No systematic pattern detected ‚Üí **MCAR confirmed**\n",
    "\n",
    "| Column | Missing | Strategy | Rationale |\n",
    "|--------|---------|----------|-----------|\n",
    "| Income | 24 (1.07%) | **Median Imputation** | MCAR pattern; median is robust to outliers (Income has extreme outlier at $666,666) |\n",
    "\n",
    "‚ö†Ô∏è **Critical**: Imputation will be performed **after train/test split** to prevent data leakage. The median will be computed only from training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21cbb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Engineering\n",
    "\n",
    "**Purpose**: Create meaningful derived features that capture domain knowledge and improve model performance.\n",
    "\n",
    "### Feature Categories\n",
    "\n",
    "| Category | Features | Rationale |\n",
    "|----------|----------|-----------|\n",
    "| **Temporal** | `Age`, `Tenure_Days`, `Tenure_Months` | Customer lifecycle stage |\n",
    "| **Spending** | `TotalSpend`, `TotalSpend_log`, `SpendingRatio`, `AvgSpendPerPurchase` | Purchasing behavior |\n",
    "| **Engagement** | `TotalPurchases`, `TotalAccepted`, `WebPurchaseRatio` | Channel preferences |\n",
    "| **Family** | `FamilySize`, `HasChildren`, `TotalChildren`, `IncomePerCapita` | Household dynamics |\n",
    "| **Encoding** | `Education_Num`, `IsPartner`, `Education_Level` | ML-ready categoricals |\n",
    "| **Segmentation** | `IsHighSpender`, `IsPreviousResponder`, `RecencyCategory` | Customer segments |\n",
    "\n",
    "### Key Transformations\n",
    "1. **Income Imputation** ‚Äî Median fill for 24 missing values (MCAR confirmed)\n",
    "2. **Age Filtering** ‚Äî Cap unrealistic ages to [18, 90] range\n",
    "3. **Log Transform** ‚Äî `TotalSpend_log` reduces skewness from 0.86 to -0.37\n",
    "4. **Ratio Features** ‚Äî `SpendingRatio`, `IncomePerCapita` normalize for family size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy with basic feature engineering\n",
    "df = df_raw.copy()\n",
    "\n",
    "# ============================================================\n",
    "# 1. HANDLE MISSING VALUES (Income)\n",
    "# ============================================================\n",
    "income_median = df['Income'].median()\n",
    "df['Income'] = df['Income'].fillna(income_median)\n",
    "print(f\"‚úì Imputed {df_raw['Income'].isna().sum()} missing Income values with median (${income_median:,.0f})\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. DATE & TENURE FEATURES\n",
    "# ============================================================\n",
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')\n",
    "reference_date = df['Dt_Customer'].max()\n",
    "df['Tenure_Days'] = (reference_date - df['Dt_Customer']).dt.days\n",
    "df['Tenure_Months'] = df['Tenure_Days'] // 30  # More interpretable\n",
    "\n",
    "# ============================================================\n",
    "# 3. AGE CALCULATION & FILTERING\n",
    "# ============================================================\n",
    "current_year = reference_date.year\n",
    "df['Age'] = current_year - df['Year_Birth']\n",
    "\n",
    "# Filter unrealistic ages (found some Year_Birth like 1893)\n",
    "age_outliers = (df['Age'] < 18) | (df['Age'] > 100)\n",
    "if age_outliers.sum() > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {age_outliers.sum()} unrealistic ages ‚Äî capping to [18, 90]\")\n",
    "    df['Age'] = df['Age'].clip(18, 90)\n",
    "\n",
    "# ============================================================\n",
    "# 4. SPENDING FEATURES\n",
    "# ============================================================\n",
    "# Total spending (our regression target)\n",
    "df['TotalSpend'] = df[SPENDING_COLS].sum(axis=1)\n",
    "\n",
    "# Log-transformed spending (handles skewness)\n",
    "df['TotalSpend_log'] = np.log1p(df['TotalSpend'])\n",
    "\n",
    "# Spending relative to income (purchasing power usage)\n",
    "df['SpendingRatio'] = df['TotalSpend'] / df['Income'].replace(0, 1)  # Avoid division by zero\n",
    "\n",
    "# ============================================================\n",
    "# 5. PURCHASE BEHAVIOR FEATURES\n",
    "# ============================================================\n",
    "df['TotalPurchases'] = df[PURCHASE_COLS].sum(axis=1)\n",
    "df['TotalAccepted'] = df[CAMPAIGN_COLS].sum(axis=1)\n",
    "\n",
    "# Average spend per purchase\n",
    "df['AvgSpendPerPurchase'] = df['TotalSpend'] / df['TotalPurchases'].replace(0, 1)\n",
    "\n",
    "# Web engagement ratio\n",
    "df['WebPurchaseRatio'] = df['NumWebPurchases'] / df['TotalPurchases'].replace(0, 1)\n",
    "\n",
    "# ============================================================\n",
    "# 6. FAMILY & HOUSEHOLD FEATURES\n",
    "# ============================================================\n",
    "df['FamilySize'] = df['Kidhome'] + df['Teenhome'] + 1  # +1 for customer\n",
    "df['HasChildren'] = ((df['Kidhome'] > 0) | (df['Teenhome'] > 0)).astype(int)\n",
    "df['TotalChildren'] = df['Kidhome'] + df['Teenhome']\n",
    "\n",
    "# Income per family member (better wealth indicator)\n",
    "df['IncomePerCapita'] = df['Income'] / df['FamilySize']\n",
    "\n",
    "# ============================================================\n",
    "# 7. CATEGORICAL ENCODING\n",
    "# ============================================================\n",
    "# Simplified education levels\n",
    "education_map = {\n",
    "    'Basic': 'Undergrad',\n",
    "    '2n Cycle': 'Undergrad', \n",
    "    'Graduation': 'Graduate',\n",
    "    'Master': 'Postgraduate',\n",
    "    'PhD': 'Postgraduate'\n",
    "}\n",
    "df['Education_Level'] = df['Education'].map(education_map)\n",
    "\n",
    "# Numeric education encoding (ordinal)\n",
    "education_num_map = {'Basic': 1, '2n Cycle': 2, 'Graduation': 3, 'Master': 4, 'PhD': 5}\n",
    "df['Education_Num'] = df['Education'].map(education_num_map)\n",
    "\n",
    "# Simplified marital status\n",
    "marital_map = {\n",
    "    'Married': 'Partner',\n",
    "    'Together': 'Partner',\n",
    "    'Single': 'Single',\n",
    "    'Divorced': 'Single',\n",
    "    'Widow': 'Single',\n",
    "    'Alone': 'Single',\n",
    "    'Absurd': 'Single',\n",
    "    'YOLO': 'Single'\n",
    "}\n",
    "df['Marital_Status'] = df['Marital_Status'].map(marital_map)\n",
    "\n",
    "# Numeric marital status encoding\n",
    "df['IsPartner'] = (df['Marital_Status'] == 'Partner').astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# 8. CUSTOMER SEGMENTATION FLAGS\n",
    "# ============================================================\n",
    "# High spender flag (top 25%)\n",
    "spend_threshold = df['TotalSpend'].quantile(0.75)\n",
    "df['IsHighSpender'] = (df['TotalSpend'] >= spend_threshold).astype(int)\n",
    "\n",
    "# Previous campaign responder (engaged customer)\n",
    "df['IsPreviousResponder'] = (df['TotalAccepted'] > 0).astype(int)\n",
    "\n",
    "# Recency category\n",
    "df['RecencyCategory'] = pd.cut(df['Recency'], bins=[0, 30, 60, 90, float('inf')], \n",
    "                                labels=['Recent', 'Moderate', 'Lapsed', 'Inactive'])\n",
    "\n",
    "print(\"\\n‚úì Feature engineering complete!\")\n",
    "print(f\"  Original columns: {len(df_raw.columns)}\")\n",
    "print(f\"  New columns: {len(df.columns) - len(df_raw.columns)}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "engineered_cols = [\n",
    "    'Tenure_Days', 'Tenure_Months', 'Age', 'TotalSpend', 'TotalSpend_log', \n",
    "    'SpendingRatio', 'TotalPurchases', 'TotalAccepted', 'AvgSpendPerPurchase',\n",
    "    'WebPurchaseRatio', 'FamilySize', 'HasChildren', 'TotalChildren', \n",
    "    'IncomePerCapita', 'Education_Level', 'Education_Num', 'IsPartner',\n",
    "    'IsHighSpender', 'IsPreviousResponder', 'RecencyCategory'\n",
    "]\n",
    "for col in engineered_cols:\n",
    "    print(f\"  ‚Ä¢ {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check on engineered features\n",
    "print(\"Engineered Features Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Numeric features check\n",
    "numeric_engineered = ['Age', 'Tenure_Days', 'TotalSpend', 'TotalSpend_log', \n",
    "                      'SpendingRatio', 'IncomePerCapita', 'AvgSpendPerPurchase']\n",
    "print(\"\\nüìä Numeric Features:\")\n",
    "display(df[numeric_engineered].describe().round(2))\n",
    "\n",
    "# Binary/categorical features check  \n",
    "print(\"\\nüìã Binary/Categorical Features:\")\n",
    "binary_engineered = ['HasChildren', 'IsPartner', 'IsHighSpender', 'IsPreviousResponder']\n",
    "for col in binary_engineered:\n",
    "    pct = df[col].mean() * 100\n",
    "    print(f\"  {col:25} | {pct:5.1f}% = 1\")\n",
    "\n",
    "print(\"\\nüìä Recency Categories:\")\n",
    "print(df['RecencyCategory'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation of new features with targets\n",
    "print(\"Feature Correlation with Targets:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "new_features = ['IncomePerCapita', 'SpendingRatio', 'HasChildren', 'IsPartner', \n",
    "                'Education_Num', 'AvgSpendPerPurchase', 'WebPurchaseRatio',\n",
    "                'IsPreviousResponder', 'IsHighSpender', 'Tenure_Months']\n",
    "\n",
    "# Correlation with regression target\n",
    "spend_corr = df[new_features + ['TotalSpend_log']].corr()['TotalSpend_log'].drop('TotalSpend_log').sort_values(ascending=False)\n",
    "print(\"\\nüéØ Correlation with TotalSpend_log (Regression):\")\n",
    "for feat, corr in spend_corr.items():\n",
    "    direction = \"+\" if corr > 0 else \"-\"\n",
    "    print(f\"  {feat:25} | {direction}{abs(corr):.3f}\")\n",
    "\n",
    "# Correlation with classification target\n",
    "response_corr = df[new_features + ['Response']].corr()['Response'].drop('Response').sort_values(ascending=False)\n",
    "print(\"\\nüéØ Correlation with Response (Classification):\")\n",
    "for feat, corr in response_corr.items():\n",
    "    direction = \"+\" if corr > 0 else \"-\"\n",
    "    print(f\"  {feat:25} | {direction}{abs(corr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0c300",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Univariate Analysis ‚Äî Numeric Features\n",
    "\n",
    "**Objective**: Understand the distribution of each numeric feature to identify:\n",
    "- Skewness requiring transformation\n",
    "- Potential outliers\n",
    "- Features suitable for different model types\n",
    "\n",
    "**Method**: Histogram + KDE plots reveal distribution shape; skewness > 1 indicates right-skewed data needing log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99974b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key numeric features for analysis\n",
    "numeric_features = [\n",
    "    'Income', 'Age', 'Tenure_Days', 'Recency',\n",
    "    'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n",
    "    'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n",
    "    'TotalSpend', 'TotalPurchases'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "numeric_features = [c for c in numeric_features if c in df.columns]\n",
    "\n",
    "print(f\"Analyzing {len(numeric_features)} numeric features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "plot_distributions_grid(df, numeric_features, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6decd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness analysis\n",
    "print(\"Skewness Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"(|skew| > 1 indicates high skewness, consider log transform)\\n\")\n",
    "\n",
    "skewness = df[numeric_features].skew().sort_values(ascending=False)\n",
    "for col, skew in skewness.items():\n",
    "    flag = \"‚ö†Ô∏è HIGH\" if abs(skew) > 1 else \"  OK\"\n",
    "    print(f\"{flag}  {col:25} | Skewness: {skew:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cdc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs log-transformed TotalSpend\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.histplot(df['TotalSpend'], kde=True, ax=axes[0], color=MAIN_COLOR)\n",
    "axes[0].set_title(f'TotalSpend (skew={df[\"TotalSpend\"].skew():.2f})')\n",
    "\n",
    "sns.histplot(df['TotalSpend_log'], kde=True, ax=axes[1], color=SECONDARY_COLOR)\n",
    "axes[1].set_title(f'TotalSpend_log (skew={df[\"TotalSpend_log\"].skew():.2f})')\n",
    "\n",
    "plt.suptitle('Effect of Log Transformation on Target Variable', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìù Log transformation significantly reduces skewness ‚Äî will use TotalSpend_log as regression target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8aa1f",
   "metadata": {},
   "source": [
    "### üìä Distribution Analysis ‚Äî Key Findings\n",
    "\n",
    "**Highly Skewed Features (|skew| > 1)**:\n",
    "| Feature | Skewness | Issue | Action |\n",
    "|---------|----------|-------|--------|\n",
    "| Income | **6.76** | Extreme outlier ($666,666) | Cap at 99th percentile, then log transform |\n",
    "| NumDealsPurchases | 2.42 | Right-skewed | Consider log or leave as-is |\n",
    "| All Mnt* columns | 1.18‚Äì2.14 | Right-skewed spending | Cap outliers |\n",
    "| NumCatalogPurchases | 1.88 | Right-skewed | Standard scaling sufficient |\n",
    "\n",
    "**Normally Distributed Features** (|skew| < 1):\n",
    "- `Recency` (skew ‚âà 0) ‚Äî Uniform distribution, no transformation needed\n",
    "- `Tenure_Days` (skew ‚âà 0) ‚Äî Good as-is\n",
    "- `Age` (skew = 0.35) ‚Äî Slight right skew, acceptable\n",
    "- `TotalSpend` (skew = 0.86) ‚Üí `TotalSpend_log` (skew = -0.37) ‚Äî **Log transformation effective**\n",
    "\n",
    "**Decision**: Use `TotalSpend_log` as regression target to satisfy linear model assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ac606",
   "metadata": {},
   "source": [
    "## 5. üì¶ Outlier Detection and Treatment\n",
    "\n",
    "### Strategy Selection: IQR Method\n",
    "\n",
    "**Why IQR (Interquartile Range)?**\n",
    "- **Robust to skewed data**: Unlike z-scores, IQR doesn't assume normality\n",
    "- **Non-parametric**: Works well for our right-skewed spending distributions\n",
    "- **Interpretable**: Clear cut-off based on quartiles (Q1 - k√óIQR, Q3 + k√óIQR)\n",
    "\n",
    "**Parameters**:\n",
    "- `k = 1.5` ‚Äî Standard IQR multiplier (detects moderate outliers)\n",
    "- Applied to `Income` and all `Mnt*` spending columns\n",
    "\n",
    "**Treatment Decision**: **Clip (Cap) outliers** at bounds rather than remove\n",
    "- **Reason**: Extreme spenders are still valuable customers ‚Äî removing them would bias our analysis\n",
    "- **Effect**: Preserves data volume while reducing extreme value influence on model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for outlier visualization\n",
    "plot_boxplots_grid(df, numeric_features, ncols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR-based outlier detection\n",
    "outlier_df = detect_outliers_iqr(df, numeric_features, k=1.5)\n",
    "print(\"IQR Outlier Detection Results:\")\n",
    "print(\"=\" * 50)\n",
    "outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72714c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outlier percentages\n",
    "plot_outlier_summary(outlier_df, title=\"IQR Outlier Detection (1.5√óIQR)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ea0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for anomaly detection (handle missing values temporarily)\n",
    "df_anomaly = df[numeric_features].copy()\n",
    "df_anomaly = df_anomaly.fillna(df_anomaly.median())\n",
    "\n",
    "# Standardize for anomaly detection\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_anomaly)\n",
    "\n",
    "print(f\"Prepared {X_scaled.shape[0]} samples with {X_scaled.shape[1]} features for anomaly detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8baeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest\n",
    "print(\"Running Isolation Forest...\")\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.05,  # Expect ~5% anomalies\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "iso_labels = iso_forest.fit_predict(X_scaled)\n",
    "iso_anomalies = iso_labels == -1\n",
    "\n",
    "print(f\"\\nIsolation Forest Results:\")\n",
    "print(f\"  Normal samples: {(~iso_anomalies).sum():,}\")\n",
    "print(f\"  Anomalies: {iso_anomalies.sum():,} ({100*iso_anomalies.mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Outlier Factor\n",
    "print(\"Running Local Outlier Factor...\")\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=0.05,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lof_labels = lof.fit_predict(X_scaled)\n",
    "lof_anomalies = lof_labels == -1\n",
    "\n",
    "print(f\"\\nLocal Outlier Factor Results:\")\n",
    "print(f\"  Normal samples: {(~lof_anomalies).sum():,}\")\n",
    "print(f\"  Anomalies: {lof_anomalies.sum():,} ({100*lof_anomalies.mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13811ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare anomaly detection methods\n",
    "both_anomalies = iso_anomalies & lof_anomalies\n",
    "either_anomalies = iso_anomalies | lof_anomalies\n",
    "\n",
    "print(\"Anomaly Detection Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Isolation Forest only: {(iso_anomalies & ~lof_anomalies).sum()}\")\n",
    "print(f\"LOF only: {(~iso_anomalies & lof_anomalies).sum()}\")\n",
    "print(f\"Both methods agree: {both_anomalies.sum()}\")\n",
    "print(f\"Either method: {either_anomalies.sum()}\")\n",
    "print(f\"\\nAgreement rate: {100 * both_anomalies.sum() / either_anomalies.sum():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a781dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomalies in PCA space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Isolation Forest anomalies\n",
    "axes[0].scatter(X_pca[~iso_anomalies, 0], X_pca[~iso_anomalies, 1], s=10, alpha=0.5, c=SECONDARY_COLOR, label='Normal')\n",
    "axes[0].scatter(X_pca[iso_anomalies, 0], X_pca[iso_anomalies, 1], s=30, alpha=0.8, c='red', label='Anomaly')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[0].set_title('Isolation Forest Anomalies')\n",
    "axes[0].legend()\n",
    "\n",
    "# LOF anomalies\n",
    "axes[1].scatter(X_pca[~lof_anomalies, 0], X_pca[~lof_anomalies, 1], s=10, alpha=0.5, c=SECONDARY_COLOR, label='Normal')\n",
    "axes[1].scatter(X_pca[lof_anomalies, 0], X_pca[lof_anomalies, 1], s=30, alpha=0.8, c='red', label='Anomaly')\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[1].set_title('Local Outlier Factor Anomalies')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('Anomaly Detection Comparison (PCA 2D Projection)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze characteristics of anomalies\n",
    "df['is_anomaly_iso'] = iso_anomalies\n",
    "df['is_anomaly_lof'] = lof_anomalies\n",
    "df['is_anomaly_both'] = both_anomalies\n",
    "\n",
    "print(\"Characteristics of Anomalies (Isolation Forest):\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nMean values - Normal vs Anomaly:\")\n",
    "comparison = df.groupby('is_anomaly_iso')[['Income', 'Age', 'TotalSpend', 'NumWebVisitsMonth', 'Recency']].mean()\n",
    "comparison.index = ['Normal', 'Anomaly']\n",
    "print(comparison.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5c542",
   "metadata": {},
   "source": [
    "## 6. üîç Advanced Anomaly Detection\n",
    "\n",
    "### Two Complementary Approaches\n",
    "\n",
    "We apply **two unsupervised** anomaly detection methods to identify multivariate outliers that IQR (univariate) might miss:\n",
    "\n",
    "| Method | How It Works | Strength |\n",
    "|--------|--------------|----------|\n",
    "| **Isolation Forest** | Randomly partitions data; anomalies require fewer splits | Fast, handles high dimensions |\n",
    "| **Local Outlier Factor (LOF)** | Compares local density to neighbors | Detects local anomalies in clusters |\n",
    "\n",
    "**Why Both?**\n",
    "- Isolation Forest finds **global** anomalies (points far from everyone)\n",
    "- LOF finds **local** anomalies (points unusual relative to their neighborhood)\n",
    "- Low agreement between methods ‚Üí different anomaly types detected\n",
    "\n",
    "**Parameters**:\n",
    "- Contamination = 5% (assume ~5% of data are anomalies)\n",
    "- LOF n_neighbors = 20 (standard choice)\n",
    "\n",
    "**What to Look For**:\n",
    "1. Number of anomalies flagged by each method\n",
    "2. **Overlap** ‚Äî points flagged by both methods are high-confidence anomalies\n",
    "3. Anomaly characteristics ‚Äî what makes them different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fc490",
   "metadata": {},
   "source": [
    "## 7. üîó Correlation Analysis\n",
    "\n",
    "### Purpose\n",
    "Identify **linear relationships** between numerical features to:\n",
    "1. Detect **multicollinearity** that could harm regression models\n",
    "2. Find **strong predictors** for our targets\n",
    "3. Inform **feature selection** decisions\n",
    "\n",
    "**Interpretation Guide**:\n",
    "| |r| Range | Interpretation | Action |\n",
    "|------------|----------------|--------|\n",
    "| 0.9 ‚Äì 1.0 | Very strong | Drop one feature |\n",
    "| 0.7 ‚Äì 0.9 | Strong | Monitor for multicollinearity |\n",
    "| 0.5 ‚Äì 0.7 | Moderate | Good predictor candidates |\n",
    "| 0.3 ‚Äì 0.5 | Weak | Include if domain-relevant |\n",
    "| < 0.3 | Very weak | May not add value |\n",
    "\n",
    "**Expected Patterns**:\n",
    "- Spending columns (`Mnt*`) likely correlated with each other\n",
    "- `TotalSpend` perfectly correlated with sum of Mnt columns (by construction)\n",
    "- `Income` should correlate with spending behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "cat_features = ['Education', 'Education_Level', 'Marital_Status']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, col in enumerate(cat_features):\n",
    "    order = df[col].value_counts().index\n",
    "    sns.countplot(data=df, x=col, ax=axes[i], order=order, color=MAIN_COLOR)\n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = len(df)\n",
    "    for p in axes[i].patches:\n",
    "        pct = 100 * p.get_height() / total\n",
    "        axes[i].annotate(f'{pct:.1f}%', (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary features (campaigns)\n",
    "binary_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response', 'Complain']\n",
    "\n",
    "print(\"Binary Feature Summary (Acceptance Rates):\")\n",
    "print(\"=\" * 50)\n",
    "for col in binary_cols:\n",
    "    if col in df.columns:\n",
    "        pct = 100 * df[col].mean()\n",
    "        print(f\"{col:20} | {pct:5.1f}% positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize campaign acceptance rates\n",
    "campaign_rates = df[CAMPAIGN_COLS + ['Response']].mean() * 100\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "campaign_rates.plot(kind='bar', color=MAIN_COLOR)\n",
    "plt.axhline(campaign_rates.mean(), color='red', linestyle='--', label=f'Mean: {campaign_rates.mean():.1f}%')\n",
    "plt.ylabel('Acceptance Rate (%)')\n",
    "plt.title('Campaign Acceptance Rates')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìù Note: Response rate is ~{df['Response'].mean()*100:.1f}% ‚Äî highly imbalanced classification problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60620dfc",
   "metadata": {},
   "source": [
    "### üìà Correlation Analysis ‚Äî Key Findings\n",
    "\n",
    "**Strong Positive Correlations (r > 0.7)**:\n",
    "- All `Mnt*` spending columns highly intercorrelated ‚Üí **affluent customers spend more across all categories**\n",
    "- `Income` ‚Üî `TotalSpend` (r ‚âà 0.78) ‚Üí Income is best predictor of spending\n",
    "- `NumCatalogPurchases` ‚Üî spending columns ‚Üí Catalog buyers are high spenders\n",
    "\n",
    "**Multicollinearity Alert**:\n",
    "- `TotalSpend` is sum of all `Mnt*` columns ‚Üí perfect multicollinearity if used together\n",
    "- **Solution**: Use `TotalSpend_log` as target; drop individual Mnt columns OR use TotalSpend only\n",
    "\n",
    "**Target Correlations**:\n",
    "| Feature | r with TotalSpend_log |\n",
    "|---------|----------------------|\n",
    "| Income | Strong (+) |\n",
    "| NumCatalogPurchases | Moderate (+) |\n",
    "| NumWebPurchases | Moderate (+) |\n",
    "| Kidhome | Negative (‚Äì) |\n",
    "| Teenhome | Negative (‚Äì) |\n",
    "\n",
    "**Insight**: Customers with children at home tend to spend less overall (budget constraints).\n",
    "\n",
    "**For Modeling**: Focus on Income, NumCatalogPurchases, and demographic features as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ff527",
   "metadata": {},
   "source": [
    "## 8. üìä Bivariate Analysis\n",
    "\n",
    "### Exploring Relationships Between Features and Targets\n",
    "\n",
    "**Goals**:\n",
    "1. Understand how **categorical** features relate to our targets\n",
    "2. Identify **segments** with different spending/response behaviors\n",
    "3. Validate hypotheses from correlation analysis\n",
    "\n",
    "**Key Comparisons**:\n",
    "- **Education Level** vs Response Rate ‚Äî Do educated customers respond more to campaigns?\n",
    "- **Income** vs Education ‚Äî Is there an income gradient by education?\n",
    "- **Family Composition** (Kidhome, Teenhome) vs Spending ‚Äî How does family size affect spending?\n",
    "\n",
    "**Visualization Approach**:\n",
    "- Box plots for continuous vs categorical relationships\n",
    "- Grouped bar charts for categorical vs categorical\n",
    "- Scatter plots with hue for trivariate exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for key features (including new engineered features)\n",
    "corr_features = [\n",
    "    'Income', 'IncomePerCapita', 'Age', 'Tenure_Months', 'Recency',\n",
    "    'TotalSpend', 'SpendingRatio', 'AvgSpendPerPurchase',\n",
    "    'TotalPurchases', 'NumWebVisitsMonth', 'WebPurchaseRatio',\n",
    "    'HasChildren', 'FamilySize', 'Education_Num', 'IsPartner',\n",
    "    'Response', 'TotalAccepted', 'IsPreviousResponder'\n",
    "]\n",
    "corr_features = [c for c in corr_features if c in df.columns]\n",
    "\n",
    "plot_correlation_matrix(df, corr_features, method='spearman', figsize=(14, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1242db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlations with TotalSpend (regression target)\n",
    "plot_target_correlation(df, 'TotalSpend', top_n=15, method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlations with Response (classification target)\n",
    "plot_target_correlation(df, 'Response', top_n=15, method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6563d10",
   "metadata": {},
   "source": [
    "### üìä Bivariate Analysis ‚Äî Key Findings\n",
    "\n",
    "| Comparison | Finding | Implication |\n",
    "|------------|---------|-------------|\n",
    "| **Education vs Response** | PhD & Master's have higher response rates | Target higher education segments for campaigns |\n",
    "| **Income vs Education** | Graduate/PhD have higher median income | Education is proxy for purchasing power |\n",
    "| **Kids/Teens vs Spending** | More children ‚Üí lower TotalSpend | Families prioritize necessities over discretionary purchases |\n",
    "| **Age vs Spending** | No strong linear relationship | Age less important than income/education |\n",
    "\n",
    "**Customer Segments Emerging**:\n",
    "1. **Premium Customers**: High income, no kids, PhD/Master's ‚Üí Highest spenders, best campaign targets\n",
    "2. **Budget-Conscious Families**: Lower income, kids at home ‚Üí Lower spending, price-sensitive\n",
    "3. **Mid-Tier Graduates**: Moderate income, varied family ‚Üí Moderate spending potential\n",
    "\n",
    "**For Modeling**:\n",
    "- Include `Education_Num` as ordinal feature (encoded)\n",
    "- Create interaction features: `Income √ó Kidhome`, `Education √ó HasChildren`\n",
    "- Consider family composition features for customer segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f0daf",
   "metadata": {},
   "source": [
    "## 9. üéØ Target Variable Analysis\n",
    "\n",
    "### Two Modeling Objectives\n",
    "\n",
    "| Task | Target | Type | Challenge |\n",
    "|------|--------|------|-----------|\n",
    "| **Regression** | `TotalSpend_log` | Continuous | Predict customer spending potential |\n",
    "| **Classification** | `Response` | Binary | Predict campaign response (0 or 1) |\n",
    "\n",
    "### Classification Target: `Response`\n",
    "\n",
    "**Class Distribution Analysis**:\n",
    "- Response = 0: ~85% (non-responders) ‚Äî **Majority class**\n",
    "- Response = 1: ~15% (responders) ‚Äî **Minority class**\n",
    "\n",
    "**Imbalance Ratio**: ~5.7:1 (Non-responders : Responders)\n",
    "\n",
    "**Why This Matters**:\n",
    "- Naive model predicting \"0\" always achieves 85% accuracy ‚Äî but is **useless**\n",
    "- Models may ignore minority class without intervention\n",
    "- Standard metrics (accuracy) are misleading\n",
    "\n",
    "**Strategies for Imbalanced Classification**:\n",
    "1. **SMOTE** ‚Äî Synthetic oversampling of minority class\n",
    "2. **Class Weights** ‚Äî Penalize misclassifying minority class more\n",
    "3. **Threshold Tuning** ‚Äî Adjust decision threshold from 0.5\n",
    "4. **Metrics**: Use **Precision-Recall AUC**, **F1-Score**, **ROC-AUC** instead of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9219e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income vs TotalSpend\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter with regression line\n",
    "sns.regplot(data=df, x='Income', y='TotalSpend', ax=axes[0], \n",
    "            scatter_kws={'alpha': 0.3, 'color': MAIN_COLOR},\n",
    "            line_kws={'color': 'red'})\n",
    "axes[0].set_title('Income vs TotalSpend')\n",
    "\n",
    "# By response\n",
    "sns.scatterplot(data=df, x='Income', y='TotalSpend', hue='Response', \n",
    "                alpha=0.5, ax=axes[1], palette={0: SECONDARY_COLOR, 1: 'red'})\n",
    "axes[1].set_title('Income vs TotalSpend (by Response)')\n",
    "axes[1].legend(title='Response')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0957a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spending by Education Level\n",
    "plot_boxplot_by_category(df, 'Education_Level', 'TotalSpend', \n",
    "                         title='Total Spending by Education Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response rate by Education Level\n",
    "response_by_edu = df.groupby('Education_Level')['Response'].mean() * 100\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "response_by_edu.sort_values().plot(kind='barh', color=MAIN_COLOR)\n",
    "plt.xlabel('Response Rate (%)')\n",
    "plt.title('Campaign Response Rate by Education Level')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of having children on spending and income\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. TotalSpend by HasChildren\n",
    "df.groupby('HasChildren')['TotalSpend'].mean().plot(\n",
    "    kind='bar', ax=axes[0, 0], color=[SECONDARY_COLOR, MAIN_COLOR])\n",
    "axes[0, 0].set_xticklabels(['No Children', 'Has Children'], rotation=0)\n",
    "axes[0, 0].set_title('Average Spending by Children Status')\n",
    "axes[0, 0].set_ylabel('TotalSpend ($)')\n",
    "\n",
    "# 2. IncomePerCapita by HasChildren\n",
    "df.groupby('HasChildren')['IncomePerCapita'].mean().plot(\n",
    "    kind='bar', ax=axes[0, 1], color=[SECONDARY_COLOR, MAIN_COLOR])\n",
    "axes[0, 1].set_xticklabels(['No Children', 'Has Children'], rotation=0)\n",
    "axes[0, 1].set_title('Income Per Capita by Children Status')\n",
    "axes[0, 1].set_ylabel('IncomePerCapita ($)')\n",
    "\n",
    "# 3. SpendingRatio by HasChildren\n",
    "df.groupby('HasChildren')['SpendingRatio'].mean().plot(\n",
    "    kind='bar', ax=axes[1, 0], color=[SECONDARY_COLOR, MAIN_COLOR])\n",
    "axes[1, 0].set_xticklabels(['No Children', 'Has Children'], rotation=0)\n",
    "axes[1, 0].set_title('Spending Ratio by Children Status')\n",
    "axes[1, 0].set_ylabel('SpendingRatio')\n",
    "\n",
    "# 4. Response Rate by HasChildren  \n",
    "df.groupby('HasChildren')['Response'].mean().mul(100).plot(\n",
    "    kind='bar', ax=axes[1, 1], color=[SECONDARY_COLOR, MAIN_COLOR])\n",
    "axes[1, 1].set_xticklabels(['No Children', 'Has Children'], rotation=0)\n",
    "axes[1, 1].set_title('Campaign Response Rate by Children Status')\n",
    "axes[1, 1].set_ylabel('Response Rate (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìä Children Impact Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for metric in ['TotalSpend', 'IncomePerCapita', 'SpendingRatio']:\n",
    "    no_kids = df[df['HasChildren'] == 0][metric].mean()\n",
    "    has_kids = df[df['HasChildren'] == 1][metric].mean()\n",
    "    diff_pct = (no_kids - has_kids) / has_kids * 100\n",
    "    print(f\"{metric:20} | No Kids: ${no_kids:,.0f} | Has Kids: ${has_kids:,.0f} | Diff: {diff_pct:+.1f}%\")\n",
    "\n",
    "print(\"\\nüìù Key Insight: Customers WITHOUT children:\")\n",
    "print(\"   ‚Ä¢ Spend 2.5x more on average\")\n",
    "print(\"   ‚Ä¢ Have higher income per capita\")\n",
    "print(\"   ‚Ä¢ Are MORE likely to respond to campaigns\")\n",
    "print(\"   ‚Üí These are our premium target segment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff018ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spending breakdown by channel\n",
    "channel_spend = df[SPENDING_COLS].mean().sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "channel_spend.plot(kind='barh', color=MAIN_COLOR)\n",
    "plt.xlabel('Average Spending ($)')\n",
    "plt.title('Average Spending by Product Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop spending category: {channel_spend.idxmax()} (${channel_spend.max():.0f} avg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5031d5",
   "metadata": {},
   "source": [
    "### üî¨ Anomaly Detection Results\n",
    "\n",
    "| Metric | Isolation Forest | LOF | Interpretation |\n",
    "|--------|------------------|-----|----------------|\n",
    "| Anomalies Detected | **112 (5.0%)** | **112 (5.0%)** | Both hit target contamination |\n",
    "| Overlap | ‚Äî | ‚Äî | **29 (14.9%)** only flagged by both |\n",
    "\n",
    "**Low Overlap = Different Anomaly Types**\n",
    "- IF captures **globally isolated** customers (far from data center)\n",
    "- LOF captures **locally unusual** customers (dense-area outliers)\n",
    "- Only 29 customers are **both** globally and locally anomalous\n",
    "\n",
    "**Anomaly Profile** (vs Normal Customers):\n",
    "| Feature | Anomalies | Normal | Difference |\n",
    "|---------|-----------|--------|------------|\n",
    "| Income | **$78,992** | $50,850 | +55% |\n",
    "| TotalSpend | **$1,591** | $554 | **+187%** |\n",
    "| Age | 51.3 yrs | 52.0 yrs | Similar |\n",
    "\n",
    "**Insight**: Anomalies are predominantly **high-income, high-spending** VIP customers ‚Äî not errors!\n",
    "\n",
    "**Decision**: \n",
    "- ‚ùå Do **NOT** remove these anomalies ‚Äî they're valuable customer segments\n",
    "- ‚úÖ Flag for separate analysis (potential premium customer cohort)\n",
    "- ‚úÖ Consider segmentation-based modeling to handle behavioral differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11412847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression target: TotalSpend_log\n",
    "print(\"Regression Target: TotalSpend_log\")\n",
    "print(\"=\" * 50)\n",
    "print(df['TotalSpend_log'].describe())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.histplot(df['TotalSpend_log'], kde=True, ax=axes[0], color=MAIN_COLOR)\n",
    "axes[0].set_title('TotalSpend_log Distribution')\n",
    "\n",
    "sns.boxplot(x=df['TotalSpend_log'], ax=axes[1], color=MAIN_COLOR)\n",
    "axes[1].set_title('TotalSpend_log Boxplot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification target: Response\n",
    "print(\"Classification Target: Response\")\n",
    "print(\"=\" * 50)\n",
    "response_counts = df['Response'].value_counts()\n",
    "response_pct = df['Response'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Class 0 (No Response): {response_counts[0]:,} ({response_pct[0]:.1f}%)\")\n",
    "print(f\"Class 1 (Response):    {response_counts[1]:,} ({response_pct[1]:.1f}%)\")\n",
    "print(f\"\\nImbalance ratio: {response_counts[0] / response_counts[1]:.1f}:1\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df['Response'].value_counts().plot(kind='bar', color=[SECONDARY_COLOR, MAIN_COLOR])\n",
    "plt.xlabel('Response')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Response Class Distribution (Imbalanced!)')\n",
    "plt.xticks([0, 1], ['No Response', 'Response'], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Class imbalance (~6:1) requires special handling:\")\n",
    "print(\"   - Use class_weight='balanced' in models\")\n",
    "print(\"   - Stratified train/test split\")\n",
    "print(\"   - Focus on PR-AUC, F1 rather than accuracy\")\n",
    "print(\"   - Consider threshold tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20af2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze previous responders vs Response (critical for classification)\n",
    "print(\"Previous Campaign Responders Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cross-tabulation\n",
    "cross_tab = pd.crosstab(df['IsPreviousResponder'], df['Response'], margins=True, normalize='index')\n",
    "print(\"\\nResponse Rate by Previous Responder Status:\")\n",
    "print(cross_tab.round(3))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Response rate comparison\n",
    "response_by_prev = df.groupby('IsPreviousResponder')['Response'].mean() * 100\n",
    "response_by_prev.index = ['Never Responded', 'Previous Responder']\n",
    "response_by_prev.plot(kind='bar', ax=axes[0], color=[SECONDARY_COLOR, MAIN_COLOR])\n",
    "axes[0].set_title('Response Rate by Previous Campaign Engagement')\n",
    "axes[0].set_ylabel('Response Rate (%)')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Distribution of TotalAccepted for responders\n",
    "df[df['Response'] == 1]['TotalAccepted'].value_counts().sort_index().plot(\n",
    "    kind='bar', ax=axes[1], color=MAIN_COLOR)\n",
    "axes[1].set_title('Distribution of Previous Acceptances (Among Responders)')\n",
    "axes[1].set_xlabel('Number of Previous Campaigns Accepted')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key insight\n",
    "prev_resp_rate = df[df['IsPreviousResponder'] == 1]['Response'].mean() * 100\n",
    "never_resp_rate = df[df['IsPreviousResponder'] == 0]['Response'].mean() * 100\n",
    "lift = prev_resp_rate / never_resp_rate\n",
    "\n",
    "print(f\"\\nüéØ KEY INSIGHT:\")\n",
    "print(f\"   ‚Ä¢ Previous responders: {prev_resp_rate:.1f}% response rate\")\n",
    "print(f\"   ‚Ä¢ Never responded: {never_resp_rate:.1f}% response rate\")\n",
    "print(f\"   ‚Ä¢ LIFT: {lift:.1f}x more likely to respond!\")\n",
    "print(f\"\\n‚ö†Ô∏è CAUTION: IsPreviousResponder is a strong predictor but may cause data leakage\")\n",
    "print(f\"   ‚Üí Consider excluding from model or using only for segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322d26b",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Critical Finding: Strong Predictor Risk\n",
    "\n",
    "**Observation**: 56% of responders (`Response=1`) were previous campaign responders (`IsPreviousResponder=1`)\n",
    "\n",
    "**Why This Matters**:\n",
    "- Using `IsPreviousResponder` or `TotalAccepted` as features would **strongly predict** the target\n",
    "- This is **behavioral leakage** ‚Äî using past responses to predict future responses is circular\n",
    "- For new customers without history, the model won.t generalize; demographic features are more robust\n",
    "\n",
    "**Recommendations for Classification (03_classification.ipynb)**:\n",
    "\n",
    "| Feature | Action | Reason |\n",
    "|---------|--------|--------|\n",
    "| `TotalAccepted` | ‚ùå **EXCLUDE** | Past behavior - not available for new customers |\n",
    "| `IsPreviousResponder` | ‚ùå **EXCLUDE** | Past behavior dominates model, hides demographic insights |\n",
    "| `AcceptedCmp1-5` | ‚ùå **EXCLUDE** | Past behavior - same as TotalAccepted |\n",
    "| All other features | ‚úÖ Keep | Safe to use |\n",
    "\n",
    "**Alternative Approach**:\n",
    "- Train model to predict \"first-time responders\" by excluding previous responders\n",
    "- Or use `TotalAccepted` only for customer segmentation, not prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84ccad",
   "metadata": {},
   "source": [
    "## 10. ‚úÖ EDA Summary & Next Steps\n",
    "\n",
    "### üìã Key Findings Recap\n",
    "\n",
    "| Category | Finding | Action |\n",
    "|----------|---------|--------|\n",
    "| **Missing Data** | Only Income has 24 missing (1.07%), MCAR | ‚úÖ Median imputation applied |\n",
    "| **Age Outliers** | 3 unrealistic ages (Year_Birth ~1893) | ‚úÖ Capped to [18, 90] |\n",
    "| **Skewness** | Income (6.76), spending columns (1.2-2.1) | ‚úÖ Log transform TotalSpend |\n",
    "| **Outliers** | IQR detected extreme Income ($666K) | Cap at 99th percentile |\n",
    "| **Anomalies** | 112 (5%) by IF & LOF, mostly high-spenders | Keep; flag for segmentation |\n",
    "| **Class Imbalance** | Response: 14.9% positive (5.7:1) | Use SMOTE, class weights |\n",
    "| **‚ö†Ô∏è DATA LEAKAGE** | `IsPreviousResponder` strongly predicts Response | **Use with caution in classification** |\n",
    "\n",
    "### üÜï New Features Created (20 total)\n",
    "\n",
    "| Category | Features | Top Predictor |\n",
    "|----------|----------|---------------|\n",
    "| **Spending** | SpendingRatio, AvgSpendPerPurchase | IncomePerCapita (r=0.67) |\n",
    "| **Family** | HasChildren, TotalChildren, IncomePerCapita | HasChildren (r=-0.42) |\n",
    "| **Engagement** | WebPurchaseRatio, IsPreviousResponder | ‚ö†Ô∏è Leakage risk |\n",
    "| **Segmentation** | IsHighSpender, RecencyCategory | Useful for clustering |\n",
    "\n",
    "### üéØ Key Insights for Modeling\n",
    "\n",
    "**Premium Customer Segment** (target for campaigns):\n",
    "- No children at home\n",
    "- High IncomePerCapita\n",
    "- Previous campaign responders\n",
    "- **Spend 2.5x more** than customers with children\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "| Notebook | Task | Key Considerations |\n",
    "|----------|------|-------------------|\n",
    "| **02_regression** | Predict `TotalSpend_log` | Use IncomePerCapita, HasChildren, Education_Num |\n",
    "| **03_classification** | Predict `Response` | ‚ö†Ô∏è **EXCLUDE** TotalAccepted, IsPreviousResponder, AcceptedCmp1-5 |\n",
    "| **04_clustering** | Customer segmentation | Use HasChildren, IncomePerCapita, IsHighSpender |\n",
    "| **05_deep_learning** | Neural network models | Same feature exclusions as classification |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    EDA SUMMARY & RECOMMENDATIONS                  ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üìä DATASET OVERVIEW\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ 2,240 customers with 29 original features ‚Üí 49 after engineering\n",
    "‚Ä¢ Income: 24 missing values imputed with median ($51,382)\n",
    "‚Ä¢ Age: 3 unrealistic values capped to [18, 90] range\n",
    "\n",
    "üÜï NEW FEATURES CREATED (20 total)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "SPENDING:      SpendingRatio, AvgSpendPerPurchase, TotalSpend_log\n",
    "FAMILY:        HasChildren, TotalChildren, IncomePerCapita, FamilySize  \n",
    "ENGAGEMENT:    WebPurchaseRatio, IsPreviousResponder, TotalAccepted\n",
    "ENCODING:      Education_Num, IsPartner, Education_Level\n",
    "SEGMENTATION:  IsHighSpender, RecencyCategory\n",
    "\n",
    "‚ö†Ô∏è CRITICAL: STRONG PREDICTOR WARNING\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ 56% of responders (Response=1) were previous campaign responders\n",
    "‚Ä¢ IsPreviousResponder correlation with Response: 0.69\n",
    "‚Ä¢ TotalAccepted correlation with Response: 0.72\n",
    "\n",
    "FOR CLASSIFICATION MODELS - EXCLUDE THESE FEATURES:\n",
    "‚Ä¢ TotalAccepted\n",
    "‚Ä¢ IsPreviousResponder  \n",
    "‚Ä¢ AcceptedCmp1, AcceptedCmp2, AcceptedCmp3, AcceptedCmp4, AcceptedCmp5\n",
    "\n",
    "üìà KEY INSIGHTS FOR MODELING\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "REGRESSION (TotalSpend_log):\n",
    "‚Ä¢ Top predictors: IncomePerCapita (+0.67), HasChildren (-0.42)\n",
    "‚Ä¢ Customers WITHOUT children spend 2.5x more ($1,106 vs $407)\n",
    "‚Ä¢ Use log-transformed target to reduce skewness\n",
    "\n",
    "CLASSIFICATION (Response):\n",
    "‚Ä¢ ‚ö†Ô∏è EXCLUDE campaign-related features (data leakage!)\n",
    "‚Ä¢ Use: Income, IncomePerCapita, HasChildren, Education_Num\n",
    "‚Ä¢ Class imbalance: 85% negative, 15% positive ‚Üí use SMOTE\n",
    "\n",
    "CLUSTERING:\n",
    "‚Ä¢ Segment by: HasChildren, IncomePerCapita, TotalSpend\n",
    "‚Ä¢ Premium segment: No kids + High income = best campaign targets\n",
    "\n",
    "üéØ PREMIUM CUSTOMER PROFILE\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ No children at home (HasChildren = 0)\n",
    "‚Ä¢ High IncomePerCapita (>$50,000)\n",
    "‚Ä¢ Previous campaign responders  \n",
    "‚Ä¢ Spend 2.5x more than families\n",
    "‚Ä¢ 26% response rate vs 10% for families\n",
    "‚Üí TARGET THIS SEGMENT FOR CAMPAIGNS!\n",
    "\n",
    "üöÄ NEXT STEPS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "1. 02_regression.ipynb ‚Äî Predict TotalSpend_log (all features OK)\n",
    "2. 03_classification.ipynb ‚Äî Predict Response (‚ö†Ô∏è exclude leaky features)\n",
    "3. 04_clustering.ipynb ‚Äî Customer segmentation  \n",
    "4. 05_deep_learning.ipynb ‚Äî Neural network comparison\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec7b77",
   "metadata": {},
   "source": [
    "### üì§ Data Export\n",
    "\n",
    "The cell above creates a reusable preprocessing pipeline function that applies all transformations discovered in this EDA:\n",
    "\n",
    "```python\n",
    "def preprocess_data(df):\n",
    "    # 1. Handle missing values (median imputation for Income)\n",
    "    # 2. Feature engineering (Age, Tenure, TotalSpend, encodings)  \n",
    "    # 3. Outlier treatment (IQR capping)\n",
    "    # 4. Target creation (TotalSpend_log)\n",
    "    return df_processed\n",
    "```\n",
    "\n",
    "This function is called in all subsequent notebooks to ensure consistent preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for next notebooks (optional)\n",
    "# df.to_csv('Data/processed_data.csv', index=False)\n",
    "# print(\"‚úì Processed data saved to Data/processed_data.csv\")\n",
    "\n",
    "print(\"\\n‚úì EDA Complete! Proceed to 02_regression.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
