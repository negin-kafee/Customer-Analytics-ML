# Customer Analytics ML Pipeline

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.6+-orange.svg)](https://scikit-learn.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.20+-red.svg)](https://www.tensorflow.org/)
[![XGBoost](https://img.shields.io/badge/XGBoost-2.0+-green.svg)](https://xgboost.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> **End-to-end machine learning pipeline for customer analytics**: Spending prediction, campaign response classification, and customer segmentation using scikit-learn, XGBoost, and TensorFlow.

---

## ğŸ¯ Key Results

| Task | Best Model | Performance | Business Impact |
|------|------------|-------------|------------------|
| **Spending Prediction** | Random Forest | RÂ² = **0.970** | Predict 97% of spending variance |
| **Campaign Response (Unified)** | Random Forest | ROC-AUC = **0.875** | Identify 85%+ of responders |
| **Campaign Response (Segmented)** | 5 Specialized Models | Activity-based segments | Targeted strategies per customer type |
| **Customer Segmentation** | K-Means (Optimized) | Silhouette = **0.614** | 4 actionable segments |
| **Deep Learning** | MLP | RÂ² = 0.953 / AUC = 0.871 | Competitive with tree models |

### ğŸ“Š Model Performance Visualization

```
REGRESSION (RÂ² Score)
Random Forest   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 97.0%
XGBoost         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  96.9%
MLP Neural Net  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    95.3%

CLASSIFICATION (ROC-AUC)
Random Forest   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 87.5%
XGBoost         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  87.2%
MLP Neural Net  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  87.1%

CLUSTERING (Silhouette Score - After Optimization)
Optimized       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    61.4%
Original        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 16.5%
                â†‘ 272% improvement through feature engineering
```

---

## ğŸ“‹ Project Overview

This project demonstrates a **complete machine learning workflow** for customer analytics, from exploratory data analysis to production-ready models. It showcases:

- **15+ ML algorithms** compared and evaluated
- **Custom sklearn transformers** for reproducible preprocessing
- **Hyperparameter optimization** with cross-validation
- **Imbalanced classification** handling (15% positive class)
- **Clustering optimization** achieving 272% improvement
- **Deep learning** with TensorFlow/Keras
- **Cold Start handling** for new customer predictions

### Business Questions Answered

1. ğŸ’° **How much will a customer spend?** â†’ Regression models predict with 97% accuracy (existing) / 78% (new customers)
2. ğŸ“§ **Will they respond to campaigns?** â†’ Unified classification identifies 85% of responders
3. ğŸ¯ **How to target different customer types?** â†’ 5-segment approach with activity-based targeting
4. ğŸ‘¥ **What customer segments exist?** â†’ 4 distinct, actionable segments identified
5. ğŸ§  **Can deep learning help?** â†’ Competitive but tree models win on small data
6. ğŸ†• **What about NEW customers?** â†’ Demographics-only model achieves RÂ² = 0.78

## ğŸ“ Project Structure

```
customer-analytics-ml/
â”œâ”€â”€ ğŸ““ notebooks/                       # Jupyter analysis notebooks
â”‚   â”œâ”€â”€ 01_eda.ipynb                   # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_regression.ipynb            # Spending Prediction (RÂ²=0.97) - Existing Customers
â”‚   â”œâ”€â”€ 02b_regression_new_customers.ipynb  # Cold Start Model (RÂ²=0.78) - New Customers
â”‚   â”œâ”€â”€ 03_classification.ipynb        # Response Prediction (AUC=0.875) - Unified Model
â”‚   â”œâ”€â”€ 03b_classification_segmented.ipynb  # Advanced Segmented Models (AUC=0.860)
â”‚   â”œâ”€â”€ 04_clustering.ipynb            # Customer Segmentation (Sil=0.614)
â”‚   â”œâ”€â”€ 05_deep_learning.ipynb         # Neural Networks
â”‚   â””â”€â”€ README.md                      # Notebook execution guide
â”‚
â”œâ”€â”€ ğŸ“¦ src/                            # Reusable Python modules
â”‚   â”œâ”€â”€ config.py                      # Hyperparameters & settings
â”‚   â”œâ”€â”€ preprocessing.py               # Custom sklearn transformers
â”‚   â”œâ”€â”€ models.py                      # Model factory functions
â”‚   â”œâ”€â”€ evaluation.py                  # Metrics & threshold optimization
â”‚   â”œâ”€â”€ visualization.py               # Plotting utilities
â”‚   â””â”€â”€ data_loader.py                 # Data loading
â”‚
â”œâ”€â”€ ğŸ“Š data/                           # Data management
â”‚   â”œâ”€â”€ raw/                          # Original datasets
â”‚   â”‚   â””â”€â”€ marketing_campaign.csv    # Customer dataset (2,240 records)
â”‚   â”œâ”€â”€ interim/                      # Intermediate processing files
â”‚   â”œâ”€â”€ processed/                    # Final, analysis-ready data
â”‚   â””â”€â”€ README.md                     # Data documentation
â”‚
â”œâ”€â”€ ğŸ“ˆ docs/                          # Documentation & reports
â”‚   â”œâ”€â”€ 00_executive_summary.md       # Cross-model comparison
â”‚   â”œâ”€â”€ 01_eda_analysis.md            # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_regression_analysis.md
â”‚   â”œâ”€â”€ 03_classification_analysis.md
â”‚   â”œâ”€â”€ 04_clustering_analysis.md
â”‚   â””â”€â”€ 05_deep_learning_analysis.md
â”‚
â”œâ”€â”€ ğŸ¤– models/                        # Saved models (generated by notebooks)
â”œâ”€â”€ ğŸ§ª tests/                         # Unit tests
â”œâ”€â”€ ğŸ”§ scripts/                       # Utility scripts
â”œâ”€â”€ ğŸ“„ pyproject.toml                 # Project configuration & dependencies
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md                # Development guidelines
â”œâ”€â”€ ğŸ“„ CHANGELOG.md                   # Version history
â””â”€â”€ ğŸ“„ Makefile                       # Development commands
â”‚   â””â”€â”€ data_loader.py                  # Data loading
â”‚
â”œâ”€â”€ ğŸ“Š Data/
â”‚   â””â”€â”€ marketing_campaign.csv          # Customer dataset (2,240 records)
â”‚
â”œâ”€â”€ ğŸ“ˆ reports/                         # Analysis reports (Markdown)
â”‚   â”œâ”€â”€ 00_executive_summary.md         # Cross-model comparison
â”‚   â”œâ”€â”€ 01_eda_analysis.md              # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_regression_analysis.md
â”‚   â”œâ”€â”€ 03_classification_analysis.md
â”‚   â”œâ”€â”€ 04_clustering_analysis.md
â”‚   â””â”€â”€ 05_deep_learning_analysis.md
â”‚
â”œâ”€â”€ ğŸ¤– models/                          # Saved models (generated by notebooks)
â”‚   â”œâ”€â”€ best_regressor.joblib           # Run 02_regression.ipynb to generate
â”‚   â”œâ”€â”€ best_classifier.joblib          # Run 03_classification.ipynb to generate
â”‚   â”œâ”€â”€ kmeans_model.joblib             # Run 04_clustering.ipynb to generate
â”‚   â””â”€â”€ mlp_*.keras                     # Run 05_deep_learning.ipynb to generate
â”‚
â”œâ”€â”€ pyproject.toml                       # Project configuration & dependencies
â””â”€â”€ README.md
```

## ğŸ“Š Dataset

**Source**: [Kaggle - Customer Personality Analysis](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis)

| Property | Value |
|----------|-------|
| **Samples** | 2,240 customers |
| **Features** | 29 original â†’ 20+ engineered |
| **Regression Target** | `TotalSpend_log` (log-transformed spending) |
| **Classification Target** | `Response` (15% positive - imbalanced) |

### Feature Categories

| Category | Features | Count |
|----------|----------|-------|
| ğŸ’° **Value** | Income, TotalSpend | 2 |
| ğŸ›’ **Products** | MntWines, MntMeatProducts, MntFruits, etc. | 6 |
| ğŸ“± **Channels** | NumWebPurchases, NumCatalogPurchases, NumStorePurchases | 4 |
| ğŸ‘¤ **Demographics** | Age, Education, Marital_Status, Kidhome, Teenhome | 5 |
| â° **Engagement** | Recency, NumWebVisitsMonth, CustomerTenure | 3 |

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- pip or conda

### Installation

```bash
# Clone the repository
git clone https://github.com/negin-kafee/Customer-Analytics-ML.git
cd Customer-Analytics-ML

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -e .
```

### Run the Analysis

```bash
# Start Jupyter
jupyter notebook

# Run notebooks in order:
# 1. 01_eda.ipynb          â†’ Understand the data
# 2. 02_regression.ipynb   â†’ Predict spending
# 3. 03_classification.ipynb â†’ Predict response
# 4. 04_clustering.ipynb   â†’ Segment customers
# 5. 05_deep_learning.ipynb â†’ Neural networks
```

---

## ğŸ““ Notebook Details

### 1ï¸âƒ£ Exploratory Data Analysis (`01_eda.ipynb`)
- Missing value analysis & imputation strategies
- Distribution analysis with statistical tests
- Correlation heatmaps and feature relationships
- **Anomaly Detection**: IQR, Isolation Forest, LOF

### 2ï¸âƒ£ Regression Modeling (`02_regression.ipynb`)
**Goal**: Predict customer spending

| Model Category | Algorithms |
|----------------|------------|
| Linear | OLS, Ridge, Lasso, ElasticNet |
| Tree-based | Decision Tree, Random Forest, Extra Trees |
| Boosting | Gradient Boosting, **XGBoost** |
| SVM | Linear, RBF, Polynomial kernels |
| Neural | MLP Regressor |

**Best Result**: Random Forest with **RÂ² = 0.970**

### 3ï¸âƒ£ Classification Modeling (`03_classification.ipynb`)
**Goal**: Predict campaign response (imbalanced: 15% positive)

**Techniques**:
- Class weight balancing
- SMOTE oversampling  
- Threshold optimization (cost-sensitive)
- Precision-Recall trade-off analysis

**Best Result**: Random Forest with **ROC-AUC = 0.875**

### 3ï¸âƒ£b Advanced Segmented Classification (`03b_classification_segmented.ipynb`)
**Goal**: Specialized models for different customer types

**Innovation**: 5-segment approach based on customer tenure and spending activity (NOT response history):

| Segment | Definition | Business Strategy |
|---------|------------|-------------------|
| **ColdStart_New** | Brand new customers, no campaign exposure | Uses unified model |
| **Newer_Inactive** | Below median tenure, lower spending | Cost-conscious targeting |
| **Newer_Active** | Below median tenure, higher spending | Loyalty building |
| **Established_Inactive** | Above median tenure, lower spending | Re-engagement |
| **Established_Active** | Above median tenure, higher spending | Premium targeting |

**Key Features**:
- **No behavioral leakage** - segments based on spending activity, not response history
- **Campaign timing discovery**: All campaigns ran after June 2014 (EligibleCampaigns = 5 for all)
- Segment-specific feature engineering
- Production deployment pipeline with automated routing
- Cold start handling for brand new customers

**Best Result**: Random Forest with **ROC-AUC = 0.875**, Recall = 78%

### 4ï¸âƒ£ Clustering Analysis (`04_clustering.ipynb`)
**Goal**: Segment customers into actionable groups

**Optimization Journey**:
| Method | Silhouette | Notes |
|--------|------------|-------|
| Original (17 features) | 0.165 | Baseline |
| Log + StandardScaler | 0.466 | +182% |
| **MinMax + 2 Features** | **0.614** | **+272%** âœ… |

**Final Segments**:
| Segment | Size | Avg Spend | Response Rate |
|---------|------|-----------|---------------|
| ğŸ’ VIP Champions | 13% | $1,753 | 40% |
| ğŸŒŸ High Value | 20% | $1,111 | 20% |
| ğŸ“Š Mid-Tier | 19% | $570 | 10% |
| ğŸ“‰ Budget | 48% | $101 | 10% |

### 5ï¸âƒ£ Deep Learning (`05_deep_learning.ipynb`)
**Goal**: Compare neural networks vs traditional ML

**Architecture**: 3-layer MLP with BatchNorm, Dropout (0.3), Early Stopping

| Task | MLP Performance | Best Traditional | Winner |
|------|-----------------|------------------|--------|
| Regression | RÂ² = 0.953 | RF: RÂ² = 0.970 | RF |
| Classification | AUC = 0.871 | RF: AUC = 0.875 | RF |

**Insight**: Tree-based models outperform deep learning on small tabular datasets (~2K samples)

---

## ğŸ”§ Technical Highlights

### Custom sklearn Transformers

```python
# Located in src/preprocessing.py
from src.preprocessing import IQRCapper, LogTransformer, FeatureEngineer

# Example pipeline
pipeline = Pipeline([
    ('imputer', MedianImputer()),
    ('outlier_cap', IQRCapper(k=1.5)),
    ('log_transform', LogTransformer()),
    ('scaler', StandardScaler())
])
```

### Threshold Optimization

```python
# Cost-sensitive classification (src/evaluation.py)
from src.evaluation import find_cost_optimal_threshold

optimal_threshold, expected_cost, sweep_df = find_cost_optimal_threshold(
    y_true, y_proba, 
    c_fp=1.0,    # Cost of false positive
    c_fn=3.0     # Cost of false negative (3x more expensive)
)
```

### Model Comparison Framework

```python
# Centralized model training (src/models.py)
from src.models import get_regression_models, get_classification_models

models = get_regression_models()  # Returns dict of configured models
for name, model in models.items():
    model.fit(X_train, y_train)
    logger.log_regression(name, model, X_test, y_test)
```

---

## ğŸ“ˆ Key Findings

### 1. Feature Importance (Top 5)
| Rank | Feature | Impact |
|------|---------|--------|
| 1 | **Income** | Primary driver of spending |
| 2 | **Recency** | Key for campaign response |
| 3 | **MntWines** | Highest revenue category |
| 4 | **NumCatalogPurchases** | Signals engagement |
| 5 | **Age** | Correlates with spending |

### 2. Business Recommendations
- **Target VIP segment** (13% of customers â†’ 38% of revenue)
- **Campaign focus**: Recency < 30 days = 3x response rate
- **Channel strategy**: Catalog for high-value, digital for budget

### 3. Model Selection Guide
| Scenario | Recommended | Why |
|----------|-------------|-----|
| Production (speed) | Random Forest | Fast inference, no GPU |
| Interpretability | Random Forest | Feature importances |
| Large data (>100K) | XGBoost or MLP | Better scaling |
| Maximum accuracy | Ensemble (RF + XGB) | Diversity helps |

---

## ğŸ“„ Reports

Detailed analysis reports are available in the `reports/` folder:

| Report | Description |
|--------|-------------|
| [Executive Summary](reports/00_executive_summary.md) | Cross-model comparison & recommendations |
| [EDA Report](reports/01_eda_report.md) | Data exploration findings |
| [Regression Analysis](reports/02_regression_analysis.md) | Spending prediction deep-dive |
| [Classification Analysis](reports/03_classification_analysis.md) | Response prediction analysis |
| [Clustering Analysis](reports/04_clustering_analysis.md) | Segmentation methodology & results |
| [Deep Learning Analysis](reports/05_deep_learning_analysis.md) | Neural network experiments |

---

## ğŸ› ï¸ Tech Stack

| Category | Technologies |
|----------|-------------|
| **Languages** | Python 3.10+ |
| **ML Frameworks** | scikit-learn, XGBoost, TensorFlow/Keras |
| **Data Processing** | pandas, NumPy |
| **Visualization** | matplotlib, seaborn |
| **Notebooks** | Jupyter |

---

## ğŸ‘¤ Author

**Negin Kafee**

[![GitHub](https://img.shields.io/badge/GitHub-Profile-black?style=flat&logo=github)](https://github.com/negin-kafee)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Dataset**: [Customer Personality Analysis](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis) (Kaggle, CC0)
- **Inspiration**: Real-world marketing analytics challenges
- **Tools**: scikit-learn, XGBoost, TensorFlow communities

---

## â­ Star This Repo

If you found this project helpful, please consider giving it a star! It helps others discover it.

```
â­ â†’ Click the star button at the top right!
```
