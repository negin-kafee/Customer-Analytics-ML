{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c40720a",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d61ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, classification_report\n",
    ")\n",
    "\n",
    "# TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, regularizers\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    HAS_TENSORFLOW = True\n",
    "    print(f\"‚úì TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    HAS_TENSORFLOW = False\n",
    "    print(\"‚ö†Ô∏è TensorFlow not installed. Run: pip install tensorflow\")\n",
    "\n",
    "# Our custom modules\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.config import (\n",
    "    DATA_PATH, RANDOM_STATE, TEST_SIZE,\n",
    "    NUM_FEATURES, CAT_FEATURES,\n",
    "    TARGET_REGRESSION, TARGET_CLASSIFICATION,\n",
    "    MAIN_COLOR, SECONDARY_COLOR\n",
    ")\n",
    "from src.data_loader import load_data\n",
    "from src.preprocessing import FeatureEngineer, IQRCapper\n",
    "from src.visualization import set_style, plot_training_history\n",
    "from src.models import build_mlp_regressor, build_mlp_classifier, get_early_stopping\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "set_style()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "if HAS_TENSORFLOW:\n",
    "    tf.random.set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not HAS_TENSORFLOW:\n",
    "    print(\"\\n‚ùå This notebook requires TensorFlow.\")\n",
    "    print(\"   Install with: pip install tensorflow\")\n",
    "    print(\"   Then restart the kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea2354",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665aae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = load_data(DATA_PATH)\n",
    "\n",
    "# Feature engineering\n",
    "engineer = FeatureEngineer()\n",
    "df = engineer.fit_transform(df)\n",
    "\n",
    "# Filter unrealistic ages\n",
    "df = df[df['Age'] <= 100]\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2864043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "feature_cols = NUM_FEATURES + CAT_FEATURES\n",
    "feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Numeric: {[c for c in NUM_FEATURES if c in feature_cols]}\")\n",
    "print(f\"Categorical: {[c for c in CAT_FEATURES if c in feature_cols]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "num_features = [c for c in NUM_FEATURES if c in feature_cols]\n",
    "cat_features = [c for c in CAT_FEATURES if c in feature_cols]\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('capper', IQRCapper(columns=None, k=1.5)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, num_features),\n",
    "    ('cat', categorical_pipeline, cat_features)\n",
    "], remainder='drop')\n",
    "\n",
    "print(\"‚úì Preprocessing pipeline created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744edac",
   "metadata": {},
   "source": [
    "### üìä Data Preprocessing Strategy\n",
    "\n",
    "**Feature Engineering Applied:**\n",
    "- **Outlier Handling**: IQR-based capping (k=1.5) to prevent extreme values from distorting neural network training\n",
    "- **Log Transform**: Target variable (TotalSpend) log-transformed for more normal distribution\n",
    "- **Binary Encoding**: Categorical features converted to binary indicators\n",
    "- **Additional Features**: Engineered features like `TotalPurchases`, `TotalSpend`, age calculations\n",
    "\n",
    "**Why This Matters for Deep Learning:**\n",
    "Neural networks are sensitive to input scale and outliers. Unlike tree-based models, MLPs can struggle with:\n",
    "1. **Outliers**: Can dominate gradient updates\n",
    "2. **Different scales**: Features on larger scales may dominate learning\n",
    "3. **Non-normal distributions**: Can slow convergence\n",
    "\n",
    "Our preprocessing addresses all these concerns before feeding data to the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f3b51",
   "metadata": {},
   "source": [
    "## 3. Deep Learning for Regression\n",
    "\n",
    "Predict customer total spending using a Multi-Layer Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0392618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare regression data\n",
    "X_reg = df[feature_cols].copy()\n",
    "y_reg = df[TARGET_REGRESSION].copy()\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Further split for validation\n",
    "X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(\n",
    "    X_train_reg, y_train_reg, test_size=0.15, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train_reg)}, Val: {len(X_val_reg)}, Test: {len(X_test_reg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f71add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "X_train_reg_processed = preprocessor.fit_transform(X_train_reg)\n",
    "X_val_reg_processed = preprocessor.transform(X_val_reg)\n",
    "X_test_reg_processed = preprocessor.transform(X_test_reg)\n",
    "\n",
    "input_dim = X_train_reg_processed.shape[1]\n",
    "print(f\"Input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ea10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP Regressor\n",
    "if HAS_TENSORFLOW:\n",
    "    model_reg = build_mlp_regressor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=[128, 64, 32],\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    model_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a05bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "if HAS_TENSORFLOW:\n",
    "    callbacks_reg = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bffac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "if HAS_TENSORFLOW:\n",
    "    print(\"Training MLP Regressor...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    history_reg = model_reg.fit(\n",
    "        X_train_reg_processed, y_train_reg,\n",
    "        validation_data=(X_val_reg_processed, y_val_reg),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks_reg,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if HAS_TENSORFLOW:\n",
    "    plot_training_history(history_reg, metrics=['loss', 'mae'], title='MLP Regressor Training History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf82e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate regression model\n",
    "if HAS_TENSORFLOW:\n",
    "    y_pred_train_reg = model_reg.predict(X_train_reg_processed, verbose=0).flatten()\n",
    "    y_pred_test_reg = model_reg.predict(X_test_reg_processed, verbose=0).flatten()\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train_reg, y_pred_train_reg)\n",
    "    test_r2 = r2_score(y_test_reg, y_pred_test_reg)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_test_reg))\n",
    "    test_mae = mean_absolute_error(y_test_reg, y_pred_test_reg)\n",
    "    \n",
    "    print(\"\\nMLP Regressor Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Train R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"Test R¬≤:  {test_r2:.4f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"Test MAE:  {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted\n",
    "if HAS_TENSORFLOW:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[0].scatter(y_test_reg, y_pred_test_reg, alpha=0.5, color=MAIN_COLOR)\n",
    "    axes[0].plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'k--')\n",
    "    axes[0].set_xlabel('Actual')\n",
    "    axes[0].set_ylabel('Predicted')\n",
    "    axes[0].set_title(f'MLP Regressor: Actual vs Predicted (R¬≤={test_r2:.4f})')\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = y_test_reg - y_pred_test_reg\n",
    "    axes[1].hist(residuals, bins=50, color=MAIN_COLOR, alpha=0.7)\n",
    "    axes[1].set_xlabel('Residuals')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Residual Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf913ed",
   "metadata": {},
   "source": [
    "### üéØ MLP Regressor Analysis\n",
    "\n",
    "**Performance Summary:**\n",
    "| Metric | Train | Test |\n",
    "|--------|-------|------|\n",
    "| R¬≤ Score | 0.9457 | 0.9530 |\n",
    "| RMSE | - | 0.3190 |\n",
    "| MAE | - | 0.2517 |\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Excellent Fit (R¬≤ = 0.953)**: The model explains 95.3% of variance in customer spending, demonstrating strong predictive capability.\n",
    "\n",
    "2. **No Overfitting**: Test R¬≤ (0.953) slightly exceeds Train R¬≤ (0.946), indicating the model generalizes well. This is due to:\n",
    "   - Effective dropout regularization (30%)\n",
    "   - Batch normalization stabilizing training\n",
    "   - Early stopping preventing over-training\n",
    "\n",
    "3. **Residual Distribution**: The residual plot shows:\n",
    "   - Residuals centered around zero (unbiased predictions)\n",
    "   - Relatively symmetric distribution\n",
    "   - Few extreme outliers\n",
    "\n",
    "4. **Comparison to Traditional ML**:\n",
    "   - Random Forest R¬≤ = 0.9703\n",
    "   - MLP R¬≤ = 0.9530\n",
    "   - **Difference**: ~1.7% lower, but still excellent performance\n",
    "\n",
    "**Why MLP Performs Slightly Lower:**\n",
    "- Small dataset (~2,200 samples) favors tree-based models\n",
    "- Neural networks typically need more data to outperform\n",
    "- Tree models naturally handle feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3dceb",
   "metadata": {},
   "source": [
    "## 4. Deep Learning for Classification\n",
    "\n",
    "Predict campaign response using a Multi-Layer Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ac24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare classification data\n",
    "X_clf = df[feature_cols].copy()\n",
    "y_clf = df[TARGET_CLASSIFICATION].copy()\n",
    "\n",
    "# Stratified split\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_clf\n",
    ")\n",
    "\n",
    "X_train_clf, X_val_clf, y_train_clf, y_val_clf = train_test_split(\n",
    "    X_train_clf, y_train_clf, test_size=0.15, random_state=RANDOM_STATE, stratify=y_train_clf\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train_clf)}, Val: {len(X_val_clf)}, Test: {len(X_test_clf)}\")\n",
    "print(f\"\\nClass distribution in train: {y_train_clf.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "preprocessor_clf = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, num_features),\n",
    "    ('cat', categorical_pipeline, cat_features)\n",
    "], remainder='drop')\n",
    "\n",
    "X_train_clf_processed = preprocessor_clf.fit_transform(X_train_clf)\n",
    "X_val_clf_processed = preprocessor_clf.transform(X_val_clf)\n",
    "X_test_clf_processed = preprocessor_clf.transform(X_test_clf)\n",
    "\n",
    "input_dim_clf = X_train_clf_processed.shape[1]\n",
    "print(f\"Input dimension: {input_dim_clf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ce282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "if HAS_TENSORFLOW:\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train_clf),\n",
    "        y=y_train_clf\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40249070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP Classifier\n",
    "if HAS_TENSORFLOW:\n",
    "    model_clf = build_mlp_classifier(\n",
    "        input_dim=input_dim_clf,\n",
    "        hidden_layers=[128, 64, 32],\n",
    "        dropout_rate=0.4,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    model_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489eb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for classification\n",
    "if HAS_TENSORFLOW:\n",
    "    callbacks_clf = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d3711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification model\n",
    "if HAS_TENSORFLOW:\n",
    "    print(\"Training MLP Classifier...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    history_clf = model_clf.fit(\n",
    "        X_train_clf_processed, y_train_clf,\n",
    "        validation_data=(X_val_clf_processed, y_val_clf),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks_clf,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if HAS_TENSORFLOW:\n",
    "    plot_training_history(history_clf, metrics=['loss', 'auc'], title='MLP Classifier Training History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b825241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classification model\n",
    "if HAS_TENSORFLOW:\n",
    "    y_proba_test = model_clf.predict(X_test_clf_processed, verbose=0).flatten()\n",
    "    y_pred_test = (y_proba_test >= 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test_clf, y_pred_test)\n",
    "    prec = precision_score(y_test_clf, y_pred_test)\n",
    "    rec = recall_score(y_test_clf, y_pred_test)\n",
    "    f1 = f1_score(y_test_clf, y_pred_test)\n",
    "    roc_auc = roc_auc_score(y_test_clf, y_proba_test)\n",
    "    pr_auc = average_precision_score(y_test_clf, y_proba_test)\n",
    "    \n",
    "    print(\"\\nMLP Classifier Results (threshold=0.5):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC:    {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR curves\n",
    "if HAS_TENSORFLOW:\n",
    "    from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test_clf, y_proba_test)\n",
    "    axes[0].plot(fpr, tpr, color=MAIN_COLOR, lw=2, label=f'AUC = {roc_auc:.3f}')\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('ROC Curve')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PR Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test_clf, y_proba_test)\n",
    "    axes[1].plot(recall, precision, color=MAIN_COLOR, lw=2, label=f'AP = {pr_auc:.3f}')\n",
    "    axes[1].axhline(y_test_clf.mean(), color='k', linestyle='--', label=f'Baseline = {y_test_clf.mean():.3f}')\n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].set_title('Precision-Recall Curve')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('MLP Classifier Performance', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b22752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "if HAS_TENSORFLOW:\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_clf, y_pred_test, target_names=['No Response', 'Response']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec72caa",
   "metadata": {},
   "source": [
    "### üéØ MLP Classifier Analysis\n",
    "\n",
    "**Performance Summary:**\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| ROC-AUC | 0.8706 |\n",
    "| PR-AUC | 0.5072 |\n",
    "| Accuracy | ~85% |\n",
    "| Precision (Response) | 43% |\n",
    "| Recall (Response) | 82% |\n",
    "| F1 Score (Response) | 0.56 |\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Strong Discrimination (ROC-AUC = 0.871)**: The model effectively ranks customers by response probability. This is comparable to the Random Forest classifier (0.8751).\n",
    "\n",
    "2. **High Recall (82%)**: The model captures 82% of actual responders, making it valuable for marketing campaigns where missing potential customers is costly.\n",
    "\n",
    "3. **Precision Trade-off (43%)**: Lower precision means some false positives, but this is acceptable given:\n",
    "   - The class imbalance (only ~15% responders)\n",
    "   - Marketing campaigns can tolerate some non-responders\n",
    "   - High recall ensures we don't miss valuable customers\n",
    "\n",
    "4. **Class Imbalance Handling**: \n",
    "   - Class weights {0: 0.588, 1: 3.348} compensate for imbalanced data\n",
    "   - This prioritizes correctly identifying the minority class (responders)\n",
    "\n",
    "5. **PR-AUC (0.507)**: Moderate precision-recall, which is expected with severe class imbalance. This metric is more conservative than ROC-AUC for imbalanced problems.\n",
    "\n",
    "**Business Implications:**\n",
    "- Use for **campaign targeting**: High recall captures most potential responders\n",
    "- Expected **campaign efficiency**: ~43% of targeted customers respond\n",
    "- **ROI calculation**: If campaign cost per contact is low, high recall strategy is profitable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2589ed97",
   "metadata": {},
   "source": [
    "## 5. Comparison with Traditional ML\n",
    "\n",
    "Load results from previous notebooks and compare with deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65880f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison summary\n",
    "if HAS_TENSORFLOW:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüìà REGRESSION (TotalSpend_log)\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"MLP Regressor:  R¬≤ = {test_r2:.4f}, RMSE = {test_rmse:.4f}\")\n",
    "    print(\"(Compare with Random Forest, XGBoost from 02_regression.ipynb)\")\n",
    "    \n",
    "    print(\"\\nüéØ CLASSIFICATION (Response)\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"MLP Classifier: ROC-AUC = {roc_auc:.4f}, PR-AUC = {pr_auc:.4f}\")\n",
    "    print(\"(Compare with XGBoost, Random Forest from 03_classification.ipynb)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c9629c",
   "metadata": {},
   "source": [
    "### üî¨ Deep Learning vs Traditional ML: Key Insights\n",
    "\n",
    "**Model Comparison Summary:**\n",
    "\n",
    "| Task | MLP | Best Traditional | Winner |\n",
    "|------|-----|-----------------|--------|\n",
    "| Regression (R¬≤) | 0.9530 | RF: 0.9703 | Random Forest |\n",
    "| Classification (ROC-AUC) | 0.8706 | RF: 0.8751 | Random Forest (marginal) |\n",
    "\n",
    "**Why Traditional ML Wins (Slightly) Here:**\n",
    "\n",
    "1. **Dataset Size**: ~2,200 samples is relatively small for deep learning\n",
    "   - Neural networks typically need >10K samples to show advantage\n",
    "   - Tree ensembles are highly efficient with smaller datasets\n",
    "\n",
    "2. **Tabular Data**: Tree-based models excel at tabular data\n",
    "   - Natural handling of feature interactions\n",
    "   - No need for feature scaling or normalization\n",
    "   - Robust to outliers without preprocessing\n",
    "\n",
    "3. **Interpretability vs Flexibility Trade-off**:\n",
    "   - Random Forest: Feature importances readily available\n",
    "   - MLP: More flexible but harder to interpret\n",
    "\n",
    "**When to Choose Deep Learning for Tabular Data:**\n",
    "- Large datasets (>100K samples)\n",
    "- Complex non-linear relationships\n",
    "- When combined with embeddings (e.g., categorical embeddings)\n",
    "- As part of an ensemble with tree models\n",
    "\n",
    "**Practical Recommendation:**\n",
    "For this marketing dataset, **Random Forest is the recommended choice** due to:\n",
    "- Slightly better performance\n",
    "- Faster training and inference\n",
    "- Better interpretability for business stakeholders\n",
    "- No hyperparameter sensitivity to architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516377c",
   "metadata": {},
   "source": [
    "## 6. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf77115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save deep learning models\n",
    "if HAS_TENSORFLOW:\n",
    "    import joblib\n",
    "    \n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Save Keras models\n",
    "    model_reg.save('models/mlp_regressor.keras')\n",
    "    print(\"‚úì MLP Regressor saved\")\n",
    "    \n",
    "    model_clf.save('models/mlp_classifier.keras')\n",
    "    print(\"‚úì MLP Classifier saved\")\n",
    "    \n",
    "    # Save preprocessors\n",
    "    joblib.dump(preprocessor, 'models/dl_preprocessor_reg.joblib')\n",
    "    joblib.dump(preprocessor_clf, 'models/dl_preprocessor_clf.joblib')\n",
    "    print(\"‚úì Preprocessors saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705481fe",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TENSORFLOW:\n",
    "    print(f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                  DEEP LEARNING ANALYSIS SUMMARY                   ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üß† MODELS TRAINED\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "1. MLP Regressor (TotalSpend prediction)\n",
    "   - Architecture: 128 ‚Üí 64 ‚Üí 32 ‚Üí 1\n",
    "   - Test R¬≤: {test_r2:.4f}\n",
    "   - Test RMSE: {test_rmse:.4f}\n",
    "\n",
    "2. MLP Classifier (Response prediction)\n",
    "   - Architecture: 128 ‚Üí 64 ‚Üí 32 ‚Üí 1 (sigmoid)\n",
    "   - Test ROC-AUC: {roc_auc:.4f}\n",
    "   - Test PR-AUC: {pr_auc:.4f}\n",
    "\n",
    "‚öôÔ∏è KEY TECHNIQUES\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ Dropout regularization (30-40%)\n",
    "‚Ä¢ Batch normalization\n",
    "‚Ä¢ Early stopping\n",
    "‚Ä¢ Learning rate reduction on plateau\n",
    "‚Ä¢ Class weights for imbalanced data\n",
    "\n",
    "üí° FINDINGS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚Ä¢ Deep learning competitive with tree-based models\n",
    "‚Ä¢ Benefits from larger datasets\n",
    "‚Ä¢ More sensitive to hyperparameters\n",
    "‚Ä¢ Longer training time\n",
    "\n",
    "üìù RECOMMENDATIONS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "For this dataset size (~2,200 samples):\n",
    "‚Ä¢ Tree-based models (XGBoost, RF) often perform equally well\n",
    "‚Ä¢ Consider DL for larger datasets (>100K samples)\n",
    "‚Ä¢ Use ensemble of ML + DL for best results\n",
    "\n",
    "üèÅ PROJECT COMPLETE!\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "All analysis notebooks completed:\n",
    "  ‚úì 01_eda.ipynb ‚Äî Exploratory Data Analysis\n",
    "  ‚úì 02_regression.ipynb ‚Äî Spending Prediction\n",
    "  ‚úì 03_classification.ipynb ‚Äî Response Prediction\n",
    "  ‚úì 04_clustering.ipynb ‚Äî Customer Segmentation\n",
    "  ‚úì 05_deep_learning.ipynb ‚Äî Neural Networks\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è TensorFlow not available. Install to run deep learning experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4096399",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Deep Learning Analysis Conclusions\n",
    "\n",
    "### Technical Achievements\n",
    "\n",
    "**1. MLP Regressor (TotalSpend Prediction)**\n",
    "- **Architecture**: 3-layer MLP (128 ‚Üí 64 ‚Üí 32 ‚Üí 1) with BatchNorm and Dropout\n",
    "- **Performance**: R¬≤ = 0.9530, RMSE = 0.3190\n",
    "- **Training**: 70 epochs with early stopping, no overfitting observed\n",
    "\n",
    "**2. MLP Classifier (Campaign Response)**\n",
    "- **Architecture**: Same structure with sigmoid output\n",
    "- **Performance**: ROC-AUC = 0.8706, Recall = 82%\n",
    "- **Class Balance**: Weighted loss (3.35x for minority class)\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "1. **Deep learning is competitive** but doesn't outperform tree ensembles on this small tabular dataset\n",
    "2. **Regularization techniques** (Dropout, BatchNorm, Early Stopping) are essential for preventing overfitting\n",
    "3. **Class weights** effectively handle imbalanced classification problems\n",
    "4. **Architecture design** matters less than data quality and regularization for tabular data\n",
    "\n",
    "### Recommendations for Production\n",
    "\n",
    "| Use Case | Recommended Model | Reason |\n",
    "|----------|-------------------|--------|\n",
    "| Spending Prediction | Random Forest | Higher R¬≤, faster inference |\n",
    "| Campaign Targeting | Random Forest or MLP | Similar performance, RF more interpretable |\n",
    "| Large-Scale (>100K) | Consider MLP | Neural networks scale better |\n",
    "| Ensemble | RF + MLP | Combine strengths of both approaches |\n",
    "\n",
    "### Files Generated\n",
    "- `models/mlp_regressor.keras` ‚Äî Trained regression model\n",
    "- `models/mlp_classifier.keras` ‚Äî Trained classification model  \n",
    "- `models/dl_preprocessor_reg.joblib` ‚Äî Regression preprocessor\n",
    "- `models/dl_preprocessor_clf.joblib` ‚Äî Classification preprocessor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
