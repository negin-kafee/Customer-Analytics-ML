{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16907737",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Customer Segmentation Analysis\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook performs **unsupervised learning** to identify distinct customer segments for targeted marketing. We compare multiple clustering algorithms and profile the resulting segments.\n",
    "\n",
    "### Business Objective\n",
    "- **Goal**: Segment customers into actionable groups based on purchasing behavior\n",
    "- **Value**: Enable personalized marketing strategies for each segment\n",
    "- **Success Criteria**: Clear, interpretable segments with distinct characteristics\n",
    "\n",
    "### Methodology\n",
    "1. **Feature Selection**: Focus on behavioral and value metrics\n",
    "2. **Algorithm Comparison**: K-Means, Hierarchical, and Gaussian Mixture Models\n",
    "3. **Optimal K Selection**: Elbow method, silhouette analysis, CH/DB indices\n",
    "4. **Segment Profiling**: Characterize each cluster for business interpretation\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9801787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Sklearn clustering\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, silhouette_samples,\n",
    "    calinski_harabasz_score, davies_bouldin_score\n",
    ")\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# Our custom modules\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.config import (\n",
    "    DATA_PATH, RANDOM_STATE,\n",
    "    SPENDING_COLS, PURCHASE_COLS,\n",
    "    MAIN_COLOR, SECONDARY_COLOR, ACCENT_COLOR\n",
    ")\n",
    "from src.data_loader import load_data\n",
    "from src.preprocessing import FeatureEngineer\n",
    "from src.models import (\n",
    "    get_kmeans, get_gmm, get_hierarchical,\n",
    "    find_optimal_k, compute_cluster_profiles, compute_spending_mix\n",
    ")\n",
    "from src.visualization import (\n",
    "    set_style, plot_elbow_silhouette, plot_cluster_scatter_pca,\n",
    "    plot_cluster_profiles, plot_dendrogram\n",
    ")\n",
    "from src.evaluation import ModelLogger\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "set_style()\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea98966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = load_data(DATA_PATH)\n",
    "\n",
    "# Feature engineering\n",
    "engineer = FeatureEngineer()\n",
    "df = engineer.fit_transform(df)\n",
    "\n",
    "# Filter unrealistic ages\n",
    "df = df[df['Age'] <= 100]\n",
    "\n",
    "# Handle missing Income\n",
    "df['Income'] = df['Income'].fillna(df['Income'].median())\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c11b0b",
   "metadata": {},
   "source": [
    "## 2. Feature Selection for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for clustering\n",
    "# Focus on behavioral and value-based features\n",
    "\n",
    "clustering_features = [\n",
    "    # Value metrics\n",
    "    'Income',\n",
    "    'TotalSpend',\n",
    "    \n",
    "    # Spending by category\n",
    "    'MntWines',\n",
    "    'MntFruits',\n",
    "    'MntMeatProducts',\n",
    "    'MntFishProducts',\n",
    "    'MntSweetProducts',\n",
    "    'MntGoldProds',\n",
    "    \n",
    "    # Purchase behavior\n",
    "    'NumWebPurchases',\n",
    "    'NumCatalogPurchases',\n",
    "    'NumStorePurchases',\n",
    "    'NumDealsPurchases',\n",
    "    \n",
    "    # Engagement\n",
    "    'NumWebVisitsMonth',\n",
    "    'Recency',\n",
    "    \n",
    "    # Demographics\n",
    "    'Age',\n",
    "    'Kidhome',\n",
    "    'Teenhome'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "clustering_features = [c for c in clustering_features if c in df.columns]\n",
    "\n",
    "print(f\"Clustering features ({len(clustering_features)}):\")\n",
    "print(clustering_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "X = df[clustering_features].copy()\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Scale features (critical for clustering!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nScaled data shape: {X_scaled.shape}\")\n",
    "print(f\"Mean (should be ~0): {X_scaled.mean():.6f}\")\n",
    "print(f\"Std (should be ~1): {X_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bbc1b1",
   "metadata": {},
   "source": [
    "## 3. Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee57d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k using elbow and silhouette methods\n",
    "k_range = range(2, 11)\n",
    "\n",
    "results = find_optimal_k(X_scaled, k_range=k_range, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Cluster Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "for k, inertia, sil in zip(results['k_range'], results['inertias'], results['silhouettes']):\n",
    "    print(f\"k={k}: Inertia={inertia:.0f}, Silhouette={sil:.4f}\")\n",
    "\n",
    "print(f\"\\nBest k by Silhouette: {results['best_k_silhouette']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26803a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize elbow and silhouette\n",
    "plot_elbow_silhouette(\n",
    "    k_range,\n",
    "    results['inertias'],\n",
    "    results['silhouettes'],\n",
    "    best_k=results['best_k_silhouette']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional metrics: Calinski-Harabasz and Davies-Bouldin\n",
    "ch_scores = []\n",
    "db_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    ch_scores.append(calinski_harabasz_score(X_scaled, labels))\n",
    "    db_scores.append(davies_bouldin_score(X_scaled, labels))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(list(k_range), ch_scores, 'o-', color=MAIN_COLOR)\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Calinski-Harabasz Index')\n",
    "axes[0].set_title('Calinski-Harabasz Index (Higher is Better)')\n",
    "axes[0].set_xticks(list(k_range))\n",
    "\n",
    "axes[1].plot(list(k_range), db_scores, 'o-', color=MAIN_COLOR)\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Davies-Bouldin Index')\n",
    "axes[1].set_title('Davies-Bouldin Index (Lower is Better)')\n",
    "axes[1].set_xticks(list(k_range))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best k by each metric\n",
    "best_k_ch = list(k_range)[np.argmax(ch_scores)]\n",
    "best_k_db = list(k_range)[np.argmin(db_scores)]\n",
    "\n",
    "print(f\"Best k by Calinski-Harabasz: {best_k_ch}\")\n",
    "print(f\"Best k by Davies-Bouldin: {best_k_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6cf558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose optimal k (consensus or business-driven)\n",
    "optimal_k = 4  # Common choice for customer segmentation\n",
    "\n",
    "print(f\"\\nâœ“ Selected k = {optimal_k} for customer segmentation\")\n",
    "print(\"  Rationale: Good silhouette score and interpretable number of segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600cbf6",
   "metadata": {},
   "source": [
    "### Optimal K Analysis\n",
    "\n",
    "**Elbow Method Observations:**\n",
    "- Clear elbow at k=3-4, with diminishing returns beyond\n",
    "- Inertia decreases steadily but rate slows after k=4\n",
    "\n",
    "**Silhouette Score Analysis:**\n",
    "- **k=2**: Highest silhouette (0.323) but too coarse for marketing\n",
    "- **k=4**: Good balance (0.165) with actionable segments\n",
    "- Silhouette scores decline with more clusters, indicating overlapping boundaries\n",
    "\n",
    "**Decision**: **k=4 clusters** provides the best balance between:\n",
    "- Statistical validity (reasonable silhouette)\n",
    "- Business interpretability (actionable segment count)\n",
    "- Marketing practicality (manageable personalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d4d99",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d863dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means\n",
    "kmeans = get_kmeans(n_clusters=optimal_k)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "sil_kmeans = silhouette_score(X_scaled, kmeans_labels)\n",
    "ch_kmeans = calinski_harabasz_score(X_scaled, kmeans_labels)\n",
    "db_kmeans = davies_bouldin_score(X_scaled, kmeans_labels)\n",
    "\n",
    "print(f\"K-Means (k={optimal_k}) Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Silhouette Score:        {sil_kmeans:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_kmeans:.2f}\")\n",
    "print(f\"Davies-Bouldin Index:    {db_kmeans:.4f}\")\n",
    "\n",
    "# Cluster sizes\n",
    "print(f\"\\nCluster Sizes:\")\n",
    "for i in range(optimal_k):\n",
    "    count = (kmeans_labels == i).sum()\n",
    "    pct = 100 * count / len(kmeans_labels)\n",
    "    print(f\"  Cluster {i}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in PCA space\n",
    "plot_cluster_scatter_pca(X_scaled, kmeans_labels, title=\"K-Means Clusters (PCA 2D Projection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette analysis per cluster\n",
    "silhouette_vals = silhouette_samples(X_scaled, kmeans_labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "y_lower = 10\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, optimal_k))\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    cluster_silhouette_vals = silhouette_vals[kmeans_labels == i]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    \n",
    "    y_upper = y_lower + len(cluster_silhouette_vals)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                     0, cluster_silhouette_vals,\n",
    "                     alpha=0.7, color=colors[i],\n",
    "                     label=f'Cluster {i} (mean={cluster_silhouette_vals.mean():.3f})')\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.axvline(sil_kmeans, color='red', linestyle='--', label=f'Overall Mean ({sil_kmeans:.3f})')\n",
    "ax.set_xlabel('Silhouette Coefficient')\n",
    "ax.set_ylabel('Cluster')\n",
    "ax.set_title('Silhouette Plot for K-Means Clustering')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4c51b",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linkage matrix\n",
    "print(\"Computing hierarchical clustering...\")\n",
    "linkage_matrix = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(14, 6))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5, leaf_rotation=90, leaf_font_size=10)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward Linkage)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92208714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Agglomerative Clustering\n",
    "hierarchical = get_hierarchical(n_clusters=optimal_k, linkage='ward')\n",
    "hier_labels = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluation\n",
    "sil_hier = silhouette_score(X_scaled, hier_labels)\n",
    "ch_hier = calinski_harabasz_score(X_scaled, hier_labels)\n",
    "db_hier = davies_bouldin_score(X_scaled, hier_labels)\n",
    "\n",
    "print(f\"Hierarchical Clustering (k={optimal_k}) Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Silhouette Score:        {sil_hier:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_hier:.2f}\")\n",
    "print(f\"Davies-Bouldin Index:    {db_hier:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ee4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hierarchical clusters\n",
    "plot_cluster_scatter_pca(X_scaled, hier_labels, title=\"Hierarchical Clusters (PCA 2D Projection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c3520",
   "metadata": {},
   "source": [
    "## 6. Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ecf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GMM\n",
    "gmm = get_gmm(n_components=optimal_k)\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluation\n",
    "sil_gmm = silhouette_score(X_scaled, gmm_labels)\n",
    "ch_gmm = calinski_harabasz_score(X_scaled, gmm_labels)\n",
    "db_gmm = davies_bouldin_score(X_scaled, gmm_labels)\n",
    "\n",
    "print(f\"Gaussian Mixture Model (k={optimal_k}) Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Silhouette Score:        {sil_gmm:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_gmm:.2f}\")\n",
    "print(f\"Davies-Bouldin Index:    {db_gmm:.4f}\")\n",
    "print(f\"\\nGMM Log-Likelihood:      {gmm.score(X_scaled):.2f}\")\n",
    "print(f\"BIC:                     {gmm.bic(X_scaled):.2f}\")\n",
    "print(f\"AIC:                     {gmm.aic(X_scaled):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b32d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GMM clusters\n",
    "plot_cluster_scatter_pca(X_scaled, gmm_labels, title=\"GMM Clusters (PCA 2D Projection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd95329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIC/AIC for model selection\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    gmm_temp = GaussianMixture(n_components=k, random_state=RANDOM_STATE, n_init=5)\n",
    "    gmm_temp.fit(X_scaled)\n",
    "    bic_scores.append(gmm_temp.bic(X_scaled))\n",
    "    aic_scores.append(gmm_temp.aic(X_scaled))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(list(k_range), bic_scores, 'o-', label='BIC', color=MAIN_COLOR)\n",
    "ax.plot(list(k_range), aic_scores, 'o-', label='AIC', color=SECONDARY_COLOR)\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('GMM Model Selection (Lower is Better)')\n",
    "ax.legend()\n",
    "ax.set_xticks(list(k_range))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k by BIC: {list(k_range)[np.argmin(bic_scores)]}\")\n",
    "print(f\"Best k by AIC: {list(k_range)[np.argmin(aic_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88836f66",
   "metadata": {},
   "source": [
    "## 7. Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['K-Means', 'Hierarchical', 'GMM'],\n",
    "    'Silhouette': [sil_kmeans, sil_hier, sil_gmm],\n",
    "    'Calinski-Harabasz': [ch_kmeans, ch_hier, ch_gmm],\n",
    "    'Davies-Bouldin': [db_kmeans, db_hier, db_gmm]\n",
    "})\n",
    "\n",
    "print(\"\\nClustering Method Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "comparison.plot(x='Method', y='Silhouette', kind='bar', ax=axes[0], color=MAIN_COLOR, legend=False)\n",
    "axes[0].set_title('Silhouette Score (Higher is Better)')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "comparison.plot(x='Method', y='Calinski-Harabasz', kind='bar', ax=axes[1], color=MAIN_COLOR, legend=False)\n",
    "axes[1].set_title('Calinski-Harabasz (Higher is Better)')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "comparison.plot(x='Method', y='Davies-Bouldin', kind='bar', ax=axes[2], color=MAIN_COLOR, legend=False)\n",
    "axes[2].set_title('Davies-Bouldin (Lower is Better)')\n",
    "axes[2].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best method\n",
    "best_method = 'K-Means'  # Typically most interpretable and efficient\n",
    "final_labels = kmeans_labels\n",
    "\n",
    "print(f\"\\nðŸ† Selected Method: {best_method}\")\n",
    "print(\"   Rationale: Good scores, interpretable, computationally efficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496732e",
   "metadata": {},
   "source": [
    "### Algorithm Comparison Summary\n",
    "\n",
    "| Method | Silhouette | Calinski-Harabasz | Davies-Bouldin | Verdict |\n",
    "|--------|------------|-------------------|----------------|---------|\n",
    "| **K-Means** | 0.165 | 608 | 1.82 | **Best overall** |\n",
    "| Hierarchical | 0.146 | 556 | 1.86 | Close second |\n",
    "| GMM | 0.105 | 402 | 2.89 | Poorest fit |\n",
    "\n",
    "**Why K-Means Wins:**\n",
    "1. **Highest Silhouette Score** (0.165) â€” best cluster cohesion\n",
    "2. **Highest CH Index** (608) â€” best-defined clusters\n",
    "3. **Lowest DB Index** (1.82) â€” least overlap between clusters\n",
    "4. **Computational efficiency** â€” fastest to train and predict\n",
    "5. **Interpretability** â€” centroid-based clusters are intuitive\n",
    "\n",
    "**Note on GMM**: While GMM offers probabilistic cluster membership (soft assignment), it produces more overlapping clusters in this dataset. For hard marketing segmentation, K-Means is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98912de8",
   "metadata": {},
   "source": [
    "## 8. Cluster Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e055b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to data\n",
    "df['Cluster'] = final_labels\n",
    "\n",
    "# Compute cluster profiles\n",
    "profile_df = compute_cluster_profiles(df, final_labels, clustering_features)\n",
    "\n",
    "print(\"Cluster Profiles (Mean Values):\")\n",
    "print(\"=\" * 80)\n",
    "profile_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb541ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster profiles (heatmap)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Normalize for better visualization\n",
    "profile_normalized = (profile_df - profile_df.mean()) / profile_df.std()\n",
    "\n",
    "sns.heatmap(profile_normalized, annot=True, cmap='RdBu_r', center=0, fmt='.2f')\n",
    "plt.title('Cluster Profiles (Standardized Mean Values)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afdf947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key metrics by cluster\n",
    "key_metrics = ['Income', 'TotalSpend', 'Age', 'Recency', 'NumWebPurchases', 'NumStorePurchases']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(key_metrics):\n",
    "    if metric in df.columns:\n",
    "        sns.boxplot(data=df, x='Cluster', y=metric, ax=axes[i], palette='Set2')\n",
    "        axes[i].set_title(f'{metric} by Cluster')\n",
    "\n",
    "plt.suptitle('Key Metrics Distribution by Cluster', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcf3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spending mix by cluster\n",
    "if all(c in df.columns for c in SPENDING_COLS):\n",
    "    spending_mix = compute_spending_mix(df, final_labels, SPENDING_COLS)\n",
    "    \n",
    "    print(\"\\nSpending Mix (Share of Wallet) by Cluster:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(spending_mix.round(3))\n",
    "    \n",
    "    # Visualize\n",
    "    spending_mix.T.plot(kind='bar', figsize=(12, 5), colormap='Set2')\n",
    "    plt.title('Spending Mix by Cluster')\n",
    "    plt.xlabel('Product Category')\n",
    "    plt.ylabel('Share of Total Spend')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59403964",
   "metadata": {},
   "source": [
    "## 9. Segment Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment summary statistics\n",
    "segment_summary = df.groupby('Cluster').agg({\n",
    "    'Income': 'mean',\n",
    "    'TotalSpend': 'mean',\n",
    "    'Age': 'mean',\n",
    "    'Recency': 'mean',\n",
    "    'NumWebPurchases': 'mean',\n",
    "    'NumStorePurchases': 'mean',\n",
    "    'NumCatalogPurchases': 'mean',\n",
    "    'Kidhome': 'mean',\n",
    "    'Response': 'mean'  # Campaign response rate\n",
    "}).round(2)\n",
    "\n",
    "# Add count\n",
    "segment_summary['Count'] = df.groupby('Cluster').size()\n",
    "segment_summary['% of Total'] = (segment_summary['Count'] / len(df) * 100).round(1)\n",
    "\n",
    "print(\"\\nSegment Summary:\")\n",
    "print(\"=\" * 80)\n",
    "segment_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name segments based on profiles\n",
    "# This requires manual interpretation based on the profiles above\n",
    "\n",
    "segment_names = {\n",
    "    0: \"Budget Shoppers\",\n",
    "    1: \"Premium Customers\", \n",
    "    2: \"Family Focused\",\n",
    "    3: \"High-Value Loyalists\"\n",
    "}\n",
    "\n",
    "# Create descriptions\n",
    "segment_descriptions = {\n",
    "    0: \"Lower income, price-sensitive, deal seekers\",\n",
    "    1: \"High income and spending, prefer premium products\",\n",
    "    2: \"Moderate income, families with children, value-conscious\",\n",
    "    3: \"Highest spenders, loyal, multi-channel shoppers\"\n",
    "}\n",
    "\n",
    "print(\"\\nSegment Interpretations:\")\n",
    "print(\"=\" * 70)\n",
    "for cluster_id in range(optimal_k):\n",
    "    name = segment_names.get(cluster_id, f\"Segment {cluster_id}\")\n",
    "    desc = segment_descriptions.get(cluster_id, \"No description\")\n",
    "    count = (final_labels == cluster_id).sum()\n",
    "    pct = 100 * count / len(final_labels)\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ Cluster {cluster_id}: {name}\")\n",
    "    print(f\"   Size: {count:,} customers ({pct:.1f}%)\")\n",
    "    print(f\"   Profile: {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360866c",
   "metadata": {},
   "source": [
    "### Detailed Segment Profiles\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”µ Cluster 0: Budget-Conscious Young Families (25.2%)\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Income | $29,748 | **Lowest** â€” budget constraints |\n",
    "| TotalSpend | $102 | **Lowest** â€” minimal engagement |\n",
    "| Age | 35.5 years | **Youngest** segment |\n",
    "| Kidhome | 0.86 | **Highest** â€” young children |\n",
    "| Response Rate | 13% | Average responsiveness |\n",
    "\n",
    "**Profile**: Young parents with limited disposable income. Focus on value and deals.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸŸ£ Cluster 1: Middle-Income Empty Nesters (25.0%)\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Income | $42,537 | Moderate |\n",
    "| TotalSpend | $128 | Low engagement |\n",
    "| Age | 50.5 years | **Oldest** segment |\n",
    "| Teenhome | 0.96 | **Highest** â€” teenagers at home |\n",
    "| Response Rate | 6% | **Lowest** â€” least responsive |\n",
    "\n",
    "**Profile**: Older customers with teens, moderate income but low spending. Hard to engage.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸŸ¡ Cluster 2: Affluent Premium Shoppers (25.6%)\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Income | $76,837 | **Highest** â€” affluent |\n",
    "| TotalSpend | $1,391 | **Highest** â€” 10Ã— Cluster 0 |\n",
    "| Catalog Purchases | 6.0 | **Highest** â€” premium channel |\n",
    "| Kidhome | 0.04 | **Lowest** â€” no young kids |\n",
    "| Response Rate | 28% | **Highest** â€” most responsive |\n",
    "\n",
    "**Profile**: High-income, high-spending customers without young children. **VIP segment** for premium products and campaigns.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸŸ¢ Cluster 3: Digital-Savvy Deal Hunters (24.2%)\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Income | $59,610 | Above average |\n",
    "| TotalSpend | $792 | Good value |\n",
    "| Web Purchases | 6.6 | **Highest** â€” digital natives |\n",
    "| Deal Purchases | 3.65 | **Highest** â€” deal seekers |\n",
    "| Recency | 47.2 | **Most recent** buyers |\n",
    "\n",
    "**Profile**: Tech-savvy customers who shop online and seek deals. Active and engaged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee220b5",
   "metadata": {},
   "source": [
    "## 10. Marketing Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                   MARKETING RECOMMENDATIONS                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Based on the customer segmentation analysis:\n",
    "\n",
    "ðŸ”¹ BUDGET SHOPPERS (Cluster 0)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ Strategy: Promote deals, discounts, and value bundles\n",
    "â€¢ Channels: Focus on web promotions and email marketing\n",
    "â€¢ Products: Highlight affordable options and bulk deals\n",
    "â€¢ Timing: Target during sales events and end-of-season\n",
    "\n",
    "ðŸ”¹ PREMIUM CUSTOMERS (Cluster 1)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ Strategy: Emphasize quality, exclusivity, and new arrivals\n",
    "â€¢ Channels: Catalog and store experiences\n",
    "â€¢ Products: Premium wines, gourmet foods, specialty items\n",
    "â€¢ Timing: New product launches, exclusive previews\n",
    "\n",
    "ðŸ”¹ FAMILY FOCUSED (Cluster 2)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ Strategy: Family bundles, kid-friendly products, convenience\n",
    "â€¢ Channels: Web (convenience) and store (family experience)\n",
    "â€¢ Products: Fruits, sweets, meal solutions\n",
    "â€¢ Timing: Back-to-school, holidays, weekends\n",
    "\n",
    "ðŸ”¹ HIGH-VALUE LOYALISTS (Cluster 3)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ Strategy: Loyalty programs, VIP treatment, personalization\n",
    "â€¢ Channels: Multi-channel (they use all channels)\n",
    "â€¢ Products: Full product range, personalized recommendations\n",
    "â€¢ Timing: Any time - they're always engaged\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffa21b",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster assignments\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save clustering model\n",
    "joblib.dump(kmeans, 'models/kmeans_model.joblib')\n",
    "print(\"âœ“ KMeans model saved\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'models/clustering_scaler.joblib')\n",
    "print(\"âœ“ Scaler saved\")\n",
    "\n",
    "# Save cluster metadata\n",
    "cluster_metadata = {\n",
    "    'n_clusters': optimal_k,\n",
    "    'features': clustering_features,\n",
    "    'segment_names': segment_names,\n",
    "    'silhouette_score': sil_kmeans,\n",
    "    'method': 'KMeans'\n",
    "}\n",
    "joblib.dump(cluster_metadata, 'models/clustering_metadata.joblib')\n",
    "print(\"âœ“ Metadata saved\")\n",
    "\n",
    "# Save clustered data\n",
    "df['Segment_Name'] = df['Cluster'].map(segment_names)\n",
    "df.to_csv('Data/clustered_customers.csv', index=False)\n",
    "print(\"âœ“ Clustered customer data saved to Data/clustered_customers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56b146",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fa0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                   CLUSTERING ANALYSIS SUMMARY                     â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ðŸŽ¯ OBJECTIVE\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Segment customers into distinct groups for targeted marketing\n",
    "\n",
    "ðŸ“Š DATA USED\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ Customers: {len(df):,}\n",
    "â€¢ Features: {len(clustering_features)} (spending, demographics, behavior)\n",
    "â€¢ Preprocessing: StandardScaler\n",
    "\n",
    "ðŸ† BEST METHOD: K-Means\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â€¢ Optimal clusters: {optimal_k}\n",
    "â€¢ Silhouette Score: {sil_kmeans:.4f}\n",
    "â€¢ Calinski-Harabasz: {ch_kmeans:.2f}\n",
    "\n",
    "ðŸ‘¥ CUSTOMER SEGMENTS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\")\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    name = segment_names.get(i, f\"Segment {i}\")\n",
    "    count = (final_labels == i).sum()\n",
    "    pct = 100 * count / len(final_labels)\n",
    "    print(f\"â€¢ Cluster {i} ({name}): {count:,} customers ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "ðŸš€ NEXT STEPS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â†’ Proceed to 05_deep_learning.ipynb for neural network models\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c59949",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ Advanced Clustering Optimization\n",
    "\n",
    "The current silhouette score (~0.165) indicates clusters have some overlap. Let's try several techniques to improve separation:\n",
    "\n",
    "1. **Dimensionality Reduction with PCA** - Remove correlated features\n",
    "2. **Feature Engineering** - Create more discriminative features  \n",
    "3. **RFM Segmentation** - Use proven marketing methodology\n",
    "4. **DBSCAN** - Density-based clustering for non-spherical clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1049702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIMIZATION 1: PCA + K-Means\n",
    "# =============================================================================\n",
    "# Reduce dimensionality to remove correlated features\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA keeping 95% variance\n",
    "pca = PCA(n_components=0.95, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original features: {X_scaled.shape[1]}\")\n",
    "print(f\"PCA components: {X_pca.shape[1]}\")\n",
    "print(f\"Variance explained: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "# K-Means on PCA-reduced data\n",
    "kmeans_pca = KMeans(n_clusters=4, random_state=RANDOM_STATE, n_init=20)\n",
    "labels_pca = kmeans_pca.fit_predict(X_pca)\n",
    "\n",
    "sil_pca = silhouette_score(X_pca, labels_pca)\n",
    "ch_pca = calinski_harabasz_score(X_pca, labels_pca)\n",
    "db_pca = davies_bouldin_score(X_pca, labels_pca)\n",
    "\n",
    "print(f\"\\nðŸ“Š PCA + K-Means Results:\")\n",
    "print(f\"   Silhouette Score: {sil_pca:.4f}\")\n",
    "print(f\"   Calinski-Harabasz: {ch_pca:.1f}\")\n",
    "print(f\"   Davies-Bouldin: {db_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIMIZATION 2: RFM Segmentation (Proven Marketing Method)\n",
    "# =============================================================================\n",
    "# RFM = Recency, Frequency, Monetary - classic customer segmentation\n",
    "\n",
    "# Create RFM features\n",
    "rfm_df = pd.DataFrame({\n",
    "    'Recency': df['Recency'],  # Lower is better (recent purchaser)\n",
    "    'Frequency': df['NumWebPurchases'] + df['NumCatalogPurchases'] + df['NumStorePurchases'],\n",
    "    'Monetary': df['TotalSpend']\n",
    "})\n",
    "\n",
    "# Scale RFM\n",
    "scaler_rfm = StandardScaler()\n",
    "rfm_scaled = scaler_rfm.fit_transform(rfm_df)\n",
    "\n",
    "# K-Means on RFM (typically works well with k=4-5)\n",
    "kmeans_rfm = KMeans(n_clusters=4, random_state=RANDOM_STATE, n_init=20)\n",
    "labels_rfm = kmeans_rfm.fit_predict(rfm_scaled)\n",
    "\n",
    "sil_rfm = silhouette_score(rfm_scaled, labels_rfm)\n",
    "ch_rfm = calinski_harabasz_score(rfm_scaled, labels_rfm)\n",
    "db_rfm = davies_bouldin_score(rfm_scaled, labels_rfm)\n",
    "\n",
    "print(f\"ðŸ“Š RFM K-Means Results:\")\n",
    "print(f\"   Silhouette Score: {sil_rfm:.4f}\")\n",
    "print(f\"   Calinski-Harabasz: {ch_rfm:.1f}\")\n",
    "print(f\"   Davies-Bouldin: {db_rfm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIMIZATION 3: Log-Transform + PCA + K-Means\n",
    "# =============================================================================\n",
    "# Spending variables are often right-skewed, log transform helps\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Log-transform spending features (add 1 to handle zeros)\n",
    "X_log = X.copy()\n",
    "spending_features = ['Income', 'TotalSpend', 'MntWines', 'MntFruits', \n",
    "                     'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n",
    "\n",
    "for col in spending_features:\n",
    "    if col in X_log.columns:\n",
    "        X_log[col] = np.log1p(X_log[col])\n",
    "\n",
    "# Scale after log transform\n",
    "X_log_scaled = StandardScaler().fit_transform(X_log)\n",
    "\n",
    "# PCA\n",
    "pca_log = PCA(n_components=0.95, random_state=RANDOM_STATE)\n",
    "X_log_pca = pca_log.fit_transform(X_log_scaled)\n",
    "\n",
    "# K-Means\n",
    "kmeans_log = KMeans(n_clusters=4, random_state=RANDOM_STATE, n_init=20)\n",
    "labels_log = kmeans_log.fit_predict(X_log_pca)\n",
    "\n",
    "sil_log = silhouette_score(X_log_pca, labels_log)\n",
    "ch_log = calinski_harabasz_score(X_log_pca, labels_log)\n",
    "db_log = davies_bouldin_score(X_log_pca, labels_log)\n",
    "\n",
    "print(f\"ðŸ“Š Log-Transform + PCA + K-Means Results:\")\n",
    "print(f\"   Silhouette Score: {sil_log:.4f}\")\n",
    "print(f\"   Calinski-Harabasz: {ch_log:.1f}\")\n",
    "print(f\"   Davies-Bouldin: {db_log:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIMIZATION 4: 2-Feature Clustering (Income + Spending)\n",
    "# =============================================================================\n",
    "# Sometimes simpler is better - use the most important features only\n",
    "\n",
    "X_simple = df[['Income', 'TotalSpend']].copy()\n",
    "X_simple['Income'] = np.log1p(X_simple['Income'])\n",
    "X_simple['TotalSpend'] = np.log1p(X_simple['TotalSpend'])\n",
    "X_simple_scaled = StandardScaler().fit_transform(X_simple)\n",
    "\n",
    "kmeans_simple = KMeans(n_clusters=4, random_state=RANDOM_STATE, n_init=20)\n",
    "labels_simple = kmeans_simple.fit_predict(X_simple_scaled)\n",
    "\n",
    "sil_simple = silhouette_score(X_simple_scaled, labels_simple)\n",
    "ch_simple = calinski_harabasz_score(X_simple_scaled, labels_simple)\n",
    "db_simple = davies_bouldin_score(X_simple_scaled, labels_simple)\n",
    "\n",
    "print(f\"ðŸ“Š Simple 2-Feature (Income + Spending) Results:\")\n",
    "print(f\"   Silhouette Score: {sil_simple:.4f}\")\n",
    "print(f\"   Calinski-Harabasz: {ch_simple:.1f}\")\n",
    "print(f\"   Davies-Bouldin: {db_simple:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIMIZATION 5: Gaussian Mixture with Different Covariance Types\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Try different covariance structures\n",
    "best_gmm_sil = 0\n",
    "best_gmm_type = None\n",
    "\n",
    "for cov_type in ['full', 'tied', 'diag', 'spherical']:\n",
    "    gmm_opt = GaussianMixture(n_components=4, covariance_type=cov_type, \n",
    "                              random_state=RANDOM_STATE, n_init=5)\n",
    "    labels_gmm_opt = gmm_opt.fit_predict(X_log_pca)\n",
    "    sil_gmm_opt = silhouette_score(X_log_pca, labels_gmm_opt)\n",
    "    print(f\"   GMM ({cov_type}): Silhouette = {sil_gmm_opt:.4f}\")\n",
    "    \n",
    "    if sil_gmm_opt > best_gmm_sil:\n",
    "        best_gmm_sil = sil_gmm_opt\n",
    "        best_gmm_type = cov_type\n",
    "        best_gmm_labels = labels_gmm_opt\n",
    "\n",
    "print(f\"\\n   Best GMM: {best_gmm_type} with Silhouette = {best_gmm_sil:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPARISON SUMMARY: All Optimization Methods\n",
    "# =============================================================================\n",
    "\n",
    "comparison_results = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Original K-Means',\n",
    "        'PCA + K-Means', \n",
    "        'RFM K-Means',\n",
    "        'Log + PCA + K-Means',\n",
    "        'Simple 2-Feature',\n",
    "        f'GMM ({best_gmm_type})'\n",
    "    ],\n",
    "    'Silhouette': [\n",
    "        sil_kmeans,  # Original\n",
    "        sil_pca,\n",
    "        sil_rfm,\n",
    "        sil_log,\n",
    "        sil_simple,\n",
    "        best_gmm_sil\n",
    "    ],\n",
    "    'CH Index': [\n",
    "        ch_kmeans,\n",
    "        ch_pca,\n",
    "        ch_rfm, \n",
    "        ch_log,\n",
    "        ch_simple,\n",
    "        calinski_harabasz_score(X_log_pca, best_gmm_labels)\n",
    "    ],\n",
    "    'DB Index': [\n",
    "        db_kmeans,\n",
    "        db_pca,\n",
    "        db_rfm,\n",
    "        db_log,\n",
    "        db_simple,\n",
    "        davies_bouldin_score(X_log_pca, best_gmm_labels)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by Silhouette Score\n",
    "comparison_results = comparison_results.sort_values('Silhouette', ascending=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ† CLUSTERING OPTIMIZATION COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_results.to_string(index=False))\n",
    "print(\"\\nðŸ“ˆ Higher Silhouette = Better | Lower DB = Better | Higher CH = Better\")\n",
    "\n",
    "# Find best method\n",
    "best_method_opt = comparison_results.iloc[0]['Method']\n",
    "best_sil_opt = comparison_results.iloc[0]['Silhouette']\n",
    "improvement = (best_sil_opt - sil_kmeans) / sil_kmeans * 100\n",
    "\n",
    "print(f\"\\nðŸŽ¯ BEST METHOD: {best_method_opt}\")\n",
    "print(f\"   Silhouette: {best_sil_opt:.4f}\")\n",
    "print(f\"   Improvement: {improvement:+.1f}% over original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd061955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION: Best Clustering Result\n",
    "# =============================================================================\n",
    "\n",
    "# Use the best performing method\n",
    "if 'Simple' in best_method_opt:\n",
    "    best_labels = labels_simple\n",
    "    plot_data = X_simple_scaled\n",
    "elif 'RFM' in best_method_opt:\n",
    "    best_labels = labels_rfm\n",
    "    plot_data = rfm_scaled\n",
    "elif 'Log' in best_method_opt:\n",
    "    best_labels = labels_log\n",
    "    plot_data = X_log_pca\n",
    "elif 'GMM' in best_method_opt:\n",
    "    best_labels = best_gmm_labels\n",
    "    plot_data = X_log_pca\n",
    "else:\n",
    "    best_labels = labels_pca\n",
    "    plot_data = X_pca\n",
    "\n",
    "# PCA for visualization (2D)\n",
    "if plot_data.shape[1] > 2:\n",
    "    pca_vis = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "    plot_data_2d = pca_vis.fit_transform(plot_data)\n",
    "else:\n",
    "    plot_data_2d = plot_data\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "scatter = axes[0].scatter(plot_data_2d[:, 0], plot_data_2d[:, 1], \n",
    "                          c=best_labels, cmap='viridis', alpha=0.6, s=30)\n",
    "axes[0].set_xlabel('Component 1')\n",
    "axes[0].set_ylabel('Component 2')\n",
    "axes[0].set_title(f'Best Clustering: {best_method_opt}\\nSilhouette = {best_sil_opt:.3f}')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Cluster sizes\n",
    "unique, counts = np.unique(best_labels, return_counts=True)\n",
    "axes[1].bar(unique, counts, color=plt.cm.viridis(unique/max(unique)))\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Cluster Distribution')\n",
    "for i, (u, c) in enumerate(zip(unique, counts)):\n",
    "    axes[1].text(u, c + 20, f'{c}\\n({c/len(best_labels)*100:.1f}%)', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a505e",
   "metadata": {},
   "source": [
    "### ðŸ“Š Clustering Optimization Summary\n",
    "\n",
    "**Why is Silhouette Score ~0.17 in Marketing Data?**\n",
    "\n",
    "Marketing customer data often has **overlapping segments** by nature:\n",
    "1. Customers don't fall into perfectly distinct groups\n",
    "2. There's a continuum from low to high spenders\n",
    "3. Multiple behaviors overlap (e.g., high income + low spending)\n",
    "\n",
    "**What's Considered \"Good\"?**\n",
    "\n",
    "| Silhouette | Interpretation |\n",
    "|------------|----------------|\n",
    "| 0.71-1.00 | Strong structure (rare in real data) |\n",
    "| 0.51-0.70 | Reasonable structure |\n",
    "| 0.26-0.50 | Weak structure, but useful |\n",
    "| < 0.25 | Overlapping clusters |\n",
    "\n",
    "**For marketing segmentation**, interpretability matters more than silhouette score. Even with ~0.20-0.30 scores, segments can be actionable if they show clear business differences.\n",
    "\n",
    "**Best Practices Applied:**\n",
    "1. âœ… Log-transform skewed spending features\n",
    "2. âœ… PCA to remove multicollinearity  \n",
    "3. âœ… RFM methodology (Recency, Frequency, Monetary)\n",
    "4. âœ… Multiple algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2db9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROFILE THE OPTIMIZED CLUSTERS\n",
    "# =============================================================================\n",
    "\n",
    "# Add best labels to dataframe\n",
    "df['OptimizedCluster'] = labels_simple\n",
    "\n",
    "# Profile by key metrics\n",
    "profile_opt = df.groupby('OptimizedCluster').agg({\n",
    "    'Income': 'mean',\n",
    "    'TotalSpend': 'mean',\n",
    "    'Recency': 'mean',\n",
    "    'NumWebPurchases': 'mean',\n",
    "    'NumCatalogPurchases': 'mean',\n",
    "    'NumStorePurchases': 'mean',\n",
    "    'Response': 'mean',\n",
    "    'Age': 'mean'\n",
    "}).round(1)\n",
    "\n",
    "profile_opt['Count'] = df.groupby('OptimizedCluster').size()\n",
    "profile_opt['Pct'] = (profile_opt['Count'] / len(df) * 100).round(1)\n",
    "\n",
    "# Rename for clarity\n",
    "profile_opt = profile_opt.sort_values('TotalSpend', ascending=False)\n",
    "\n",
    "print(\"ðŸ“Š OPTIMIZED CLUSTER PROFILES (Income + Spending Based)\")\n",
    "print(\"=\" * 80)\n",
    "print(profile_opt.to_string())\n",
    "\n",
    "# Name the segments based on Income/Spending\n",
    "print(\"\\n\\nðŸŽ¯ SEGMENT NAMES (by Income Ã— Spending):\")\n",
    "for idx, row in profile_opt.iterrows():\n",
    "    income_level = \"High\" if row['Income'] > df['Income'].median() else \"Low\"\n",
    "    spend_level = \"High\" if row['TotalSpend'] > df['TotalSpend'].median() else \"Low\"\n",
    "    response_rate = row['Response'] * 100\n",
    "    \n",
    "    if income_level == \"High\" and spend_level == \"High\":\n",
    "        name = \"ðŸ’Ž Premium Champions\"\n",
    "    elif income_level == \"High\" and spend_level == \"Low\":\n",
    "        name = \"ðŸ’° Affluent Underperformers\"  \n",
    "    elif income_level == \"Low\" and spend_level == \"High\":\n",
    "        name = \"ðŸŒŸ Value Maximizers\"\n",
    "    else:\n",
    "        name = \"ðŸ“‰ Budget Conscious\"\n",
    "    \n",
    "    print(f\"   Cluster {idx}: {name}\")\n",
    "    print(f\"      Income: ${row['Income']:,.0f} | Spend: ${row['TotalSpend']:,.0f}\")\n",
    "    print(f\"      Response Rate: {response_rate:.1f}% | Size: {row['Count']:.0f} ({row['Pct']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb25f3e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸš€ Further Optimization Attempts\n",
    "\n",
    "Let's push the boundaries with more advanced techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71adb433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED OPTIMIZATION 1: Try Different K Values on Simple 2-Feature\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "print(\"ðŸ” Testing different cluster counts on Simple 2-Feature approach:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_k = []\n",
    "for k in range(2, 9):\n",
    "    kmeans_k = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=20)\n",
    "    labels_k = kmeans_k.fit_predict(X_simple_scaled)\n",
    "    sil_k = silhouette_score(X_simple_scaled, labels_k)\n",
    "    ch_k = calinski_harabasz_score(X_simple_scaled, labels_k)\n",
    "    db_k = davies_bouldin_score(X_simple_scaled, labels_k)\n",
    "    results_k.append({'k': k, 'Silhouette': sil_k, 'CH': ch_k, 'DB': db_k})\n",
    "    print(f\"   k={k}: Silhouette={sil_k:.4f}, CH={ch_k:.0f}, DB={db_k:.3f}\")\n",
    "\n",
    "# Find best k\n",
    "best_k_result = max(results_k, key=lambda x: x['Silhouette'])\n",
    "print(f\"\\nðŸ† Best k by Silhouette: k={best_k_result['k']} with score={best_k_result['Silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED OPTIMIZATION 2: Try 3-Feature (Income + Spend + Recency)\n",
    "# =============================================================================\n",
    "\n",
    "# Add Recency for RFM-like approach but simplified\n",
    "X_3feat = df[['Income', 'TotalSpend', 'Recency']].copy()\n",
    "X_3feat['Income'] = np.log1p(X_3feat['Income'])\n",
    "X_3feat['TotalSpend'] = np.log1p(X_3feat['TotalSpend'])\n",
    "# Recency doesn't need log (already reasonable distribution)\n",
    "X_3feat_scaled = StandardScaler().fit_transform(X_3feat)\n",
    "\n",
    "print(\"ðŸ” Testing 3-Feature (Income + Spend + Recency):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_3feat = []\n",
    "for k in range(2, 7):\n",
    "    kmeans_3f = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=20)\n",
    "    labels_3f = kmeans_3f.fit_predict(X_3feat_scaled)\n",
    "    sil_3f = silhouette_score(X_3feat_scaled, labels_3f)\n",
    "    results_3feat.append({'k': k, 'Silhouette': sil_3f})\n",
    "    print(f\"   k={k}: Silhouette={sil_3f:.4f}\")\n",
    "\n",
    "best_3feat = max(results_3feat, key=lambda x: x['Silhouette'])\n",
    "print(f\"\\nðŸ† Best 3-Feature: k={best_3feat['k']} with score={best_3feat['Silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED OPTIMIZATION 3: GMM on Simple 2-Feature Data\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ” Testing GMM on Simple 2-Feature approach:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_gmm_simple = []\n",
    "for k in range(2, 7):\n",
    "    for cov in ['full', 'tied', 'diag', 'spherical']:\n",
    "        gmm_s = GaussianMixture(n_components=k, covariance_type=cov, \n",
    "                                random_state=RANDOM_STATE, n_init=5, max_iter=200)\n",
    "        labels_gmm_s = gmm_s.fit_predict(X_simple_scaled)\n",
    "        sil_gmm_s = silhouette_score(X_simple_scaled, labels_gmm_s)\n",
    "        results_gmm_simple.append({'k': k, 'cov': cov, 'Silhouette': sil_gmm_s})\n",
    "\n",
    "# Find best GMM configuration\n",
    "best_gmm_simple = max(results_gmm_simple, key=lambda x: x['Silhouette'])\n",
    "print(f\"ðŸ† Best GMM on 2-Feature: k={best_gmm_simple['k']}, cov={best_gmm_simple['cov']}\")\n",
    "print(f\"   Silhouette: {best_gmm_simple['Silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb66888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED OPTIMIZATION 4: Different Scalers\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, PowerTransformer\n",
    "\n",
    "print(\"ðŸ” Testing different scalers on 2-Feature approach:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'PowerTransformer': PowerTransformer(method='yeo-johnson')\n",
    "}\n",
    "\n",
    "X_2feat_raw = df[['Income', 'TotalSpend']].copy()\n",
    "\n",
    "for scaler_name, scaler_obj in scalers.items():\n",
    "    X_scaled_test = scaler_obj.fit_transform(X_2feat_raw)\n",
    "    kmeans_test = KMeans(n_clusters=4, random_state=RANDOM_STATE, n_init=20)\n",
    "    labels_test = kmeans_test.fit_predict(X_scaled_test)\n",
    "    sil_test = silhouette_score(X_scaled_test, labels_test)\n",
    "    print(f\"   {scaler_name}: Silhouette={sil_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED OPTIMIZATION 5: Hierarchical Clustering on 2-Feature\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "print(\"ðŸ” Testing Hierarchical Clustering on 2-Feature:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "linkages = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "for linkage_type in linkages:\n",
    "    hier_test = AgglomerativeClustering(n_clusters=4, linkage=linkage_type)\n",
    "    labels_hier = hier_test.fit_predict(X_simple_scaled)\n",
    "    sil_hier_test = silhouette_score(X_simple_scaled, labels_hier)\n",
    "    print(f\"   {linkage_type}: Silhouette={sil_hier_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2952f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED OPTIMIZATION 6: K-Means++ with More Initializations\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ” Testing K-Means with more initializations (n_init):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for n_init in [10, 20, 50, 100]:\n",
    "    kmeans_init = KMeans(n_clusters=4, random_state=RANDOM_STATE, n_init=n_init, max_iter=500)\n",
    "    labels_init = kmeans_init.fit_predict(X_simple_scaled)\n",
    "    sil_init = silhouette_score(X_simple_scaled, labels_init)\n",
    "    print(f\"   n_init={n_init}: Silhouette={sil_init:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46218fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "# Test MinMaxScaler + k=2 combination\n",
    "X_minmax = MinMaxScaler().fit_transform(df[['Income', 'TotalSpend']])\n",
    "kmeans_best = KMeans(n_clusters=2, random_state=RANDOM_STATE, n_init=20)\n",
    "labels_best = kmeans_best.fit_predict(X_minmax)\n",
    "sil_best = silhouette_score(X_minmax, labels_best)\n",
    "\n",
    "# Also test k=3 with MinMax\n",
    "kmeans_3 = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=20)\n",
    "labels_3_mm = kmeans_3.fit_predict(X_minmax)\n",
    "sil_3_mm = silhouette_score(X_minmax, labels_3_mm)\n",
    "\n",
    "# Test k=4 with MinMax\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=RANDOM_STATE, n_init=20)\n",
    "labels_4_mm = kmeans_4.fit_predict(X_minmax)\n",
    "sil_4_mm = silhouette_score(X_minmax, labels_4_mm)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ† COMPREHENSIVE OPTIMIZATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_final_results = [\n",
    "    (\"Original (17 features, k=4)\", 0.165, \"Baseline\"),\n",
    "    (\"Log 2-Feature (k=4)\", sil_simple, \"Simple log transform\"),\n",
    "    (\"Log 2-Feature (k=2)\", 0.5903, \"Fewer clusters\"),\n",
    "    (\"MinMax 2-Feature (k=4)\", sil_4_mm, \"Different scaler\"),\n",
    "    (\"MinMax 2-Feature (k=3)\", sil_3_mm, \"Different scaler + k\"),\n",
    "    (\"MinMax 2-Feature (k=2)\", sil_best, \"Best combination\"),\n",
    "    (\"Hierarchical Single (k=4)\", 0.6681, \"âš ï¸ May have outlier cluster\"),\n",
    "    (\"RFM (k=4)\", sil_rfm, \"Classic marketing\"),\n",
    "]\n",
    "\n",
    "# Sort\n",
    "all_final_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nðŸ“Š All Methods Ranked by Silhouette Score:\")\n",
    "print(\"-\" * 70)\n",
    "for i, (name, score, note) in enumerate(all_final_results, 1):\n",
    "    improvement = (score - 0.165) / 0.165 * 100\n",
    "    stars = \"â­\" * min(5, int(score * 10))\n",
    "    print(f\"  {i}. {name}\")\n",
    "    print(f\"     Score: {score:.4f} {stars} ({improvement:+.1f}%)\")\n",
    "    if note:\n",
    "        print(f\"     Note: {note}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ðŸ’¡ KEY INSIGHTS:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"1. Simpler is better: 2 features > 17 features\")\n",
    "print(\"2. Fewer clusters often score higher (k=2 > k=4)\")\n",
    "print(\"3. MinMaxScaler works better than StandardScaler here\")\n",
    "print(\"4. Single-linkage may inflate scores via outlier separation\")\n",
    "print(\"\")\n",
    "print(\"âš ï¸ IMPORTANT: Higher silhouette â‰  better business value!\")\n",
    "print(\"   - k=4 provides more actionable segments than k=2\")\n",
    "print(\"   - Single-linkage may just separate outliers\")\n",
    "print(\"   - For marketing: k=3-4 with ~0.45-0.50 is excellent\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RECOMMENDED MODEL: MinMax + k=4 (Best Balance of Score + Business Value)\n",
    "# =============================================================================\n",
    "\n",
    "# Create the recommended clustering\n",
    "X_recommended = MinMaxScaler().fit_transform(df[['Income', 'TotalSpend']])\n",
    "kmeans_rec = KMeans(n_clusters=4, random_state=RANDOM_STATE, n_init=50)\n",
    "df['RecommendedCluster'] = kmeans_rec.fit_predict(X_recommended)\n",
    "\n",
    "# Profile the recommended clusters\n",
    "profile_rec = df.groupby('RecommendedCluster').agg({\n",
    "    'Income': 'mean',\n",
    "    'TotalSpend': 'mean',\n",
    "    'Recency': 'mean',\n",
    "    'Response': 'mean',\n",
    "    'Age': 'mean'\n",
    "}).round(1)\n",
    "\n",
    "profile_rec['Count'] = df.groupby('RecommendedCluster').size()\n",
    "profile_rec['Pct'] = (profile_rec['Count'] / len(df) * 100).round(1)\n",
    "profile_rec = profile_rec.sort_values('TotalSpend', ascending=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸŽ¯ RECOMMENDED MODEL: MinMax + K-Means (k=4)\")\n",
    "print(f\"   Silhouette Score: {sil_4_mm:.4f} (+272% vs baseline)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nðŸ“Š Cluster Profiles:\")\n",
    "print(profile_rec.to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter\n",
    "scatter = axes[0].scatter(X_recommended[:, 0], X_recommended[:, 1], \n",
    "                          c=df['RecommendedCluster'], cmap='viridis', alpha=0.6, s=30)\n",
    "axes[0].set_xlabel('Income (Normalized)')\n",
    "axes[0].set_ylabel('Total Spend (Normalized)')\n",
    "axes[0].set_title(f'Recommended Clustering\\nSilhouette = {sil_4_mm:.3f}')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Bar chart\n",
    "cluster_counts = df['RecommendedCluster'].value_counts().sort_index()\n",
    "axes[1].bar(cluster_counts.index, cluster_counts.values, color=plt.cm.viridis(cluster_counts.index/3))\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Cluster Distribution')\n",
    "for i, v in enumerate(cluster_counts.values):\n",
    "    axes[1].text(i, v + 20, f'{v}\\n({v/len(df)*100:.1f}%)', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190cc74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š Final Analysis: Business Value & Recommendations\n",
    "\n",
    "### Segment Revenue Potential\n",
    "\n",
    "| Segment | Size | Avg Spend | Total Revenue | Campaign Response |\n",
    "|---------|------|-----------|---------------|-------------------|\n",
    "| Budget-Conscious | 564 | $102 | $57,580 | 13% (average) |\n",
    "| Empty Nesters | 559 | $128 | $71,552 | 6% (lowest) |\n",
    "| **Affluent Premium** | 573 | **$1,391** | **$797,046** | **28% (highest)** |\n",
    "| Digital Deal Hunters | 541 | $792 | $428,472 | 13% (average) |\n",
    "\n",
    "**Key Insight**: Cluster 2 (Affluent Premium) represents **59% of total revenue** despite being only 26% of customers.\n",
    "\n",
    "### Strategic Recommendations\n",
    "\n",
    "1. **Prioritize Cluster 2** â€” They're the most valuable AND most responsive\n",
    "2. **Re-engage Cluster 1** â€” Large segment but lowest response rate; needs different approach\n",
    "3. **Digital-first for Cluster 3** â€” Match their channel preference\n",
    "4. **Value messaging for Cluster 0** â€” Budget constraints require deal-focused marketing\n",
    "\n",
    "### Model Deployment\n",
    "\n",
    "The clustering model can be used to:\n",
    "- **Score new customers** into segments for personalized onboarding\n",
    "- **Target campaigns** by segment for improved ROI\n",
    "- **Track segment migration** to identify at-risk customers\n",
    "- **Product recommendations** based on segment preferences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
